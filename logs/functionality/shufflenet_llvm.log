Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_avg_pool2d"
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 16, 16):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 15 and 1 <= ax3 and ax3 < 15, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 272, 7, 7, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 13) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 13) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 272, 7, 7, 9], dtype="float32")
            for i0, i1, i2, i3, i4_i5_fused_0 in T.grid(1, 272, 7, 7, 9):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(272, i1 + ax1)
                        ax2_1 = T.axis.spatial(16, i2 * 2 + i4_i5_fused_0 // 3 + ax2)
                        ax3_1 = T.axis.spatial(16, i3 * 2 + i4_i5_fused_0 % 3 + ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 15 and 1 <= ax3_1 and ax3_1 < 15, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1], T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_0 = T.axis.spatial(9, i4_i5_fused_0 + ax0)
                        ax0_2 = T.axis.spatial(1, ax1)
                        ax1_2 = T.axis.spatial(272, i1 + ax2)
                        ax2_2 = T.axis.spatial(7, i2 + ax3)
                        ax3_2 = T.axis.spatial(7, i3 + ax4)
                        T.reads(pad_temp[ax0_2, ax1_2, ax2_2 * 2 + vi4_i5_fused_0 // 3, ax3_2 * 2 + vi4_i5_fused_0 % 3])
                        T.writes(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0])
                        with T.init():
                            tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0] = T.float32(0)
                        tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0] = tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0] + pad_temp[ax0_2, ax1_2, ax2_2 * 2 + vi4_i5_fused_0 // 3, ax3_2 * 2 + vi4_i5_fused_0 % 3]
                for i4_i5_fused_1 in T.serial(1):
                    with T.block("tensor"):
                        vi4_i5_fused_0 = T.axis.reduce(9, i4_i5_fused_0)
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3, ax2_3, ax3_3 = T.axis.remap("SSS", [i1, i2, i3])
                        T.reads(tensor_rf[ax0_3, ax1_3, ax2_3, ax3_3, vi4_i5_fused_0])
                        T.writes(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3])
                        with T.init():
                            tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = T.float32(0)
                        tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] + tensor_rf[ax0_3, ax1_3, ax2_3, ax3_3, vi4_i5_fused_0]
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("tensor_1"):
                    ax0_4, ax1_4, ax2_4, ax3_4 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0_4, ax1_4, ax2_4, ax3_4])
                    T.writes(tensor[ax0_4, ax1_4, ax2_4, ax3_4])
                    tensor[ax0_4, ax1_4, ax2_4, ax3_4] = tensor_1[ax0_4, ax1_4, ax2_4, ax3_4] / T.cast(T.max((T.min(ax2_4 * 2 + 1, 13) + 2 - T.max(1 - ax2_4 * 2, 0) - ax2_4 * 2) * (T.min(ax3_4 * 2 + 1, 13) + 2 - T.max(1 - ax3_4 * 2, 0) - ax3_4 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=4)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l19, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 272, 7, 7, 1], dtype="float32")
            for i0, i1 in T.grid(1, 272):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 15, 15):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(272, i1 + ax1)
                        ax2_1 = T.axis.spatial(16, ax2)
                        ax3_1 = T.axis.spatial(16, ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 15 and 1 <= ax3_1 and ax3_1 < 15, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1], T.float32(0), dtype="float32")
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 7, 7, 3, 3):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_1, ax0_2 = T.axis.remap("SS", [ax0, ax1])
                        ax1_2 = T.axis.spatial(272, i1 + ax2)
                        ax2_2, ax3_2, rv0, rv1 = T.axis.remap("SSRR", [ax3, ax4, ax5, ax6])
                        T.reads(pad_temp[ax0_2, ax1_2, ax2_2 * 2 + rv0, ax3_2 * 2 + rv1])
                        T.writes(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1])
                        with T.init():
                            tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] = T.float32(0)
                        tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] = tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] + pad_temp[ax0_2, ax1_2, ax2_2 * 2 + rv0, ax3_2 * 2 + rv1]
                for i2, i3, i4_i5_fused_1 in T.grid(7, 7, 1):
                    with T.block("tensor"):
                        vi4_i5_fused_1 = T.axis.reduce(1, i4_i5_fused_1)
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3, ax2_3, ax3_3 = T.axis.remap("SSS", [i1, i2, i3])
                        T.reads(tensor_rf[ax0_3, ax1_3, ax2_3, ax3_3, vi4_i5_fused_1])
                        T.writes(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3])
                        with T.init():
                            tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = T.float32(0)
                        tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] + tensor_rf[ax0_3, ax1_3, ax2_3, ax3_3, vi4_i5_fused_1]
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("tensor_1"):
                    ax0_4, ax1_4, ax2_4, ax3_4 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0_4, ax1_4, ax2_4, ax3_4])
                    T.writes(tensor[ax0_4, ax1_4, ax2_4, ax3_4])
                    tensor[ax0_4, ax1_4, ax2_4, ax3_4] = tensor_1[ax0_4, ax1_4, ax2_4, ax3_4] / T.cast(T.max((T.min(ax2_4 * 2 + 1, 13) + 2 - T.max(1 - ax2_4 * 2, 0) - ax2_4 * 2) * (T.min(ax3_4 * 2 + 1, 13) + 2 - T.max(1 - ax3_4 * 2, 0) - ax3_4 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[9, 1])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l19, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], tensor: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 272, 16, 16], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            for i0, i1 in T.grid(1, 272):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 15, 15):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(272, i1 + ax1)
                        ax2_1 = T.axis.spatial(16, ax2)
                        ax3_1 = T.axis.spatial(16, ax3)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 15 and 1 <= ax3_1 and ax3_1 < 15, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1], T.float32(0), dtype="float32")
                for i2, i3 in T.grid(7, 7):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 1, 3, 3):
                        with T.block("tensor"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(272, i1 + ax1)
                            ax2_2 = T.axis.spatial(7, i2 + ax2)
                            ax3_2 = T.axis.spatial(7, i3 + ax3)
                            rv0, rv1 = T.axis.remap("RR", [ax4, ax5])
                            T.reads(pad_temp[ax0_2, ax1_2, ax2_2 * 2 + rv0, ax3_2 * 2 + rv1])
                            T.writes(tensor_1[ax0_2, ax1_2, ax2_2, ax3_2])
                            with T.init():
                                tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] = T.float32(0)
                            tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] = tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] + pad_temp[ax0_2, ax1_2, ax2_2 * 2 + rv0, ax3_2 * 2 + rv1]
                    with T.block("tensor_1"):
                        ax0_3, ax1_3, ax2_3, ax3_3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3])
                        T.writes(tensor[ax0_3, ax1_3, ax2_3, ax3_3])
                        tensor[ax0_3, ax1_3, ax2_3, ax3_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] / T.cast(T.max((T.min(ax2_3 * 2 + 1, 13) + 2 - T.max(1 - ax2_3 * 2, 0) - ax2_3 * 2) * (T.min(ax3_3 * 2 + 1, 13) + 2 - T.max(1 - ax3_3 * 2, 0) - ax3_3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
l5 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l5, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_avg_pool2d_1"
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 30, 30):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 29 and 1 <= ax3 and ax3 < 29, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 136, 14, 14, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 * 2 + 1, 27) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 27) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 136, 14, 14, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 136, 30, 30):
                with T.block("pad_temp"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1])
                    T.writes(pad_temp[ax0, ax1, ax2, ax3])
                    pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(1 <= ax2 and ax2 < 29 and 1 <= ax3 and ax3 < 29, placeholder[ax0, ax1, ax2 - 1, ax3 - 1], T.float32(0), dtype="float32")
            for i0, i1, i2 in T.grid(1, 136, 14):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 14, 3, 3):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_0, ax0_1 = T.axis.remap("SS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(136, i1 + ax2)
                        ax2_1 = T.axis.spatial(14, i2 + ax3)
                        ax3_1, rv0, rv1 = T.axis.remap("SRR", [ax4, ax5, ax6])
                        T.reads(pad_temp[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1])
                        T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0])
                        with T.init():
                            tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] = T.float32(0)
                        tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] = tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] + pad_temp[ax0_1, ax1_1, ax2_1 * 2 + rv0, ax3_1 * 2 + rv1]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 14):
                    with T.block("tensor"):
                        vi4_i5_fused_0, ax0_2 = T.axis.remap("RS", [ax0, ax1])
                        ax1_2 = T.axis.spatial(136, i1 + ax2)
                        ax2_2 = T.axis.spatial(14, i2 + ax3)
                        ax3_2 = T.axis.spatial(14, ax4)
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0])
                        T.writes(tensor_1[ax0_2, ax1_2, ax2_2, ax3_2])
                        with T.init():
                            tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] = T.float32(0)
                        tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] = tensor_1[ax0_2, ax1_2, ax2_2, ax3_2] + tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, vi4_i5_fused_0]
                for i3 in T.serial(14):
                    with T.block("tensor_1"):
                        ax0_3, ax1_3, ax2_3, ax3_3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3])
                        T.writes(tensor[ax0_3, ax1_3, ax2_3, ax3_3])
                        tensor[ax0_3, ax1_3, ax2_3, ax3_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] / T.cast(T.max((T.min(ax2_3 * 2 + 1, 27) + 2 - T.max(1 - ax2_3 * 2, 0) - ax2_3 * 2) * (T.min(ax3_3 * 2 + 1, 27) + 2 - T.max(1 - ax3_3 * 2, 0) - ax3_3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l12, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=2)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l19, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 136, 14, 14, 9], dtype="float32")
            for i0, i1 in T.grid(1, 136):
                for ax0 in T.serial(9):
                    for ax0_1, ax1, ax2, ax3 in T.grid(1, 1, 27, 27):
                        with T.block("pad_temp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_1 = T.axis.spatial(136, i1 + ax1)
                            ax2_1 = T.axis.spatial(30, ax0 // 3 + ax2)
                            ax3_1 = T.axis.spatial(30, ax0 % 3 + ax3)
                            T.reads(placeholder[ax0_2, ax1_1, ax2_1 - 1, ax3_1 - 1])
                            T.writes(pad_temp[ax0_2, ax1_1, ax2_1, ax3_1])
                            pad_temp[ax0_2, ax1_1, ax2_1, ax3_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 29 and 1 <= ax3_1 and ax3_1 < 29, placeholder[ax0_2, ax1_1, ax2_1 - 1, ax3_1 - 1], T.float32(0), dtype="float32")
                    for ax1, ax2, ax3, ax4 in T.grid(1, 1, 14, 14):
                        with T.block("tensor_rf"):
                            vi4_i5_fused_1, ax0_3 = T.axis.remap("SS", [ax0, ax1])
                            ax1_2 = T.axis.spatial(136, i1 + ax2)
                            ax2_2, ax3_2 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(pad_temp[ax0_3, ax1_2, ax2_2 * 2 + vi4_i5_fused_1 // 3, ax3_2 * 2 + vi4_i5_fused_1 % 3])
                            T.writes(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1])
                            with T.init():
                                tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] = T.float32(0)
                            tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] = tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, vi4_i5_fused_1] + pad_temp[ax0_3, ax1_2, ax2_2 * 2 + vi4_i5_fused_1 // 3, ax3_2 * 2 + vi4_i5_fused_1 % 3]
                for i2, i3 in T.grid(14, 14):
                    for ax0_4, ax1_3, ax2_3, ax3_3, ax4 in T.grid(9, 1, 1, 1, 1):
                        with T.block("tensor"):
                            vi4_i5_fused_1, ax0_5 = T.axis.remap("RS", [ax0_4, ax1_3])
                            ax1_4 = T.axis.spatial(136, i1 + ax2_3)
                            ax2_4 = T.axis.spatial(14, i2 + ax3_3)
                            ax3_4 = T.axis.spatial(14, i3 + ax4)
                            T.reads(tensor_rf[ax0_5, ax1_4, ax2_4, ax3_4, vi4_i5_fused_1])
                            T.writes(tensor_1[ax0_5, ax1_4, ax2_4, ax3_4])
                            with T.init():
                                tensor_1[ax0_5, ax1_4, ax2_4, ax3_4] = T.float32(0)
                            tensor_1[ax0_5, ax1_4, ax2_4, ax3_4] = tensor_1[ax0_5, ax1_4, ax2_4, ax3_4] + tensor_rf[ax0_5, ax1_4, ax2_4, ax3_4, vi4_i5_fused_1]
                    with T.block("tensor_1"):
                        ax0_6, ax1_5, ax2_5, ax3_5 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(tensor_1[ax0_6, ax1_5, ax2_5, ax3_5])
                        T.writes(tensor[ax0_6, ax1_5, ax2_5, ax3_5])
                        tensor[ax0_6, ax1_5, ax2_5, ax3_5] = tensor_1[ax0_6, ax1_5, ax2_5, ax3_5] / T.cast(T.max((T.min(ax2_5 * 2 + 1, 27) + 2 - T.max(1 - ax2_5 * 2, 0) - ax2_5 * 2) * (T.min(ax3_5 * 2 + 1, 27) + 2 - T.max(1 - ax3_5 * 2, 0) - ax3_5 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8 = sch.get_loops(block=b1)
l9 = sch.fuse(l7, l8)
v10, v11 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 9])
l12, l13 = sch.split(loop=l9, factors=[v10, v11])
b14 = sch.rfactor(loop=l13, factor_axis=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v15 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v15)
b16, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l17 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l17, preserve_unit_loops=True)
l18 = sch.sample_compute_location(block=b16, decision=1)
sch.compute_at(block=b16, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l19, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], tensor: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 136, 30, 30], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            for i0, i1 in T.grid(1, 136):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                    for ax0_1, ax1_1, ax2_1, ax3_1 in T.grid(1, 1, 3, 3):
                        with T.block("pad_temp"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(136, i1 + ax1_1)
                            ax2_2 = T.axis.spatial(30, ax2 * 2 + ax2_1)
                            ax3_2 = T.axis.spatial(30, ax3 * 2 + ax3_1)
                            T.reads(placeholder[ax0_2, ax1_2, ax2_2 - 1, ax3_2 - 1])
                            T.writes(pad_temp[ax0_2, ax1_2, ax2_2, ax3_2])
                            pad_temp[ax0_2, ax1_2, ax2_2, ax3_2] = T.if_then_else(1 <= ax2_2 and ax2_2 < 29 and 1 <= ax3_2 and ax3_2 < 29, placeholder[ax0_2, ax1_2, ax2_2 - 1, ax3_2 - 1], T.float32(0), dtype="float32")
                    for ax4, ax5 in T.grid(3, 3):
                        with T.block("tensor"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(136, i1 + ax1)
                            ax2_3, ax3_3, rv0, rv1 = T.axis.remap("SSRR", [ax2, ax3, ax4, ax5])
                            T.reads(pad_temp[ax0_3, ax1_3, ax2_3 * 2 + rv0, ax3_3 * 2 + rv1])
                            T.writes(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3])
                            with T.init():
                                tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = T.float32(0)
                            tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3] + pad_temp[ax0_3, ax1_3, ax2_3 * 2 + rv0, ax3_3 * 2 + rv1]
                for i2, i3 in T.grid(14, 14):
                    with T.block("tensor_1"):
                        ax0_4, ax1_4, ax2_4, ax3_4 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(tensor_1[ax0_4, ax1_4, ax2_4, ax3_4])
                        T.writes(tensor[ax0_4, ax1_4, ax2_4, ax3_4])
                        tensor[ax0_4, ax1_4, ax2_4, ax3_4] = tensor_1[ax0_4, ax1_4, ax2_4, ax3_4] / T.cast(T.max((T.min(ax2_4 * 2 + 1, 27) + 2 - T.max(1 - ax2_4 * 2, 0) - ax2_4 * 2) * (T.min(ax3_4 * 2 + 1, 27) + 2 - T.max(1 - ax3_4 * 2, 0) - ax3_4 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
l5 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l5, preserve_unit_loops=True)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_layout_transform"
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_layout_trans: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 24 and ax2 < 56 and ax3 < 56, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], T_layout_trans: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 24 and ax2 < 56 and ax3 < 56, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_nn_conv2d_multiply_add_nn_relu"
[16:59:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 2, 56, 3, 56], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 7, 2, 1, 1, 3, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        T_multiply = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 2, 56, 3, 56):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 6 + C * 3 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 6 + C * 3 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 7, 2, 1, 1, 3, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 7, 56, 56, 4, 6, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw], kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw] * kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 2, 56, 3, 56], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 7, 2, 1, 1, 3, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 7, 7, 14):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 2, 8, 3, 4):
                    with T.block("data_vec"):
                        g, n, C = T.axis.remap("SSS", [ax0, ax1, ax2])
                        h = T.axis.spatial(56, i3_0 * 8 + ax3)
                        c = T.axis.spatial(3, ax4)
                        w = T.axis.spatial(56, i4_0 * 4 + ax5)
                        T.reads(placeholder[n, g * 6 + C * 3 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 6 + C * 3 + c, h, w]
                for i5_0, i0_1, i1_1, i2_1, i3_1 in T.grid(2, 1, 1, 1, 4):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 1, 2, 1, 1, 3, 2):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, ax0)
                            out_channel = T.axis.spatial(7, i2_0 + ax1)
                            in_channel, h, w, ci = T.axis.remap("SSSS", [ax2, ax3, ax4, ax5])
                            co = T.axis.spatial(4, i5_0 * 2 + ax6)
                            T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w]
                    for i4_1, i5_1 in T.grid(2, 1):
                        for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 1, 2, 2, 2):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(7, i2_0)
                                oh = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_3)
                                ow = T.axis.spatial(56, i4_0 * 4 + i4_1 * 2 + i4_3)
                                oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                                ic = T.axis.reduce(6, i6_0 * 3 + i6_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw], kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw] * kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 2, 2, 2):
                            with T.block("conv_global"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(7, i2_0 + ax2)
                                v3 = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + ax3)
                                v4 = T.axis.spatial(56, i4_0 * 4 + i4_1 * 2 + ax4)
                                v5 = T.axis.spatial(4, i5_0 * 2 + ax5)
                                T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                                T.writes(conv[v0, v1, v2, v3, v4, v5])
                                conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(112, i1 + ax1)
                        h = T.axis.spatial(56, i2 + ax2)
                        w = T.axis.spatial(56, i3 + ax3)
                        T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 4, 1, 2])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 3])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=9)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 2, 56, 3, 56], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 7, 7, 14, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1 in T.grid(1, 1, 1, 4, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 2, 1, 2):
                        with T.block("data_vec"):
                            g, n = T.axis.remap("SS", [ax0, ax1])
                            C = T.axis.spatial(2, i6_0 + ax2)
                            h = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + ax3)
                            c = T.axis.spatial(3, i6_1 + ax4)
                            w = T.axis.spatial(56, i4_0 * 4 + i4_1 * 2 + ax5)
                            T.reads(placeholder[n, g * 6 + C * 3 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = placeholder[n, g * 6 + C * 3 + c, h, w]
                    for i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 4, 1, 1, 2, 2, 2):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(7, i2_0)
                            oh = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_3)
                            ow = T.axis.spatial(56, i4_0 * 4 + i4_1 * 2 + i4_3)
                            oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                            ic = T.axis.reduce(6, i6_0 * 3 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw], placeholder_1[g * 28 + oc_chunk * 4 + oc_block, ic, kh, kw])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 3, oh + kh, ic % 3, ow + kw] * placeholder_1[g * 28 + oc_chunk * 4 + oc_block, ic // 3 * 3 + ic % 3, kh, kw]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 8, 4, 2):
                    with T.block("conv_global"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(7, i2_0 + ax2)
                        v3 = T.axis.spatial(56, i3_0 * 8 + ax3)
                        v4 = T.axis.spatial(56, i4_0 * 4 + ax4)
                        v5 = T.axis.spatial(4, i5_0 * 2 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1 in T.grid(1, 112):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 56, 56):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(112, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
                for i2, i3 in T.grid(56, 56):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 4, 1, 2])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 3])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=21)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(112, 6, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_relu: T.Buffer[(1, 112, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            kernel_vec = T.alloc_buffer([4, 7, 2, 1, 1, 3, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 56, 56, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 7, 7, 14, 2, 1, 1, 1, 4, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 1, 1, 1, 1, 3, 2):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, ax0)
                        out_channel = T.axis.spatial(7, i2_0 + ax1)
                        in_channel = T.axis.spatial(2, i6_0 + ax2)
                        h, w, ci = T.axis.remap("SSS", [ax3, ax4, ax5])
                        co = T.axis.spatial(4, i5_0 * 2 + ax6)
                        T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 3 + ci, h, w]
                for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 1, 2, 2, 2):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_3)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(7, i2_0)
                        oh = T.axis.spatial(56, i3_0 * 8 + i3_1 * 2 + i3_3)
                        ow = T.axis.spatial(56, i4_0 * 4 + i4_1 * 2 + i4_3)
                        oc_block = T.axis.spatial(4, i5_0 * 2 + i5_3)
                        ic = T.axis.reduce(6, i6_0 * 3 + i6_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, g * 6 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 6 + ic // 3 * 3 + ic % 3, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1 in T.grid(1, 112):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 56, 56):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(112, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [112, 6, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
                for i2, i3 in T.grid(56, 56):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 4, 1, 2])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[14, 2, 1, 2])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 3])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_reshape_transpose_reshape_layout_transform"
[16:59:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 28, 56, 56):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 4, 56, 56):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 112, 56, 56):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax1) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 56 + ax2) // 56 + ax1) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax1) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 56, 56, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 112 and ax2 < 56 and ax3 < 56, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
            T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
            T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
            for i0, i1 in T.grid(1, 28):
                for ax0, ax1, ax2 in T.grid(1, 1, 4):
                    for ax0_1, ax1_1, ax2_1, ax3, ax4 in T.grid(1, 1, 1, 56, 56):
                        with T.block("T_reshape"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(4, ax2 + ax1_1)
                            ax2_2 = T.axis.spatial(28, i1 + ax2_1)
                            ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                            T.reads(placeholder[0, (ax1_2 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_2) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56])
                            T.writes(T_reshape[ax0_2, ax1_2, ax2_2, ax3_1, ax4_1])
                            T_reshape[ax0_2, ax1_2, ax2_2, ax3_1, ax4_1] = placeholder[0, (ax1_2 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_2) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56]
                    for ax3, ax4 in T.grid(56, 56):
                        with T.block("T_transpose"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(28, i1 + ax1)
                            ax2_3, ax3_2, ax4_2 = T.axis.remap("SSS", [ax2, ax3, ax4])
                            T.reads(T_reshape[ax0_3, ax2_3, ax1_3, ax3_2, ax4_2])
                            T.writes(T_transpose[ax0_3, ax1_3, ax2_3, ax3_2, ax4_2])
                            T_transpose[ax0_3, ax1_3, ax2_3, ax3_2, ax4_2] = T_reshape[ax0_3, ax2_3, ax1_3, ax3_2, ax4_2]
                for ax0_4, ax1_4, ax2_4, ax3_3 in T.grid(1, 4, 56, 56):
                    with T.block("T_reshape_1"):
                        ax0_5 = T.axis.spatial(1, ax0_4)
                        ax1_5 = T.axis.spatial(112, i1 * 4 + ax1_4)
                        ax2_5, ax3_4 = T.axis.remap("SS", [ax2_4, ax3_3])
                        T.reads(T_transpose[0, ((ax3_4 // 56 + ax2_5) // 56 + ax1_5) % 112 // 4, ((ax3_4 // 56 + ax2_5) // 56 + ax1_5) % 4, (ax3_4 // 56 + ax2_5) % 56, ax3_4 % 56])
                        T.writes(T_reshape_1[ax0_5, ax1_5, ax2_5, ax3_4])
                        T_reshape_1[ax0_5, ax1_5, ax2_5, ax3_4] = T_transpose[0, ((ax3_4 // 56 + ax2_5) // 56 + ax1_5) % 112 // 4, ((ax3_4 // 56 + ax2_5) // 56 + ax1_5) % 4, (ax3_4 // 56 + ax2_5) % 56, ax3_4 % 56]
                for i2, i3, i4 in T.grid(56, 56, 4):
                    with T.block("T_layout_trans"):
                        ax0_6, ax1_6, ax2_6, ax3_5, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_6, ax1_6 * 4 + ax4_3, ax2_6, ax3_5])
                        T.writes(T_layout_trans[ax0_6, ax1_6, ax2_6, ax3_5, ax4_3])
                        T_layout_trans[ax0_6, ax1_6, ax2_6, ax3_5, ax4_3] = T.if_then_else(ax0_6 < 1 and ax1_6 * 4 + ax4_3 < 112 and ax2_6 < 56 and ax3_5 < 56, T_reshape_1[ax0_6, ax1_6 * 4 + ax4_3, ax2_6, ax3_5], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(28, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 28, 58, 58, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 28, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 58, 58, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 28, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 28, 56, 56, 4], "float32"], ["TENSOR", [28, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 28, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(28, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 28, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 28, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 7, 1, 7, 2, 1, 1, 1, 2, 1, 1, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 57, 3, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(28, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(58, ax2)
                        i3 = T.axis.spatial(58, i3_0 * 8 + i3_1 * 4 + i6_0 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 1, 2, 3, 1, 1, 4, 4, 2, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(28, i1_0 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 28, 56, 56, 4], "float32"], ["TENSOR", [28, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 28, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(28, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 28, 58, 58, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 28, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0 in T.grid(1, 7, 1, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 57, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(28, i1_0 * 4 + ax1)
                        i2 = T.axis.spatial(58, ax2)
                        i3 = T.axis.spatial(58, i3_0 * 8 + ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 57 and 1 <= i3 and i3 < 57, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 1, 2, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 1, 1, 7, 1, 2, 3, 1, 1, 4, 4, 2, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(28, i1_0 * 4 + i1_3)
                            oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                            ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 28, 56, 56, 4], "float32"], ["TENSOR", [28, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 2, 2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(28, i1_0 * 4 + ax1)
                            ax2_1 = T.axis.spatial(28, ax2)
                            ax3_1 = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 56, 56, 4), "float32"], placeholder_1: T.Buffer[(28, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 28, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 28, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 28, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 7, 1, 7, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 3, 1, 1, 7, 1, 2, 3, 1, 1, 4, 4, 2, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(28, i1_0 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_2 * 4 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 4 + i3_1 * 2 + i3_3)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_0])
                        T.reads(placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 28, 56, 56, 4], "float32"], ["TENSOR", [28, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 57 and 1 <= ow * 2 + kw and ow * 2 + kw < 57, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 28, 4, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(28, i1_0 * 4 + ax1)
                        ax2_1 = T.axis.spatial(28, ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[7, 1, 1, 4])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 4])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[7, 2, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_layout_transform_1"
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 28, 28, 4), "float32"], T_layout_trans: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 112 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 28, 28, 28, 4), "float32"], T_layout_trans: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 112 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_conv2d_multiply_add"
[16:59:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 7, 28, 4, 28], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 7, 7, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 7, 28, 4, 28):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 28 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 28 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 7, 7, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 7, 28, 28, 4, 28, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 112, 28, 28], "float32"], ["TENSOR", [112, 28, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 7, 28, 4, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 7, 7, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
            for i0_0 in T.serial(2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 7, 7, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_0 * 2 + ax0)
                        out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSS", [ax1, ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 7, 1, 1, 1, 1, 1, 1, 7, 2, 1):
                    for i6_0 in T.serial(2):
                        for ax0, ax1 in T.grid(2, 1):
                            for ax2, ax3, ax4, ax5 in T.grid((i6_0 * 14 % 4 + 13) // 4 + 1, 4, 4, 14):
                                with T.block("data_vec"):
                                    g = T.axis.spatial(4, i0_0 * 2 + ax0)
                                    n = T.axis.spatial(1, ax1)
                                    C = T.axis.spatial(7, i6_0 * 14 // 4 + ax2)
                                    h = T.axis.spatial(28, i3_1 * 4 + ax3)
                                    c = T.axis.spatial(4, ax4)
                                    w = T.axis.spatial(28, i4_1 * 14 + ax5)
                                    T.reads(placeholder[n, g * 28 + C * 4 + c, h, w])
                                    T.writes(data_vec[g, n, C, h, c, w])
                                    data_vec[g, n, C, h, c, w] = placeholder[n, g * 28 + C * 4 + c, h, w]
                        for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 2, 14, 1, 14, 1, 1, 2, 1, 1, 2, 1, 4):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(7, i2_0)
                                oh = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                                ow = T.axis.spatial(28, i4_1 * 14 + i4_2)
                                oc_block = T.axis.spatial(4, i5_3)
                                ic = T.axis.reduce(28, i6_0 * 14 + i6_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 4, 14, 4):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_0 * 2 + ax0)
                            v1 = T.axis.spatial(1, ax1)
                            v2 = T.axis.spatial(7, i2_0 + ax2)
                            v3 = T.axis.spatial(28, i3_1 * 4 + ax3)
                            v4 = T.axis.spatial(28, i4_1 * 14 + ax4)
                            v5 = T.axis.spatial(4, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2 in T.grid(1, 112, 28):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 28):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(112, i1 + ax1)
                        h = T.axis.spatial(28, i2 + ax2)
                        w = T.axis.spatial(28, ax3)
                        T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 112, 28, 28], "float32"], ["TENSOR", [112, 28, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
                for i3 in T.serial(28):
                    with T.block("T_add"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_add[ax0, ax1, ax2, ax3])
                        T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 14])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=0)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            kernel_vec = T.alloc_buffer([4, 7, 7, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 112, 28, 28], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(2, 1, 7, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 1, 7):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 7, 1, 1, 4, 4):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_0 * 2 + ax0)
                            out_channel = T.axis.spatial(7, i2_0 + ax1)
                            in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                            T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                    for i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 2, 1, 1, 1, 1, 1, 2, 14, 1, 14, 1, 1, 2, 1, 1, 2, 1, 4):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(7, i2_0)
                            oh = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                            ow = T.axis.spatial(28, i4_1 * 14 + i4_2)
                            oc_block = T.axis.spatial(4, i5_3)
                            ic = T.axis.reduce(28, i6_0 * 14 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(placeholder[n, g * 28 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 28 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 28, 28, 4):
                    with T.block("conv_global"):
                        v0 = T.axis.spatial(4, i0_0 * 2 + ax0)
                        v1 = T.axis.spatial(1, ax1)
                        v2 = T.axis.spatial(7, i2_0 + ax2)
                        v3, v4, v5 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(112, i1 + ax1)
                        h = T.axis.spatial(28, i2 + ax2)
                        w = T.axis.spatial(28, i3 + ax3)
                        T.reads(conv[c // 28, n, c % 28 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 112, 28, 28], "float32"], ["TENSOR", [112, 28, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 28, n, c % 28 // 4, h, w, c % 4]
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 14])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l59, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=9)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 28, 28), "float32"], placeholder_1: T.Buffer[(112, 28, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 7, 28, 4, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 7, 7, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 7, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 7, 1, 1, 1, 1, 1, 1, 7, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 7, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_0 * 2 + ax0)
                        out_channel = T.axis.spatial(7, i2_0 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 28 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i5_1, i6_0 in T.grid(1, 2):
                    for ax0, ax1 in T.grid(2, 1):
                        for ax2, ax3, ax4, ax5 in T.grid((i6_0 * 14 % 4 + 13) // 4 + 1, 4, 4, 14):
                            with T.block("data_vec"):
                                g = T.axis.spatial(4, i0_0 * 2 + ax0)
                                n = T.axis.spatial(1, ax1)
                                C = T.axis.spatial(7, i6_0 * 14 // 4 + ax2)
                                h = T.axis.spatial(28, i3_1 * 4 + ax3)
                                c = T.axis.spatial(4, ax4)
                                w = T.axis.spatial(28, i4_1 * 14 + ax5)
                                T.reads(placeholder[n, g * 28 + C * 4 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = placeholder[n, g * 28 + C * 4 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 2, 14, 1, 14, 1, 1, 2, 1, 1, 2, 1, 4):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(7, i2_0)
                            oh = T.axis.spatial(28, i3_1 * 4 + i3_2 * 2 + i3_3)
                            ow = T.axis.spatial(28, i4_1 * 14 + i4_2)
                            oc_block = T.axis.spatial(4, i5_3)
                            ic = T.axis.reduce(28, i6_0 * 14 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 112, 28, 28):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[ax1 // 28, ax0, ax1 % 28 // 4, ax2, ax3, ax1 % 4], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = conv[ax1 // 28, ax0, ax1 % 28 // 4, ax2, ax3, ax1 % 4] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 14, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 14])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v75 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v75)
l76 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l76, preserve_unit_loops=True)
l77 = sch.sample_compute_location(block=b1, decision=10)
sch.compute_at(block=b1, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l78, preserve_unit_loops=True)
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_layout_transform_2"
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], T_layout_trans: T.Buffer[(1, 1, 224, 224, 3), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 224, 224, 3):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 * 3 + ax4, ax2, ax3])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 3 + ax4 < 3 and ax2 < 224 and ax3 < 224, placeholder[ax0, ax1 * 3 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"
[16:59:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(6, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        T_add = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
            with T.block("data_pad"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 6, 112, 112, 4, 3, 3, 3):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                T.block_attr({"workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(6, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 3, 7, 112, 1, 1, 1, 16, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_2)
                    oh = T.axis.spatial(112, i2_0 * 16 + i2_1_1)
                    ow = T.axis.spatial(112, i3_0)
                    oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_3)
                    ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                    T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 6, 112, 112, 4):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4], placeholder_3[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4] + placeholder_3[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 2, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 16, 1, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[112, 1, 1, 1])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
l66 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l66, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(6, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 1, 226, 226, 3):
                with T.block("data_pad"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(data_pad[i0_1, i1_1, i2_1, i3_1, i4_1])
                    data_pad[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1 in T.grid(1, 3, 7, 112, 1, 1, 1, 16, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 3, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 16 + i2_1_1)
                        ow = T.axis.spatial(112, i3_0)
                        oc_block = T.axis.spatial(4, i4_1_1 * 2 + i4_3)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_0 * 16 + i2_1_1 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_1_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 2, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 16, 1, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[112, 1, 1, 1])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
b65, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b65, loop=l50, preserve_unit_loops=True)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
l67 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l67, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(6, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 3, 7, 112, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 16, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(6, i1_0 * 2 + i1_2)
                        oh = T.axis.spatial(112, i2_0 * 16 + i2_1)
                        ow = T.axis.spatial(112, i3_0)
                        oc_block = T.axis.spatial(4, i4_1 * 2 + i4_3)
                        ic, kh, kw = T.axis.remap("RRR", [i5_1, i6_1, i7_0])
                        T.reads(placeholder[n, ic // 3, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 225 and 1 <= ow * 2 + kw and ow * 2 + kw < 225, placeholder[n, ic // 3, oh * 2 + kh - 1, ow * 2 + kw - 1, ic % 3], T.float32(0), dtype="float32") * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 16, 1, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(112, i2_0 * 16 + ax2)
                        ax3_1 = T.axis.spatial(112, i3_0 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[3, 1, 2, 1])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[7, 16, 1, 1])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[112, 1, 1, 1])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 3])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 3])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
b65, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b65, loop=l49, preserve_unit_loops=True)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
l67 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l67, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_nn_max_pool2d"
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 6, 114, 114, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 114, 114, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(1 <= ax2 and ax2 < 113 and 1 <= ax3 and ax3 < 113, placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 6, 56, 56, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
    

[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 6, 56, 56, 4, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 6, 56, 56, 4, 1, 9):
                with T.block("tensor_rf"):
                    vi5_i6_fused_0 = T.axis.spatial(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    rv0 = T.axis.reduce(3, i5_i6_fused_1 // 3)
                    rv1 = T.axis.reduce(3, i5_i6_fused_1 % 3)
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 113 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 113, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
            for i0, i1, i2, i3, i4, i5_i6_fused_0 in T.grid(1, 6, 56, 56, 4, 1):
                with T.block("tensor"):
                    vi5_i6_fused_0 = T.axis.reduce(1, i5_i6_fused_0)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_0])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_rf = T.alloc_buffer([1, 6, 56, 56, 4, 9], dtype="float32")
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 6, 56, 56, 4, 1, 9):
                with T.block("tensor_rf"):
                    vi5_i6_fused_1 = T.axis.spatial(9, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3 - 1, ax3 * 2 + vi5_i6_fused_1 % 3 - 1, ax4])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(-3.4028234663852886e+38)
                    tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.max(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1], T.if_then_else(1 <= ax2 * 2 + vi5_i6_fused_1 // 3 and ax2 * 2 + vi5_i6_fused_1 // 3 < 113 and 1 <= ax3 * 2 + vi5_i6_fused_1 % 3 and ax3 * 2 + vi5_i6_fused_1 % 3 < 113, placeholder[ax0, ax1, ax2 * 2 + vi5_i6_fused_1 // 3 - 1, ax3 * 2 + vi5_i6_fused_1 % 3 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
            for i0, i1, i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(1, 6, 56, 56, 4, 1, 9):
                with T.block("tensor"):
                    vi5_i6_fused_1 = T.axis.reduce(9, i5_i6_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3, ax4 = T.axis.remap("SSSS", [i1, i2, i3, i4])
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    T.block_attr({"meta_schedule.random_compute_producer":True})
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 9])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
l17 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 112, 112, 4), "float32"], tensor: T.Buffer[(1, 6, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 6, 56, 56, 4, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3, ax4] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3, ax4] = T.max(tensor[ax0, ax1, ax2, ax3, ax4], T.if_then_else(1 <= ax2 * 2 + rv0 and ax2 * 2 + rv0 < 113 and 1 <= ax3 * 2 + rv1 and ax3 * 2 + rv1 < 113, placeholder[ax0, ax1, ax2 * 2 + rv0 - 1, ax3 * 2 + rv1 - 1, ax4], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_avg_pool2d_2"
[16:59:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 6, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 6, 58, 58, 4], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 6, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 58, 58, 4):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4])
                T.writes(pad_temp[ax0, ax1, ax2, ax3, ax4])
                pad_temp[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(1 <= ax2 and ax2 < 57 and 1 <= ax3 and ax3 < 57, placeholder[ax0, ax1, ax2 - 1, ax3 - 1, ax4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 6, 28, 28, 4, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, ax4, rv0, rv1 = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4])
                T.writes(tensor_1[ax0, ax1, ax2, ax3, ax4])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3, ax4] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] + pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 6, 28, 28, 4):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2 * 2 + 1, 55) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 55) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 6, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 6, 58, 58, 4], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 6, 28, 28, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 6, 28, 28, 4, 3], dtype="float32")
            for i0, i1 in T.grid(1, 6):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 57, 57, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1 + ax1)
                        ax2_1 = T.axis.spatial(58, ax2)
                        ax3_1 = T.axis.spatial(58, ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 57 and 1 <= ax3_1 and ax3_1 < 57, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1], T.float32(0), dtype="float32")
                for i2 in T.serial(28):
                    for ax0 in T.serial(3):
                        for ax0_2, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 28, 4, 3):
                            with T.block("tensor_rf"):
                                vi5_i6_fused_0 = T.axis.spatial(3, ax0 + ax0_2)
                                ax0_3 = T.axis.spatial(1, ax1)
                                ax1_2 = T.axis.spatial(6, i1 + ax2)
                                ax2_2 = T.axis.spatial(28, i2 + ax3)
                                ax3_2, ax4_2, vi5_i6_fused_1 = T.axis.remap("SSR", [ax4, ax5, ax6])
                                T.reads(pad_temp[ax0_3, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3_2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4_2])
                                T.writes(tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0])
                                with T.init():
                                    tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0] = T.float32(0)
                                tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0] = tensor_rf[ax0_3, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_0] + pad_temp[ax0_3, ax1_2, ax2_2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3_2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4_2]
                        for ax1_3, ax2_3, ax3_3, ax4_3, ax5 in T.grid(1, 1, 1, 28, 4):
                            with T.block("tensor"):
                                vi5_i6_fused_0, ax0_4 = T.axis.remap("RS", [ax0, ax1_3])
                                ax1_4 = T.axis.spatial(6, i1 + ax2_3)
                                ax2_4 = T.axis.spatial(28, i2 + ax3_3)
                                ax3_4, ax4_4 = T.axis.remap("SS", [ax4_3, ax5])
                                T.reads(tensor_rf[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4, vi5_i6_fused_0])
                                T.writes(tensor_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4])
                                with T.init():
                                    tensor_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4] = T.float32(0)
                                tensor_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4] = tensor_1[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4] + tensor_rf[ax0_4, ax1_4, ax2_4, ax3_4, ax4_4, vi5_i6_fused_0]
                    for i3, i4 in T.grid(28, 4):
                        with T.block("tensor_1"):
                            ax0_5, ax1_5, ax2_5, ax3_5, ax4_5 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                            T.reads(tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5])
                            T.writes(tensor[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5])
                            tensor[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] = tensor_1[ax0_5, ax1_5, ax2_5, ax3_5, ax4_5] / T.cast(T.max((T.min(ax2_5 * 2 + 1, 55) + 2 - T.max(1 - ax2_5 * 2, 0) - ax2_5 * 2) * (T.min(ax3_5 * 2 + 1, 55) + 2 - T.max(1 - ax3_5 * 2, 0) - ax3_5 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 3])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l13, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b17, decision=3)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l20, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 6, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            pad_temp = T.alloc_buffer([1, 6, 58, 58, 4], dtype="float32")
            tensor_1 = T.alloc_buffer([1, 6, 28, 28, 4], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 6, 28, 28, 4, 3], dtype="float32")
            for i0, i1 in T.grid(1, 6):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 57, 57, 4):
                    with T.block("pad_temp"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1 + ax1)
                        ax2_1 = T.axis.spatial(58, ax2)
                        ax3_1 = T.axis.spatial(58, ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1])
                        T.writes(pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        pad_temp[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.if_then_else(1 <= ax2_1 and ax2_1 < 57 and 1 <= ax3_1 and ax3_1 < 57, placeholder[ax0_1, ax1_1, ax2_1 - 1, ax3_1 - 1, ax4_1], T.float32(0), dtype="float32")
                for i2, i3, i4, i5_i6_fused_0, i5_i6_fused_1 in T.grid(28, 28, 4, 3, 3):
                    with T.block("tensor_rf"):
                        vi5_i6_fused_1 = T.axis.spatial(3, i5_i6_fused_1)
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3, ax4, vi5_i6_fused_0 = T.axis.remap("SSSSR", [i1, i2, i3, i4, i5_i6_fused_0])
                        T.reads(pad_temp[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4])
                        T.writes(tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1])
                        with T.init():
                            tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = T.float32(0)
                        tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] = tensor_rf[ax0, ax1, ax2, ax3, ax4, vi5_i6_fused_1] + pad_temp[ax0, ax1, ax2 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) // 3, ax3 * 2 + (vi5_i6_fused_0 * 3 + vi5_i6_fused_1) % 3, ax4]
            for i0, i1 in T.grid(1, 6):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(3, 1, 1, 28, 28, 4):
                    with T.block("tensor"):
                        vi5_i6_fused_1, ax0_2 = T.axis.remap("RS", [ax0, ax1])
                        ax1_2 = T.axis.spatial(6, i1 + ax2)
                        ax2_2, ax3_2, ax4_2 = T.axis.remap("SSS", [ax3, ax4, ax5])
                        T.reads(tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1])
                        T.writes(tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                        with T.init():
                            tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T.float32(0)
                        tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = tensor_1[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] + tensor_rf[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2, vi5_i6_fused_1]
                for i2, i3, i4 in T.grid(28, 28, 4):
                    with T.block("tensor_1"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T.writes(tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        tensor[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = tensor_1[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] / T.cast(T.max((T.min(ax2_3 * 2 + 1, 55) + 2 - T.max(1 - ax2_3 * 2, 0) - ax2_3 * 2) * (T.min(ax3_3 * 2 + 1, 55) + 2 - T.max(1 - ax3_3 * 2, 0) - ax3_3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
l10 = sch.fuse(l8, l9)
v11, v12 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 3])
l13, l14 = sch.split(loop=l10, factors=[v11, v12])
b15 = sch.rfactor(loop=l14, factor_axis=5)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v16 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v16)
b17, = sch.get_producers(block=b1)
sch.unannotate(block_or_loop=b1, ann_key="meta_schedule.random_compute_producer")
l18 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l18, preserve_unit_loops=True)
l19 = sch.sample_compute_location(block=b17, decision=-1)
sch.compute_at(block=b17, loop=l19, preserve_unit_loops=True)
l20 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l20, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 56, 56, 4), "float32"], tensor: T.Buffer[(1, 6, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 6, 28, 28, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 6, 28, 28, 4):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 1, 1, 1, 1, 3, 3):
                    with T.block("tensor"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(6, i1 + ax1)
                        ax2_1 = T.axis.spatial(28, i2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3 + ax3)
                        ax4_1 = T.axis.spatial(4, i4 + ax4)
                        rv0, rv1 = T.axis.remap("RR", [ax5, ax6])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0 - 1, ax3_1 * 2 + rv1 - 1, ax4_1])
                        T.writes(tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        with T.init():
                            tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.float32(0)
                        tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = tensor_1[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + T.if_then_else(1 <= ax2_1 * 2 + rv0 and ax2_1 * 2 + rv0 < 57 and 1 <= ax3_1 * 2 + rv1 and ax3_1 * 2 + rv1 < 57, placeholder[ax0_1, ax1_1, ax2_1 * 2 + rv0 - 1, ax3_1 * 2 + rv1 - 1, ax4_1], T.float32(0), dtype="float32")
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3, ax4])
                    T.writes(tensor[ax0, ax1, ax2, ax3, ax4])
                    tensor[ax0, ax1, ax2, ax3, ax4] = tensor_1[ax0, ax1, ax2, ax3, ax4] / T.cast(T.max((T.min(ax2 * 2 + 1, 55) + 2 - T.max(1 - ax2 * 2, 0) - ax2 * 2) * (T.min(ax3 * 2 + 1, 55) + 2 - T.max(1 - ax3 * 2, 0) - ax3 * 2), 1), "float32")
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="tensor", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
l5 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l5, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_layout_transform_concatenate_nn_relu"
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_layout_trans = T.alloc_buffer([1, 24, 28, 28], dtype="float32")
        T_concat = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 28, 28):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 24 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_layout_trans[ax0, ax1 - 112, ax2, ax3], placeholder_1[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(112 <= ax1, T_layout_trans[ax0, ax1 - 112, ax2, ax3], placeholder_1[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 6, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(1, 112, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_layout_trans = T.alloc_buffer([1, 24, 28, 28], dtype="float32")
            T_concat = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            for i0, i1 in T.grid(1, 136):
                for ax0, ax1, ax2 in T.grid(1, 1, 28):
                    for ax0_1, ax1_1, ax2_1, ax3 in T.grid(1, 1, 1, 28):
                        with T.block("T_layout_trans"):
                            ax0_2 = T.axis.spatial(1, ax0_1)
                            ax1_2 = T.axis.spatial(24, i1 - 112 + ax1_1)
                            ax2_2 = T.axis.spatial(28, ax2 + ax2_1)
                            ax3_1 = T.axis.spatial(28, ax3)
                            T.where(112 <= i1)
                            T.reads(placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_1, ax1_2 % 4])
                            T.writes(T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_1])
                            T_layout_trans[ax0_2, ax1_2, ax2_2, ax3_1] = T.if_then_else(ax0_2 < 1 and ax1_2 < 24 and ax2_2 < 28 and ax3_1 < 28, placeholder[ax0_2, ax1_2 // 4, ax2_2, ax3_1, ax1_2 % 4], T.float32(0), dtype="float32")
                    for ax3 in T.serial(28):
                        with T.block("T_concat"):
                            ax0_3 = T.axis.spatial(1, ax0)
                            ax1_3 = T.axis.spatial(136, i1 + ax1)
                            ax2_3, ax3_2 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(T_layout_trans[ax0_3, ax1_3 - 112, ax2_3, ax3_2], placeholder_1[ax0_3, ax1_3, ax2_3, ax3_2])
                            T.writes(T_concat[ax0_3, ax1_3, ax2_3, ax3_2])
                            T_concat[ax0_3, ax1_3, ax2_3, ax3_2] = T.if_then_else(112 <= ax1_3, T_layout_trans[ax0_3, ax1_3 - 112, ax2_3, ax3_2], placeholder_1[ax0_3, ax1_3, ax2_3, ax3_2], dtype="float32")
                for i2, i3 in T.grid(28, 28):
                    with T.block("T_relu"):
                        ax0_4, ax1_4, ax2_4, ax3_3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_concat[ax0_4, ax1_4, ax2_4, ax3_3])
                        T.writes(T_relu[ax0_4, ax1_4, ax2_4, ax3_3])
                        T_relu[ax0_4, ax1_4, ax2_4, ax3_3] = T.max(T_concat[ax0_4, ax1_4, ax2_4, ax3_3], T.float32(0))
    

b0 = sch.get_block(name="T_layout_trans", func_name="main")
b1 = sch.get_block(name="T_concat", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
l4 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
l5 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l5, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 28, 28, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 34, 28, 28, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 28, 28, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 34, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 34, 14, 4, 2, 1, 1, 2, 1, 1, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 9, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(34, i1_0 + ax1)
                        i2 = T.axis.spatial(30, i2_0 * 2 + i2_1 + i5_0 + ax2)
                        i3 = T.axis.spatial(30, i3_0 * 7 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(34, i1_0)
                        oh = T.axis.spatial(28, i2_0 * 2 + i2_1)
                        ow = T.axis.spatial(28, i3_0 * 7 + i3_2)
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 34, 28, 28, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[34, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 34, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 34, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 4, 30, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(34, i1_0 + ax1)
                        i2 = T.axis.spatial(30, i2_0 * 2 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(4, 2, 1, 1, 2, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_0)
                            oh = T.axis.spatial(28, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(28, i3_0 * 7 + i3_2)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 7, 2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(34, i1_0 + ax1)
                            ax2_1 = T.axis.spatial(28, i2_0 * 2 + i2_1 + ax2)
                            ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[34, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 34, 28, 28, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 34, 14, 4, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 2, 1, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 9, 2):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(34, i1_0 + ax1)
                            i2 = T.axis.spatial(30, i2_0 * 2 + i2_1 + i5_0 + ax2)
                            i3 = T.axis.spatial(30, i3_0 * 7 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_0)
                            oh = T.axis.spatial(28, i2_0 * 2 + i2_1)
                            ow = T.axis.spatial(28, i3_0 * 7 + i3_2)
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 2, 7, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(34, i1_0 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 2 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[34, 1, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 2, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_layout_transform_3"
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], T_layout_trans: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 136 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], T_layout_trans: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 136 and ax2 < 28 and ax3 < 28, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"
[16:59:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
        output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 28, 2, 28):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 2, 2):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 28, 28, 2, 34, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 17, 17, 1, 1, 2, 2):
                    with T.block("kernel_vec"):
                        g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                for i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(2, 1, 1, 1, 1, 1, 1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 1, 1, 2, 1, 17, 14, 14):
                        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 2, 2, 1):
                            with T.block("data_vec"):
                                g = T.axis.spatial(4, i0_2 * 2 + ax0)
                                n, C = T.axis.remap("SS", [ax1, ax2])
                                h = T.axis.spatial(28, i3_2 * 2 + ax3)
                                c = T.axis.spatial(2, ax4)
                                w = T.axis.spatial(28, i4_0 * 14 + i4_2 + ax5)
                                T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                        for i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 34, 1, 1, 2, 1, 1, 2, 1, 1):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_2 * 2 + i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(17, i2_2)
                                oh = T.axis.spatial(28, i3_2 * 2 + i3_3)
                                ow = T.axis.spatial(28, i4_0 * 14 + i4_2)
                                oc_block, ic = T.axis.remap("SR", [i5_0, i6_1])
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 28, 14, 1):
                        with T.block("conv_global"):
                            v0, v1, v2, v3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                            v4 = T.axis.spatial(28, i4_0 * 14 + ax4)
                            v5 = T.axis.spatial(2, i5_0 + ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1 in T.grid(1, 136):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 28):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                for i2, i3 in T.grid(28, 28):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 34])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l62, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=19)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 2, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 17):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 28, 2, 14):
                        with T.block("data_vec"):
                            g = T.axis.spatial(4, i0_2 * 2 + ax0)
                            n, C, h, c = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4])
                            w = T.axis.spatial(28, i4_0 * 14 + ax5)
                            T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                    for i3_2, i4_2 in T.grid(14, 14):
                        for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 17, 1, 1, 2, 1):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(4, i0_2 * 2 + ax0)
                                out_channel = T.axis.spatial(17, i2_2 + ax1)
                                in_channel, h, w, ci = T.axis.remap("SSSS", [ax2, ax3, ax4, ax5])
                                co = T.axis.spatial(2, i5_0 + ax6)
                                T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                        for i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 34, 1, 1, 2, 1, 1, 2, 1, 1):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_2 * 2 + i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(17, i2_2)
                                oh = T.axis.spatial(28, i3_2 * 2 + i3_3)
                                ow = T.axis.spatial(28, i4_0 * 14 + i4_2)
                                oc_block, ic = T.axis.remap("SR", [i5_0, i6_1])
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 28, 14, 1):
                    with T.block("conv_global"):
                        v0, v1, v2, v3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        v4 = T.axis.spatial(28, i4_0 * 14 + ax4)
                        v5 = T.axis.spatial(2, i5_0 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[ax1 // 34, ax0, ax1 % 34 // 2, ax2, ax3, ax1 % 2], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(conv[ax1 // 34, ax0, ax1 % 34 // 2, ax2, ax3, ax1 % 2] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 34])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=19)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=17)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 28, 2, 14):
                    with T.block("data_vec"):
                        g, n, C, h, c = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        w = T.axis.spatial(28, i4_0 * 14 + ax5)
                        T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                for i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 17):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 17, 1, 1, 2, 1):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_2 * 2 + ax0)
                            out_channel = T.axis.spatial(17, i2_2 + ax1)
                            in_channel, h, w, ci = T.axis.remap("SSSS", [ax2, ax3, ax4, ax5])
                            co = T.axis.spatial(2, i5_0 + ax6)
                            T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                    for i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(14, 14, 1, 34, 1, 1, 2, 1, 1, 2, 1, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_2 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(17, i2_2)
                            oh = T.axis.spatial(28, i3_2 * 2 + i3_3)
                            ow = T.axis.spatial(28, i4_0 * 14 + i4_2)
                            oc_block, ic = T.axis.remap("SR", [i5_0, i6_1])
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h = T.axis.spatial(28, i2 + ax2)
                        w = T.axis.spatial(28, i3 + ax3)
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 2])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 14, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 34])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=17)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"
[16:59:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
        output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 28, 2, 28):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 2, 2):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 28, 28, 2, 34, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 7, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 17, 17, 1, 1, 2, 1):
                    with T.block("kernel_vec"):
                        g, out_channel, in_channel, h, w, ci = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                        co = T.axis.spatial(2, i5_0 + ax6)
                        T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 4, 2, 28):
                    with T.block("data_vec"):
                        g, n, C = T.axis.remap("SSS", [ax0, ax1, ax2])
                        h = T.axis.spatial(28, i3_0 * 4 + ax3)
                        c, w = T.axis.remap("SS", [ax4, ax5])
                        T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(2, 1, 1, 1, 28, 1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(34, 1, 1, 2, 1, 17, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(17, i2_2)
                            oh = T.axis.spatial(28, i3_0 * 4 + i3_2)
                            ow, oc_block, ic = T.axis.remap("SSR", [i4_1, i5_0, i6_0])
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 4, 1, 1):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_1 * 2 + ax0)
                            v1, v2 = T.axis.remap("SS", [ax1, ax2])
                            v3 = T.axis.spatial(28, i3_0 * 4 + ax3)
                            v4 = T.axis.spatial(28, i4_1 + ax4)
                            v5 = T.axis.spatial(2, i5_0 + ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[34, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 28, 2, 28):
                with T.block("data_vec"):
                    g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                    T.writes(data_vec[g, n, C, h, c, w])
                    data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 2, 2):
                with T.block("kernel_vec"):
                    g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 7, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 1, 28, 1, 34, 1, 1, 2, 1, 17, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(17, i2_2)
                        oh = T.axis.spatial(28, i3_0 * 4 + i3_2)
                        ow, oc_block, ic = T.axis.remap("SSR", [i4_1, i5_0, i6_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 4, 28, 1):
                    with T.block("conv_global"):
                        v0, v1, v2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        v3 = T.axis.spatial(28, i3_0 * 4 + ax3)
                        v4 = T.axis.spatial(28, ax4)
                        v5 = T.axis.spatial(2, i5_0 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h = T.axis.spatial(28, i2 + ax2)
                        w = T.axis.spatial(28, i3 + ax3)
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[34, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 1, 7, 1, 2, 2, 1, 1, 1, 28, 1, 34):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 17, 1, 1, 1, 1, 1):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        out_channel = T.axis.spatial(17, ax1)
                        in_channel = T.axis.spatial(17, i6_0 // 2 + ax2)
                        h, w = T.axis.remap("SS", [ax3, ax4])
                        ci = T.axis.spatial(2, i6_0 % 2 + ax5)
                        co = T.axis.spatial(2, i5_0 + ax6)
                        T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 4, 1, 1):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        n = T.axis.spatial(1, ax1)
                        C = T.axis.spatial(17, i6_0 // 2 + ax2)
                        h = T.axis.spatial(28, i3_0 * 4 + ax3)
                        c = T.axis.spatial(2, i6_0 % 2 + ax4)
                        w = T.axis.spatial(28, i4_1 + ax5)
                        T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 2, 1, 17, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(17, i2_2)
                        oh = T.axis.spatial(28, i3_0 * 4 + i3_2)
                        ow, oc_block, ic = T.axis.remap("SSR", [i4_1, i5_0, i6_0])
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h = T.axis.spatial(28, i2 + ax2)
                        w = T.axis.spatial(28, i3 + ax3)
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 17, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 28, 1, 1])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[34, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_reshape_transpose_reshape_layout_transform_1"
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 34, 28, 28):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28]
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 4, 28, 28):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 136, 28, 28):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4, ((ax3 // 28 + ax2) // 28 + ax1) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 28 + ax2) // 28 + ax1) % 136 // 4, ((ax3 // 28 + ax2) // 28 + ax1) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28]
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 28, 28, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 136 and ax2 < 28 and ax3 < 28, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
            T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
            for i0, i1, i2 in T.grid(1, 34, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 28, 28):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(4, i2 + ax1)
                        ax2_1 = T.axis.spatial(34, i1 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[0, (ax1_1 * 34 + (ax4_1 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4_1 // 28 + ax3_1) % 28, ax4_1 % 28])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_1 * 34 + (ax4_1 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4_1 // 28 + ax3_1) % 28, ax4_1 % 28]
                for i3, i4 in T.grid(28, 28):
                    with T.block("T_transpose"):
                        ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape[ax0, ax2, ax1, ax3, ax4])
                        T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                        T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax2, ax1, ax3, ax4]
            for i0, i1, i2, i3, i4 in T.grid(1, 34, 28, 28, 4):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_transpose[0, (ax1 * 4 + (ax3 // 28 + ax2) // 28 + ax4) % 136 // 4, ((ax3 // 28 + ax2) // 28 + ax4) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 136 and ax2 < 28 and ax3 < 28, T_transpose[0, ((ax3 // 28 + ax2) // 28 + (ax1 * 4 + ax4)) % 136 // 4, ((ax3 // 28 + ax2) // 28 + (ax1 * 4 + ax4)) % 4, (ax3 // 28 + ax2) % 28, ax3 % 28], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"
[16:59:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 30, 30, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 34, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 34, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 34, 30, 30, 4):
                with T.block("PaddedInput"):
                    i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                    T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                    PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1_1, i1_1_1, i2_1_1, i3_1_1, i4_1_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 1, 1, 34, 7, 7, 1, 1, 3, 1, 1, 2, 1, 4):
                with T.block("DepthwiseConv2d"):
                    b = T.axis.spatial(1, 0)
                    oco = T.axis.spatial(34, i1_2)
                    oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                    oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_0, i6_1])
                    T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                    T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                    with T.init():
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 34, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 34, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 34, 7, 7, 1, 1, 3, 1, 1, 2, 1, 4):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(34, i1_2)
                        oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                        oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_0, i6_1])
                        T.reads(placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + T.if_then_else(1 <= oh * 2 + kh and oh * 2 + kh < 29 and 1 <= ow * 2 + kw and ow * 2 + kw < 29, placeholder[b, oci // 4 + oco, oh * 2 + kh - 1, ow * 2 + kw - 1, oci % 4], T.float32(0), dtype="float32") * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 14, 7, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 34, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0 in T.grid(1, 1, 1, 1, 1, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 27, 15, 4):
                        with T.block("PaddedInput"):
                            i0, i1 = T.axis.remap("SS", [ax0, ax1])
                            i2 = T.axis.spatial(30, i5_0 + ax2)
                            i3 = T.axis.spatial(30, i3_0 * 14 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 34, 7, 7, 1, 1, 3, 1, 1, 2, 1, 4):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_2)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_0 * 7 + i3_2)
                            oci, kh, kw = T.axis.remap("SRR", [i4_3, i5_0, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 14, 7, 4):
                    with T.block("T_add"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(14, i3_0 * 7 + ax3)
                        ax4_1 = T.axis.spatial(4, ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 34, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[2, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_layout_transform_4"
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 14, 14, 4), "float32"], T_layout_trans: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 136 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 14, 14, 4), "float32"], T_layout_trans: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 136 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_nn_conv2d_multiply_add_1"
[16:59:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 14, 2, 14], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
        output_unpack = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 14, 2, 14):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 2, 2):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 14, 14, 2, 34, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 2, 14], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1 in T.grid(1, 1, 1, 1, 7, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 17, 17, 1, 1, 2, 2):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSS", [ax1, ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 14, 2, 2):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        n, C, h, c = T.axis.remap("SSSS", [ax1, ax2, ax3, ax4])
                        w = T.axis.spatial(14, i4_0 * 2 + ax5)
                        T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                for i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 1, 1, 1, 1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 2, 1, 1, 14, 1, 2, 17, 1, 1, 1, 1, 17, 1, 2, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh = T.axis.remap("SS", [i2_3, i3_2])
                            ow = T.axis.spatial(14, i4_0 * 2 + i4_3)
                            oc_block = T.axis.spatial(2, i5_2)
                            ic = T.axis.reduce(34, i6_0 * 17 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 14, 2, 2):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_1 * 2 + ax0)
                            v1, v2, v3 = T.axis.remap("SSS", [ax1, ax2, ax3])
                            v4 = T.axis.spatial(14, i4_0 * 2 + ax4)
                            v5 = T.axis.spatial(2, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
            for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 17])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=6)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 2, 14], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 7, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(2, 1, 1, 1, 1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 17, 9, 1, 1, 2, 2):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_1 * 2 + ax0)
                            out_channel = T.axis.spatial(17, ax1)
                            in_channel = T.axis.spatial(17, i6_0 * 17 // 2 + ax2)
                            h, w, ci, co = T.axis.remap("SSSS", [ax3, ax4, ax5, ax6])
                            T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 9, 14, 2, 2):
                        with T.block("data_vec"):
                            g = T.axis.spatial(4, i0_1 * 2 + ax0)
                            n = T.axis.spatial(1, ax1)
                            C = T.axis.spatial(17, i6_0 * 17 // 2 + ax2)
                            h, c = T.axis.remap("SS", [ax3, ax4])
                            w = T.axis.spatial(14, i4_0 * 2 + ax5)
                            T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 2, 1, 1, 14, 1, 2, 17, 1, 1, 1, 1, 17, 1, 2, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh = T.axis.remap("SS", [i2_3, i3_2])
                            ow = T.axis.spatial(14, i4_0 * 2 + i4_3)
                            oc_block = T.axis.spatial(2, i5_2)
                            ic = T.axis.reduce(34, i6_0 * 17 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 14, 2, 2):
                    with T.block("conv_global"):
                        v0, v1, v2, v3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        v4 = T.axis.spatial(14, i4_0 * 2 + ax4)
                        v5 = T.axis.spatial(2, ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1 in T.grid(1, 136):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                for i2, i3 in T.grid(14, 14):
                    with T.block("T_add"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_add[ax0, ax1, ax2, ax3])
                        T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 17])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l59, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], T_add: T.Buffer[(1, 136, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 2, 14], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 2, 2], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 2], dtype="float32")
            output_unpack = T.alloc_buffer([1, 136, 14, 14], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 14, 2, 14):
                with T.block("data_vec"):
                    g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                    T.writes(data_vec[g, n, C, h, c, w])
                    data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
            for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 2, 2):
                with T.block("kernel_vec"):
                    g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                    T.reads(placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 34 + out_channel * 2 + co, in_channel * 2 + ci, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 7, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 14, 1, 2, 17, 1, 1, 1, 1, 17, 1, 2, 1):
                with T.block("conv"):
                    g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                    n = T.axis.spatial(1, 0)
                    oc_chunk, oh = T.axis.remap("SS", [i2_3, i3_2])
                    ow = T.axis.spatial(14, i4_0 * 2 + i4_3)
                    oc_block = T.axis.spatial(2, i5_2)
                    ic = T.axis.reduce(34, i6_0 * 17 + i6_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block])
                    T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * kernel_vec[g, oc_chunk, ic // 2, kh, kw, ic % 2, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 136, 14, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(136, i1 + ax1)
                        h = T.axis.spatial(14, i2 + ax2)
                        w = T.axis.spatial(14, i3 + ax3)
                        T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 14, 14], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 2])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 2, 1])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[2, 17])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v75 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v75)
l76 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l76, preserve_unit_loops=True)
l77 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l78, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_concatenate_nn_relu"
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 136, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(136 <= ax1, placeholder_1[ax0, ax1 - 136, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 14, 14), "float32"], placeholder_1: T.Buffer[(1, 136, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_concat = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            for i0, i1 in T.grid(1, 272):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                    with T.block("T_concat"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(272, i1 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        T.reads(placeholder_1[ax0_1, ax1_1 - 136, ax2_1, ax3_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1])
                        T.writes(T_concat[ax0_1, ax1_1, ax2_1, ax3_1])
                        T_concat[ax0_1, ax1_1, ax2_1, ax3_1] = T.if_then_else(136 <= ax1_1, placeholder_1[ax0_1, ax1_1 - 136, ax2_1, ax3_1], placeholder[ax0_1, ax1_1, ax2_1, ax3_1], dtype="float32")
                for i2, i3 in T.grid(14, 14):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(T_concat[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 68, 14, 14, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 68, 14, 14, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 14, 14, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 14, 14, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 16, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(14, 1, 2, 1, 2, 1, 14, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 1, 17, 1, 1, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(68, i1_0 * 34 + i1_1 * 17 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                        kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 68, 14, 14, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 2, 14):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 3, 16, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        i2 = T.axis.spatial(16, i2_0 + ax2)
                        i3, i4 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 2, 1, 14, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 1, 1, 2, 3, 3, 1, 17, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(68, i1_0 * 34 + i1_1 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 17, 1, 1, 2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(68, i1_0 * 34 + i1_1 * 17 + ax1)
                            ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                            ax3_1 = T.axis.spatial(14, i3_1 + ax3)
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 14, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2 in T.grid(1, 2, 1, 14, 1, 1, 1, 1, 1, 1, 1, 2):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 17, 3, 3, 1):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(68, i1_0 * 34 + i1_1 * 17 + ax1)
                            i2 = T.axis.spatial(16, i2_0 + ax2)
                            i3 = T.axis.spatial(16, i3_1 + ax3)
                            i4 = T.axis.spatial(4, i4_0 * 2 + i4_2 + ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 17, 1, 1, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(68, i1_0 * 34 + i1_1 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_0, i3_1])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_1, i6_1])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 1, 14, 2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        ax2_1 = T.axis.spatial(14, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(14, ax3)
                        ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[14, 1, 1, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 3])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=16)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_layout_transform_5"
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], T_layout_trans: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 272 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], T_layout_trans: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 272 and ax2 < 14 and ax3 < 14, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[16:59:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 14, 4, 14):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 14, 14, 4, 68, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 17, 17, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 1, 1, 17, 1, 7, 1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(4, 1, 1, 4, 1, 1, 14, 1, 4, 17, 1, 1, 1, 1, 1, 1, 1, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh = T.axis.remap("SS", [i2_1, i3_2])
                            ow = T.axis.spatial(14, i4_0 * 7 + i4_1)
                            oc_block = T.axis.spatial(4, i5_2)
                            ic = T.axis.reduce(68, i6_0 * 17 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(placeholder[n, g * 68 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 68 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 14, 1, 4):
                        with T.block("conv_global"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(17, i2_1 + ax2)
                            v3 = T.axis.spatial(14, ax3)
                            v4 = T.axis.spatial(14, i4_0 * 7 + i4_1 + ax4)
                            v5 = T.axis.spatial(4, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2 in T.grid(1, 272, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(14, i2 + ax2)
                        w = T.axis.spatial(14, ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                for i3 in T.serial(14):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 17, 1, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[4, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l62, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 2, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 17, 1, 7, 1, 4, 1, 1, 4, 1, 1, 14, 1, 4, 17, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_2)
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh = T.axis.remap("SS", [i2_1, i3_2])
                        ow = T.axis.spatial(14, i4_0 * 7 + i4_1)
                        oc_block = T.axis.spatial(4, i5_2)
                        ic = T.axis.reduce(68, i6_0 * 17 + i6_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, g * 68 + ic, oh + kh, ow + kw], placeholder_1[g * 68 + oc_chunk * 4 + oc_block, ic, kh, kw])
                        T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 68 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * placeholder_1[g * 68 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 14, 7, 4):
                    with T.block("conv_global"):
                        v0, v1, v2, v3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                        v4 = T.axis.spatial(14, i4_0 * 7 + ax4)
                        v5 = T.axis.spatial(4, ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(14, i2 + ax2)
                        w = T.axis.spatial(14, i3 + ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 17, 1, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[4, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 17):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 14, 4, 7):
                    with T.block("data_vec"):
                        g, n, C, h, c = T.axis.remap("SSSSS", [ax0, ax1, ax2, ax3, ax4])
                        w = T.axis.spatial(14, i4_0 * 7 + ax5)
                        T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                for i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 7, 1, 4, 1, 1, 4, 1, 1, 14, 1, 4, 17, 1, 1, 1, 1, 1, 1, 1, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_2)
                        n = T.axis.spatial(1, 0)
                        oc_chunk, oh = T.axis.remap("SS", [i2_1, i3_2])
                        ow = T.axis.spatial(14, i4_0 * 7 + i4_1)
                        oc_block = T.axis.spatial(4, i5_2)
                        ic = T.axis.reduce(68, i6_0 * 17 + i6_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], placeholder_1[g * 68 + oc_chunk * 4 + oc_block, ic, kh, kw])
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * placeholder_1[g * 68 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(14, i2 + ax2)
                        w = T.axis.spatial(14, i3 + ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 17, 1, 1])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 4, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[4, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"
[16:59:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 14, 4, 14):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 14, 14, 4, 68, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 1, 17):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(4, 1, 17, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, ax0)
                        out_channel = T.axis.spatial(17, i2_0 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 4, 4, 1, 1, 2, 2):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 17, 7, 4, 7):
                        with T.block("data_vec"):
                            g = T.axis.spatial(4, i0_1 + ax0)
                            n, C = T.axis.remap("SS", [ax1, ax2])
                            h = T.axis.spatial(14, i3_1 * 7 + ax3)
                            c = T.axis.spatial(4, ax4)
                            w = T.axis.spatial(14, i4_1 * 7 + ax5)
                            T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                    for i5_1 in T.serial(1):
                        for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 34, 1, 1, 1, 1, 1, 7, 7, 1):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_1)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(17, i2_0)
                                oh = T.axis.spatial(14, i3_1 * 7 + i3_3)
                                ow = T.axis.spatial(14, i4_1 * 7 + i4_3)
                                oc_block = T.axis.spatial(4, i5_0)
                                ic = T.axis.reduce(68, i6_0 * 34 + i6_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                        for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 1, 7, 7, 1):
                            with T.block("conv_global"):
                                v0 = T.axis.spatial(4, i0_1 + ax0)
                                v1 = T.axis.spatial(1, ax1)
                                v2 = T.axis.spatial(17, i2_0 + ax2)
                                v3 = T.axis.spatial(14, i3_1 * 7 + ax3)
                                v4 = T.axis.spatial(14, i4_1 * 7 + ax4)
                                v5 = T.axis.spatial(4, i5_0 + ax5)
                                T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                                T.writes(conv[v0, v1, v2, v3, v4, v5])
                                conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(14, i2 + ax2)
                        w = T.axis.spatial(14, i3 + ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 34])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 17, 1, 1, 4):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(4, 1, 1, 2, 2, 1, 2):
                    for ax0, ax1 in T.grid(1, 1):
                        for ax2, ax3, ax4, ax5, ax6 in T.grid((i6_0 * 34 % 4 + 33) // 4 + 1, 1, 1, 4, 1):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(4, i0_1 + ax0)
                                out_channel = T.axis.spatial(17, i2_0 + ax1)
                                in_channel = T.axis.spatial(17, i6_0 * 34 // 4 + ax2)
                                h, w, ci = T.axis.remap("SSS", [ax3, ax4, ax5])
                                co = T.axis.spatial(4, i5_0 + ax6)
                                T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                    for ax0, ax1 in T.grid(1, 1):
                        for ax2, ax3, ax4, ax5 in T.grid((i6_0 * 34 % 4 + 33) // 4 + 1, 7, 4, 7):
                            with T.block("data_vec"):
                                g = T.axis.spatial(4, i0_1 + ax0)
                                n = T.axis.spatial(1, ax1)
                                C = T.axis.spatial(17, i6_0 * 34 // 4 + ax2)
                                h = T.axis.spatial(14, i3_1 * 7 + ax3)
                                c = T.axis.spatial(4, ax4)
                                w = T.axis.spatial(14, i4_1 * 7 + ax5)
                                T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                                T.writes(data_vec[g, n, C, h, c, w])
                                data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 34, 1, 1, 1, 1, 1, 7, 7, 1):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(17, i2_0)
                            oh = T.axis.spatial(14, i3_1 * 7 + i3_3)
                            ow = T.axis.spatial(14, i4_1 * 7 + i4_3)
                            oc_block = T.axis.spatial(4, i5_0)
                            ic = T.axis.reduce(68, i6_0 * 34 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 14, 14, 1):
                    with T.block("conv_global"):
                        v0, v1 = T.axis.remap("SS", [ax0, ax1])
                        v2 = T.axis.spatial(17, i2_0 + ax2)
                        v3, v4 = T.axis.remap("SS", [ax3, ax4])
                        v5 = T.axis.spatial(4, i5_0 + ax5)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[ax1 // 68, ax0, ax1 % 68 // 4, ax2, ax3, ax1 % 4], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(conv[ax1 // 68, ax0, ax1 % 68 // 4, ax2, ax3, ax1 % 4] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 34])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 17, 1, 1, 4, 4, 1, 1, 2, 2, 1, 2):
                for ax0, ax1 in T.grid(1, 1):
                    for ax2, ax3, ax4, ax5, ax6 in T.grid((i6_0 * 34 % 4 + 33) // 4 + 1, 1, 1, 4, 1):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_1 + ax0)
                            out_channel = T.axis.spatial(17, i2_0 + ax1)
                            in_channel = T.axis.spatial(17, i6_0 * 34 // 4 + ax2)
                            h, w, ci = T.axis.remap("SSS", [ax3, ax4, ax5])
                            co = T.axis.spatial(4, i5_0 + ax6)
                            T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 34, 1, 1, 1, 1, 1, 7, 7, 1):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_1)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(17, i2_0)
                        oh = T.axis.spatial(14, i3_1 * 7 + i3_3)
                        ow = T.axis.spatial(14, i4_1 * 7 + i4_3)
                        oc_block = T.axis.spatial(4, i5_0)
                        ic = T.axis.reduce(68, i6_0 * 34 + i6_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, g * 68 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 68 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1 in T.grid(1, 272):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h, w = T.axis.remap("SS", [ax2, ax3])
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                for i2, i3 in T.grid(14, 14):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[2, 34])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #26: "fused_reshape_transpose_reshape_layout_transform_2"
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], T_layout_trans: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 68, 14, 14], dtype="float32")
        T_transpose = T.alloc_buffer([1, 68, 4, 14, 14], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 68, 14, 14):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 68 + (ax4 // 14 + ax3) // 14 + ax2) % 272, (ax4 // 14 + ax3) % 14, ax4 % 14])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 68 + (ax4 // 14 + ax3) // 14 + ax2) % 272, (ax4 // 14 + ax3) % 14, ax4 % 14]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 4, 14, 14):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 272, 14, 14):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4, ((ax3 // 14 + ax2) // 14 + ax1) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 14 + ax2) // 14 + ax1) % 272 // 4, ((ax3 // 14 + ax2) // 14 + ax1) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 14, 14, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 272 and ax2 < 14 and ax3 < 14, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], T_layout_trans: T.Buffer[(1, 68, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_transpose = T.alloc_buffer([1, 68, 4, 14, 14], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 68, 14, 14, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(68, i1 + ax1)
                        ax2_1 = T.axis.spatial(4, i4 + ax2)
                        ax3_1 = T.axis.spatial(14, i2 + ax3)
                        ax4_1 = T.axis.spatial(14, i3 + ax4)
                        T.reads(placeholder[0, (ax2_1 * 68 + (ax4_1 // 14 + ax3_1) // 14 + ax1_1) % 272, (ax4_1 // 14 + ax3_1) % 14, ax4_1 % 14])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax2_1 * 68 + (ax4_1 // 14 + ax3_1) // 14 + ax1_1) % 272, (ax4_1 // 14 + ax3_1) % 14, ax4_1 % 14]
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(T_transpose[0, (ax1 * 4 + (ax3 // 14 + ax2) // 14 + ax4) % 272 // 4, ((ax3 // 14 + ax2) // 14 + ax4) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 272 and ax2 < 14 and ax3 < 14, T_transpose[0, ((ax3 // 14 + ax2) // 14 + (ax1 * 4 + ax4)) % 272 // 4, ((ax3 // 14 + ax2) // 14 + (ax1 * 4 + ax4)) % 4, (ax3 // 14 + ax2) % 14, ax3 % 14], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"
[16:59:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 16, 16, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 68, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 3, 3):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 13, 13, 1):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        i2 = T.axis.spatial(16, i5_0 + ax2)
                        i3 = T.axis.spatial(16, i6_0 + ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 1, 1, 1, 17, 1, 7, 1):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(68, i1_0 * 34 + i1_2 * 17 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                        oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 68, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 15, 15, 2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, i4_0 * 2 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 2):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 2, 7, 1, 1, 1, 1, 1, 17, 1, 7, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(68, i1_0 * 34 + i1_2 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 7, 7, 1):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                            ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + i4_1 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 14, 14, 4), "float32"], placeholder_1: T.Buffer[(68, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 68, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 68, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 68, 16, 16, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 68, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 15, 15, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                        i2 = T.axis.spatial(16, ax2)
                        i3 = T.axis.spatial(16, ax3)
                        i4 = T.axis.spatial(4, ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 15 and 1 <= i3 and i3 < 15, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0 in T.grid(1, 1, 2):
                    for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 2, 3, 3, 1, 2, 7, 1, 1, 1, 1, 1, 17, 1, 7, 1):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(68, i1_0 * 34 + i1_2 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_2, i3_3])
                            oci = T.axis.spatial(4, i4_0 * 2 + i4_1)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 68, 14, 14, 4], "float32"], ["TENSOR", [68, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 7, 7, 2):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(68, i1_0 * 34 + ax1)
                            ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                            ax4_1 = T.axis.spatial(4, i4_0 * 2 + ax4)
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 1, 2, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #28: "fused_layout_transform_6"
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_layout_trans: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 272 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 68, 7, 7, 4), "float32"], T_layout_trans: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 272 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #29: "fused_nn_conv2d_multiply_add_2"
[16:59:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 7, 4, 7], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 7, 4, 7):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 17, 17, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 17, 7, 7, 4, 68, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
    

[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 17, 7, 4, 7):
                with T.block("data_vec"):
                    g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                    T.writes(data_vec[g, n, C, h, c, w])
                    data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1 in T.grid(1, 1, 17, 7, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 17, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        out_channel = T.axis.spatial(17, i2_0 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 1, 1, 7, 1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(34, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i2_0, i3_0, i4_1, i5_3])
                            ic = T.axis.reduce(68, i6_0 * 2 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 1, 1, 4):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_1 * 2 + ax0)
                            v1 = T.axis.spatial(1, ax1)
                            v2 = T.axis.spatial(17, i2_0 + ax2)
                            v3 = T.axis.spatial(7, i3_0 + ax3)
                            v4 = T.axis.spatial(7, i4_1 + ax4)
                            v5 = T.axis.spatial(4, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(7, i2 + ax2)
                        w = T.axis.spatial(7, i3 + ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[34, 2])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0 in T.grid(1, 1, 17):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 17, 7, 4, 7):
                    with T.block("data_vec"):
                        g, n, C, h, c, w = T.axis.remap("SSSSSS", [ax0, ax1, ax2, ax3, ax4, ax5])
                        T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                for i3_0, i4_0, i5_0 in T.grid(7, 1, 1):
                    for i0_1 in T.serial(2):
                        for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 17, 1, 1, 4, 4):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(4, i0_1 * 2 + ax0)
                                out_channel = T.axis.spatial(17, i2_0 + ax1)
                                in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                                T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                        for i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 7, 1, 34, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                                n = T.axis.spatial(1, 0)
                                oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i2_0, i3_0, i4_1, i5_3])
                                ic = T.axis.reduce(68, i6_0 * 2 + i6_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 1, 1, 7, 4):
                        with T.block("conv_global"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(17, i2_0 + ax2)
                            v3 = T.axis.spatial(7, i3_0 + ax3)
                            v4, v5 = T.axis.remap("SS", [ax4, ax5])
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[34, 2])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
b75 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b75, loop=l59, preserve_unit_loops=True)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 17, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 17, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1 in T.grid(1, 1, 17, 7, 1, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 1, 17, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        out_channel = T.axis.spatial(17, i2_0 + ax1)
                        in_channel, h, w, ci, co = T.axis.remap("SSSSS", [ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i1_1, i2_1, i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 1, 7, 1, 34):
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 1, 1, 2, 1):
                        with T.block("data_vec"):
                            g = T.axis.spatial(4, i0_1 * 2 + ax0)
                            n = T.axis.spatial(1, ax1)
                            C = T.axis.spatial(17, i6_0 // 2 + ax2)
                            h = T.axis.spatial(7, i3_0 + ax3)
                            c = T.axis.spatial(4, i6_0 % 2 * 2 + ax4)
                            w = T.axis.spatial(7, i4_1 + ax5)
                            T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                            T.writes(data_vec[g, n, C, h, c, w])
                            data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_2)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh, ow, oc_block = T.axis.remap("SSSS", [i2_0, i3_0, i4_1, i5_3])
                            ic = T.axis.reduce(68, i6_0 * 2 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2 in T.grid(1, 272, 7):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(272, i1 + ax1)
                        h = T.axis.spatial(7, i2 + ax2)
                        w = T.axis.spatial(7, ax3)
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                for i3 in T.serial(7):
                    with T.block("T_add"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_add[ax0, ax1, ax2, ax3])
                        T_add[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l6, l7, l8, l9, l10, l11, l12, l13, l14 = sch.get_loops(block=b2)
v15, v16, v17, v18 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l19, l20, l21, l22 = sch.split(loop=l6, factors=[v15, v16, v17, v18])
v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l27, l28, l29, l30 = sch.split(loop=l7, factors=[v23, v24, v25, v26])
v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[17, 1, 1, 1])
l35, l36, l37, l38 = sch.split(loop=l8, factors=[v31, v32, v33, v34])
v39, v40, v41, v42 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l43, l44, l45, l46 = sch.split(loop=l9, factors=[v39, v40, v41, v42])
v47, v48, v49, v50 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l51, l52, l53, l54 = sch.split(loop=l10, factors=[v47, v48, v49, v50])
v55, v56, v57, v58 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l59, l60, l61, l62 = sch.split(loop=l11, factors=[v55, v56, v57, v58])
v63, v64 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[34, 2])
l65, l66 = sch.split(loop=l12, factors=[v63, v64])
v67, v68 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[1, 1])
l69, l70 = sch.split(loop=l13, factors=[v67, v68])
v71, v72 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l73, l74 = sch.split(loop=l14, factors=[v71, v72])
sch.reorder(l19, l27, l35, l43, l51, l59, l20, l28, l36, l44, l52, l60, l65, l69, l73, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.vectorize", ann_val=64)
v75 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v75)
l76 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l76, preserve_unit_loops=True)
l77 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b0, decision=12)
sch.compute_at(block=b0, loop=l78, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #30: "fused_concatenate_nn_relu_1"
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 272, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_concat = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_concat"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                T.writes(T_concat[ax0, ax1, ax2, ax3])
                T_concat[ax0, ax1, ax2, ax3] = T.if_then_else(272 <= ax1, placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_concat[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_concat[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1, 272, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(T.if_then_else(272 <= ax1, placeholder_1[ax0, ax1 - 272, ax2, ax3], placeholder[ax0, ax1, ax2, ax3], dtype="float32"), T.float32(0))
    

b0 = sch.get_block(name="T_concat", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 34, 7, 4, 7], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 34, 7, 4, 7):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 136 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 136 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 34, 34, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 34, 7, 7, 4, 136, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 34, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 7, 1, 1, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 34, 1, 4, 7):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        n, C = T.axis.remap("SS", [ax1, ax2])
                        h = T.axis.spatial(7, i3_0 + ax3)
                        c, w = T.axis.remap("SS", [ax4, ax5])
                        T.reads(placeholder[n, g * 136 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 136 + C * 4 + c, h, w]
                for i3_1, i4_1, i5_1 in T.grid(1, 1, 2):
                    for i6_0 in T.serial(136):
                        for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 17, 1, 1, 1, 1, 2):
                            with T.block("kernel_vec"):
                                g = T.axis.spatial(4, i0_1 * 2 + ax0)
                                out_channel = T.axis.spatial(34, i2_1 * 17 + ax1)
                                in_channel = T.axis.spatial(34, i6_0 // 4 + ax2)
                                h, w = T.axis.remap("SS", [ax3, ax4])
                                ci = T.axis.spatial(4, i6_0 % 4 + ax5)
                                co = T.axis.spatial(4, i5_1 * 2 + ax6)
                                T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                        for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 17, 1, 7, 2):
                            with T.block("conv"):
                                g = T.axis.spatial(4, i0_1 * 2 + i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk = T.axis.spatial(34, i2_1 * 17 + i2_3)
                                oh, ow = T.axis.remap("SS", [i3_0, i4_3])
                                oc_block = T.axis.spatial(4, i5_1 * 2 + i5_3)
                                ic = T.axis.reduce(136, i6_0)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                with T.init():
                                    conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 17, 1, 7, 2):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_1 * 2 + ax0)
                            v1 = T.axis.spatial(1, ax1)
                            v2 = T.axis.spatial(34, i2_1 * 17 + ax2)
                            v3 = T.axis.spatial(7, i3_0 + ax3)
                            v4 = T.axis.spatial(7, ax4)
                            v5 = T.axis.spatial(4, i5_1 * 2 + ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2 in T.grid(1, 544, 7):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(544, i1 + ax1)
                        h = T.axis.spatial(7, i2 + ax2)
                        w = T.axis.spatial(7, ax3)
                        T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
                for i3 in T.serial(7):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 17])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[136, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 34, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 34, 7, 4, 7):
                with T.block("data_vec"):
                    g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[n, g * 136 + C * 4 + c, h, w])
                    T.writes(data_vec[g, n, C, h, c, w])
                    data_vec[g, n, C, h, c, w] = placeholder[n, g * 136 + C * 4 + c, h, w]
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 7, 1, 1):
                for i0_1 in T.serial(2):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 34, 34, 1, 1, 4, 4):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_1 * 2 + ax0)
                            out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSS", [ax1, ax2, ax3, ax4, ax5, ax6])
                            T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                    for i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 2, 1, 1, 2, 136, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 17, 1, 7, 2):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(34, i2_1 * 17 + i2_3)
                            oh, ow = T.axis.remap("SS", [i3_0, i4_3])
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_3)
                            ic = T.axis.reduce(136, i6_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(4, 1, 34, 1, 7, 4):
                    with T.block("conv_global"):
                        v0, v1, v2 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        v3 = T.axis.spatial(7, i3_0 + ax3)
                        v4, v5 = T.axis.remap("SS", [ax4, ax5])
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 17])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[136, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=6)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            data_vec = T.alloc_buffer([4, 1, 34, 7, 4, 7], dtype="float32")
            kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1 in T.grid(1, 1, 1, 7, 1, 1, 2, 1, 2):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 34, 1, 4, 7):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_1 * 2 + ax0)
                        n, C = T.axis.remap("SS", [ax1, ax2])
                        h = T.axis.spatial(7, i3_0 + ax3)
                        c, w = T.axis.remap("SS", [ax4, ax5])
                        T.reads(placeholder[n, g * 136 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 136 + C * 4 + c, h, w]
                for i3_1, i4_1, i5_1, i6_0 in T.grid(1, 1, 2, 136):
                    for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 17, 1, 1, 1, 1, 2):
                        with T.block("kernel_vec"):
                            g = T.axis.spatial(4, i0_1 * 2 + ax0)
                            out_channel = T.axis.spatial(34, i2_1 * 17 + ax1)
                            in_channel = T.axis.spatial(34, i6_0 // 4 + ax2)
                            h, w = T.axis.remap("SS", [ax3, ax4])
                            ci = T.axis.spatial(4, i6_0 % 4 + ax5)
                            co = T.axis.spatial(4, i5_1 * 2 + ax6)
                            T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                            T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                            kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                    for i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 17, 1, 7, 2):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_1 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(34, i2_1 * 17 + i2_3)
                            oh, ow = T.axis.remap("SS", [i3_0, i4_3])
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_3)
                            ic = T.axis.reduce(136, i6_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[ax1 // 136, ax0, ax1 % 136 // 4, ax2, ax3, ax1 % 4], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(conv[ax1 // 136, ax0, ax1 % 136 // 4, ax2, ax3, ax1 % 4] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 2, 1, 17])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[136, 1])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v76 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v76)
l77 = sch.sample_compute_location(block=b3, decision=-2)
sch.compute_at(block=b3, loop=l77, preserve_unit_loops=True)
l78 = sch.sample_compute_location(block=b1, decision=12)
sch.compute_at(block=b1, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l79, preserve_unit_loops=True)
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #32: "fused_reshape_transpose_reshape_layout_transform_3"
[16:59:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 4, 136, 7, 7):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[0, (ax1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 4, 7, 7):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape[ax0, ax2, ax1, ax3, ax4])
                T.writes(T_transpose[ax0, ax1, ax2, ax3, ax4])
                T_transpose[ax0, ax1, ax2, ax3, ax4] = T_reshape[ax0, ax2, ax1, ax3, ax4]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 544 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0, ax1 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
            T_reshape = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 136, 7, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 136, 4, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1, ax1_1, ax2_1 = T.axis.remap("SSS", [ax0, ax1, ax2])
                        ax3_1 = T.axis.spatial(7, i2 + ax3)
                        ax4_1 = T.axis.spatial(7, i3 + ax4)
                        T.reads(placeholder[0, (ax2_1 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax1_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax2_1 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax1_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7]
                for i4 in T.serial(4):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("T_reshape_1"):
                            ax0_2 = T.axis.spatial(1, ax0)
                            ax1_2 = T.axis.spatial(544, i1 * 4 + i4 + ax1)
                            ax2_2 = T.axis.spatial(7, i2 + ax2)
                            ax3_2 = T.axis.spatial(7, i3 + ax3)
                            T.reads(T_transpose[0, ((ax3_2 // 7 + ax2_2) // 7 + ax1_2) % 544 // 4, ((ax3_2 // 7 + ax2_2) // 7 + ax1_2) % 4, (ax3_2 // 7 + ax2_2) % 7, ax3_2 % 7])
                            T.writes(T_reshape[ax0_2, ax1_2, ax2_2, ax3_2])
                            T_reshape[ax0_2, ax1_2, ax2_2, ax3_2] = T_transpose[0, ((ax3_2 // 7 + ax2_2) // 7 + ax1_2) % 544 // 4, ((ax3_2 // 7 + ax2_2) // 7 + ax1_2) % 4, (ax3_2 // 7 + ax2_2) % 7, ax3_2 % 7]
                    with T.block("T_layout_trans"):
                        ax0_3, ax1_3, ax2_3, ax3_3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                        T.reads(T_reshape[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 544 and ax2_3 < 7 and ax3_3 < 7, T_reshape[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=3)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 136, 9, 9, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 9, 9, 4):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1, i4_1 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1, i4_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1, i4_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 136, 7, 7, 4, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, oco, oh, ow, oci, kh, kw = T.axis.remap("SSSSSRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                T.block_attr({"workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [136, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                with T.init():
                    DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 136, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 9, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(136, i1_0 * 68 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 1, 4, 1, 1, 1, 3, 3, 1, 1, 1, 7, 2, 1, 1, 1, 17, 7, 1, 2):
                    with T.block("DepthwiseConv2d"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(136, i1_0 * 68 + i1_1 * 17 + i1_3)
                        oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                        oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [136, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        with T.init():
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for i0, i1, i2, i3, i4 in T.grid(1, 136, 7, 7, 4):
                with T.block("T_add"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 136, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 2):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 9, 9, 4):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, ax0)
                        i1 = T.axis.spatial(136, i1_0 * 68 + ax1)
                        i2, i3, i4 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 1, 4, 1, 1, 1):
                    for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 1, 1, 7, 2, 1, 1, 1, 17, 7, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(136, i1_0 * 68 + i1_1 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [136, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 17, 7, 7, 4):
                        with T.block("T_add"):
                            ax0_1 = T.axis.spatial(1, ax0)
                            ax1_1 = T.axis.spatial(136, i1_0 * 68 + i1_1 * 17 + ax1)
                            ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                            T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                            T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                            T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l47, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], placeholder_1: T.Buffer[(136, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            PaddedInput = T.alloc_buffer([1, 136, 9, 9, 4], dtype="float32")
            DepthwiseConv2d = T.alloc_buffer([1, 136, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0 in T.grid(1, 4, 1, 1, 1, 3, 3):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 17, 7, 7, 4):
                        with T.block("PaddedInput"):
                            i0 = T.axis.spatial(1, ax0)
                            i1 = T.axis.spatial(136, i1_0 * 68 + i1_1 * 17 + ax1)
                            i2 = T.axis.spatial(9, i5_0 + ax2)
                            i3 = T.axis.spatial(9, i6_0 + ax3)
                            i4 = T.axis.spatial(4, ax4)
                            T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                            T.writes(PaddedInput[i0, i1, i2, i3, i4])
                            PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 8 and 1 <= i3 and i3 < 8, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 2, 1, 1, 1, 17, 7, 1, 2):
                        with T.block("DepthwiseConv2d"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(136, i1_0 * 68 + i1_1 * 17 + i1_3)
                            oh, ow = T.axis.remap("SS", [i2_3, i3_2])
                            oci = T.axis.spatial(4, i4_2 * 2 + i4_3)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 136, 7, 7, 4], "float32"], ["TENSOR", [136, 1, 3, 3, 1, 4], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            with T.init():
                                DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh + kh, ow + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 68, 7, 7, 4):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(136, i1_0 * 68 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 17])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 7])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 1, 2, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=11)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #34: "fused_layout_transform_7"
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], T_layout_trans: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_layout_trans"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 544 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 7, 7, 4), "float32"], T_layout_trans: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_layout_trans"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3])
                    T_layout_trans[ax0, ax1, ax2, ax3] = T.if_then_else(ax0 < 1 and ax1 < 544 and ax2 < 7 and ax3 < 7, placeholder[ax0, ax1 // 4, ax2, ax3, ax1 % 4], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[16:59:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 34, 7, 4, 7], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_multiply = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(4, 1, 34, 7, 4, 7):
            with T.block("data_vec"):
                g, n, C, h, c, w = T.axis.remap("SSSSSS", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[n, g * 136 + C * 4 + c, h, w])
                T.writes(data_vec[g, n, C, h, c, w])
                data_vec[g, n, C, h, c, w] = placeholder[n, g * 136 + C * 4 + c, h, w]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 34, 34, 1, 1, 4, 4):
            with T.block("kernel_vec"):
                g, out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSSS", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0, i1, i2, i3, i4, i5, i6, i7, i8 in T.grid(4, 1, 34, 7, 7, 4, 136, 1, 1):
            with T.block("conv"):
                g, n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7, i8])
                T.reads(data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("output_unpack"):
                n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                T.writes(output_unpack[n, c, h, w])
                T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_multiply[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_multiply[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3], placeholder_4[ax0, ax1, ax2, ax3])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = T_add[ax0, ax1, ax2, ax3] + placeholder_4[ax0, ax1, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add_1[ax0, ax1, ax2, ax3], T.float32(0))
    

[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            kernel_vec = T.alloc_buffer([4, 34, 34, 1, 1, 4, 4], dtype="float32")
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(2, 34, 34, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g = T.axis.spatial(4, i0_0 * 2 + ax0)
                        out_channel, in_channel, h, w, ci, co = T.axis.remap("SSSSSS", [ax1, ax2, ax3, ax4, ax5, ax6])
                        T.reads(placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 136 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for i5_1 in T.serial(1):
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(8, 1, 1, 1, 1, 17, 7, 1, 1, 17, 1, 1, 2, 1, 2, 1, 1, 4):
                        with T.block("conv"):
                            g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(34, i2_2 * 2 + i2_3)
                            oh, ow, oc_block = T.axis.remap("SSS", [i3_2, i4_1, i5_3])
                            ic = T.axis.reduce(136, i6_0 * 17 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(placeholder[n, g * 136 + ic, oh + kh, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            with T.init():
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 136 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                    for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 34, 7, 1, 4):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, i0_0 * 2 + ax0)
                            v1, v2, v3 = T.axis.remap("SSS", [ax1, ax2, ax3])
                            v4 = T.axis.spatial(7, i4_1 + ax4)
                            v5 = T.axis.spatial(4, ax5)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 2])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[8, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l62, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=10)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            conv_global = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(2, 1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(1, 1, 1, 1, 7, 1, 8, 1, 1, 1, 1, 17, 7, 1, 1, 17, 1, 1, 2, 1, 2, 1, 1, 4):
                    with T.block("conv"):
                        g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                        n = T.axis.spatial(1, 0)
                        oc_chunk = T.axis.spatial(34, i2_2 * 2 + i2_3)
                        oh, ow, oc_block = T.axis.remap("SSS", [i3_2, i4_1, i5_3])
                        ic = T.axis.reduce(136, i6_0 * 17 + i6_1)
                        kh = T.axis.reduce(1, 0)
                        kw = T.axis.reduce(1, 0)
                        T.reads(placeholder[n, g * 136 + ic, oh + kh, ow + kw], placeholder_1[g * 136 + oc_chunk * 4 + oc_block, ic, kh, kw])
                        T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 136 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * placeholder_1[g * 136 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(2, 1, 34, 7, 7, 4):
                    with T.block("conv_global"):
                        v0 = T.axis.spatial(4, i0_0 * 2 + ax0)
                        v1, v2, v3, v4, v5 = T.axis.remap("SSSSS", [ax1, ax2, ax3, ax4, ax5])
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
            for i0, i1, i2 in T.grid(1, 544, 7):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, ax0)
                        c = T.axis.spatial(544, i1 + ax1)
                        h = T.axis.spatial(7, i2 + ax2)
                        w = T.axis.spatial(7, ax3)
                        T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
                for i3 in T.serial(7):
                    with T.block("T_relu"):
                        ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 2])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[8, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=2)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], placeholder_1: T.Buffer[(544, 136, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 544, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 544, 7, 7), "float32"], T_relu: T.Buffer[(1, 544, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv = T.alloc_buffer([4, 1, 34, 7, 7, 4], dtype="float32")
            output_unpack = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i5_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_1, i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 1, 8, 1, 1, 1, 1, 17, 7, 1, 1, 17, 1, 1, 2, 1, 2, 1, 1, 4):
                with T.block("conv"):
                    g = T.axis.spatial(4, i0_0 * 2 + i0_3)
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(34, i2_2 * 2 + i2_3)
                    oh, ow, oc_block = T.axis.remap("SSS", [i3_2, i4_1, i5_3])
                    ic = T.axis.reduce(136, i6_0 * 17 + i6_1)
                    kh = T.axis.reduce(1, 0)
                    kw = T.axis.reduce(1, 0)
                    T.reads(placeholder[n, g * 136 + ic, oh + kh, ow + kw], placeholder_1[g * 136 + oc_chunk * 4 + oc_block, ic, kh, kw])
                    T.writes(conv[g, n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv[g, n, oc_chunk, oh, ow, oc_block] = conv[g, n, oc_chunk, oh, ow, oc_block] + placeholder[n, g * 136 + ic // 4 * 4 + ic % 4, oh + kh, ow + kw] * placeholder_1[g * 136 + oc_chunk * 4 + oc_block, ic // 4 * 4 + ic % 4, kh, kw]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("output_unpack"):
                    n, c, h, w = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(conv[c // 136, n, c % 136 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 544, 7, 7], "float32"], ["TENSOR", [544, 136, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 136, n, c % 136 // 4, h, w, c % 4]
            for i0, i1, i2, i3 in T.grid(1, 544, 7, 7):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 17, 2])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[8, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=-2)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #36: "fused_nn_avg_pool2d_3"
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 544, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 544, 1, 1, 7], dtype="float32")
            for i0, i1 in T.grid(1, 544):
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(7, 1, 1, 1, 1, 7):
                    with T.block("tensor_rf"):
                        vi4_i5_fused_0, ax0_1 = T.axis.remap("SS", [ax0, ax1])
                        ax1_1 = T.axis.spatial(544, i1 + ax2)
                        ax2_1, ax3_1, vi4_i5_fused_1 = T.axis.remap("SSR", [ax3, ax4, ax5])
                        T.reads(placeholder[ax0_1, ax1_1, ax2_1 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) // 7, ax3_1 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) % 7])
                        T.writes(tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0])
                        with T.init():
                            tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] = T.float32(0)
                        tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] = tensor_rf[ax0_1, ax1_1, ax2_1, ax3_1, vi4_i5_fused_0] + placeholder[ax0_1, ax1_1, ax2_1 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) // 7, ax3_1 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) % 7]
                for i2, i3, i4_i5_fused_0 in T.grid(1, 1, 7):
                    with T.block("tensor"):
                        vi4_i5_fused_0 = T.axis.reduce(7, i4_i5_fused_0)
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(544, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0])
                        T.writes(tensor_1[ax0, ax1, ax2, ax3])
                        with T.init():
                            tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                        tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_0]
            for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7 = sch.get_loops(block=b0)
l8 = sch.fuse(l6, l7)
v9, v10 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 7])
l11, l12 = sch.split(loop=l8, factors=[v9, v10])
b13 = sch.rfactor(loop=l11, factor_axis=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
b15, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b15, decision=1)
sch.compute_at(block=b15, loop=l17, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
            tensor_rf = T.alloc_buffer([1, 544, 1, 1, 7], dtype="float32")
            for i0, i1, i2, i3, i4_i5_fused_0, i4_i5_fused_1 in T.grid(1, 544, 1, 1, 7, 7):
                with T.block("tensor_rf"):
                    vi4_i5_fused_1 = T.axis.spatial(7, i4_i5_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(544, i1)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    vi4_i5_fused_0 = T.axis.reduce(7, i4_i5_fused_0)
                    T.reads(placeholder[ax0, ax1, ax2 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) // 7, ax3 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) % 7])
                    T.writes(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                    with T.init():
                        tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = T.float32(0)
                    tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] = tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1] + placeholder[ax0, ax1, ax2 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) // 7, ax3 + (vi4_i5_fused_0 * 7 + vi4_i5_fused_1) % 7]
            for i0, i1, i2, i3, i4_i5_fused_1 in T.grid(1, 544, 1, 1, 7):
                with T.block("tensor"):
                    vi4_i5_fused_1 = T.axis.reduce(7, i4_i5_fused_1)
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(544, i1)
                    ax2 = T.axis.spatial(1, 0)
                    ax3 = T.axis.spatial(1, 0)
                    T.reads(tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + tensor_rf[ax0, ax1, ax2, ax3, vi4_i5_fused_1]
            for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
l2, l3, l4, l5, l6, l7 = sch.get_loops(block=b0)
l8 = sch.fuse(l6, l7)
v9, v10 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[7, 7])
l11, l12 = sch.split(loop=l8, factors=[v9, v10])
b13 = sch.rfactor(loop=l12, factor_axis=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
b15, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
l17 = sch.sample_compute_location(block=b15, decision=-1)
sch.compute_at(block=b15, loop=l17, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], tensor: T.Buffer[(1, 544, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            tensor_1 = T.alloc_buffer([1, 544, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 544, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 544, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
l3 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l3, preserve_unit_loops=True)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #37: "fused_reshape"
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 1, 1), "float32"], T_reshape: T.Buffer[(1, 544), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 544):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 544, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 544, 0, 0]
    

[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 1, 1), "float32"], T_reshape: T.Buffer[(1, 544), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            for i0, i1 in T.grid(1, 544):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 544, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 544, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.vectorize", ann_val=64)
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #38: "fused_nn_dense_add"
[16:59:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
        for i0, i1, i2 in T.grid(1, 1000, 544):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 3 design space(s) generated
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 100, 1, 2, 544, 1, 1, 1, 1, 5):
                with T.block("T_matmul_NT"):
                    i = T.axis.spatial(1, 0)
                    j = T.axis.spatial(1000, i1_0 * 10 + i1_1 * 5 + i1_3)
                    k = T.axis.reduce(544, i2_0)
                    T.reads(placeholder[i, k], placeholder_1[j, k])
                    T.writes(T_matmul_NT[i, j])
                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        T_matmul_NT[i, j] = T.float32(0)
                    T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_add"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                    T.writes(T_add[ax0, ax1])
                    T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[100, 2, 1, 5])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[544, 1])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v25 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v25)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0, i0_1, i1_1 in T.grid(1, 100, 1, 2):
                for i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(544, 1, 1, 1, 1, 5):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(1000, i1_0 * 10 + i1_1 * 5 + i1_3)
                        k = T.axis.reduce(544, i2_0)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 5):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1000, i1_0 * 10 + i1_1 * 5 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[100, 2, 1, 5])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[544, 1])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l18, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544), "float32"], placeholder_1: T.Buffer[(1000, 544), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
            for i0_0, i1_0 in T.grid(1, 100):
                for i0_1, i1_1, i2_0, i0_2, i1_2, i2_1, i0_3, i1_3 in T.grid(1, 2, 544, 1, 1, 1, 1, 5):
                    with T.block("T_matmul_NT"):
                        i = T.axis.spatial(1, 0)
                        j = T.axis.spatial(1000, i1_0 * 10 + i1_1 * 5 + i1_3)
                        k = T.axis.reduce(544, i2_0)
                        T.reads(placeholder[i, k], placeholder_1[j, k])
                        T.writes(T_matmul_NT[i, j])
                        T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            T_matmul_NT[i, j] = T.float32(0)
                        T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
                for ax0, ax1 in T.grid(1, 10):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(1000, i1_0 * 10 + ax1)
                        T.reads(T_matmul_NT[ax0_1, ax1_1], placeholder_2[ax0_1, ax1_1])
                        T.writes(T_add[ax0_1, ax1_1])
                        T_add[ax0_1, ax1_1] = T_matmul_NT[ax0_1, ax1_1] + placeholder_2[ax0_1, ax1_1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8 = sch.sample_perfect_tile(loop=l2, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l9, l10, l11, l12 = sch.split(loop=l2, factors=[v5, v6, v7, v8])
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[100, 2, 1, 5])
l17, l18, l19, l20 = sch.split(loop=l3, factors=[v13, v14, v15, v16])
v21, v22 = sch.sample_perfect_tile(loop=l4, n=2, max_innermost_factor=64, decision=[544, 1])
l23, l24 = sch.split(loop=l4, factors=[v21, v22])
sch.reorder(l9, l17, l10, l18, l23, l11, l19, l24, l12, l20)
b25, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b25, loop=l17, preserve_unit_loops=True)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.vectorize", ann_val=64)
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #39: "fused_nn_softmax"
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1000):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1000):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 9 design space(s) generated
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 125], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 125, 8):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(125, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(8, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 8 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 8 + vi1_1])
            for i0, i1_0 in T.grid(1, 125):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(125, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
            for i0_3, i1 in T.grid(1, 1000):
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_exp"):
                        i0_4, i1_2 = T.axis.remap("SS", [ax0, ax1])
                        T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4])
                        T.writes(T_softmax_exp[i0_4, i1_2])
                        T_softmax_exp[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32")
                for ax0 in T.serial(500):
                    for ax0_1, ax1, ax2 in T.grid(1, 1, 2):
                        with T.block("T_softmax_expsum_rf"):
                            vi1_0 = T.axis.spatial(500, ax0 + ax0_1)
                            i0_5, vi1_1 = T.axis.remap("SR", [ax1, ax2])
                            T.reads(T_softmax_exp[i0_5, vi1_0 * 2 + vi1_1])
                            T.writes(T_softmax_expsum_rf[i0_5, vi1_0])
                            with T.init():
                                T_softmax_expsum_rf[i0_5, vi1_0] = T.float32(0)
                            T_softmax_expsum_rf[i0_5, vi1_0] = T_softmax_expsum_rf[i0_5, vi1_0] + T_softmax_exp[i0_5, vi1_0 * 2 + vi1_1]
                    for ax1 in T.serial(1):
                        with T.block("T_softmax_expsum"):
                            vi1_0, i0_6 = T.axis.remap("RS", [ax0, ax1])
                            T.reads(T_softmax_expsum_rf[i0_6, vi1_0])
                            T.writes(T_softmax_expsum[i0_6])
                            with T.init():
                                T_softmax_expsum[i0_6] = T.float32(0)
                            T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_0]
                with T.block("T_softmax_norm"):
                    i0_7, i1_3 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(T_softmax_exp[i0_7, i1_3], T_softmax_expsum[i0_7])
                    T.writes(T_softmax_norm[i0_7, i1_3])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_7, i1_3] = T_softmax_exp[i0_7, i1_3] / T_softmax_expsum[i0_7]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[125, 8])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=2)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=1)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 8], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 125, 8):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(8, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(125, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 8 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 8 + vi1_1])
            for i0, i1_1 in T.grid(1, 8):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(8, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(500, i1_0)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(2, i1_1)
                    T.reads(placeholder[i0_4, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_0] = T_softmax_expsum_rf[i0_4, vi1_0] + T.exp(placeholder[i0_4, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_0 in T.grid(1, 500):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(500, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_0])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_0]
            for i0_7, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(placeholder[i0_8, i1_2], T_softmax_maxelem[i0_8], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T.exp(placeholder[i0_8, i1_2] - T_softmax_maxelem[i0_8], dtype="float32") / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[125, 8])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 500], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_exp"):
                    i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_exp[i0_2, i1_1])
                    T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_0, i1_1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_0 = T.axis.spatial(500, i1_0)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(2, i1_1_1)
                    T.reads(T_softmax_exp[i0_4, vi1_0 * 2 + vi1_1])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_0])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_0] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_0] = T_softmax_expsum_rf[i0_4, vi1_0] + T_softmax_exp[i0_4, vi1_0 * 2 + vi1_1]
            for i0_5, i1_0 in T.grid(1, 500):
                with T.block("T_softmax_expsum"):
                    vi1_0 = T.axis.reduce(500, i1_0)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_0])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_0]
            for i0_7, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(T_softmax_exp[i0_8, i1_2], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T_softmax_exp[i0_8, i1_2] / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 100], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 100, 10):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(100, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(10, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 10 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 10 + vi1_1])
            for i0, i1_0 in T.grid(1, 100):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(100, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
            for i0_3, i1 in T.grid(1, 1000):
                for ax0 in T.serial(2):
                    for ax0_1, ax1, ax2 in T.grid(1, 1, 500):
                        with T.block("T_softmax_expsum_rf"):
                            vi1_1 = T.axis.spatial(2, ax0 + ax0_1)
                            i0_4, vi1_0 = T.axis.remap("SR", [ax1, ax2])
                            T.reads(placeholder[i0_4, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_4])
                            T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                            with T.init():
                                T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
                            T_softmax_expsum_rf[i0_4, vi1_1] = T_softmax_expsum_rf[i0_4, vi1_1] + T.exp(placeholder[i0_4, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
                    for ax1 in T.serial(1):
                        with T.block("T_softmax_expsum"):
                            vi1_1, i0_5 = T.axis.remap("RS", [ax0, ax1])
                            T.reads(T_softmax_expsum_rf[i0_5, vi1_1])
                            T.writes(T_softmax_expsum[i0_5])
                            with T.init():
                                T_softmax_expsum[i0_5] = T.float32(0)
                            T_softmax_expsum[i0_5] = T_softmax_expsum[i0_5] + T_softmax_expsum_rf[i0_5, vi1_1]
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[100, 10])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l15, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=2)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #4:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 10], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 100, 10):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(10, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(100, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 10 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 10 + vi1_1])
            for i0, i1_1 in T.grid(1, 10):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(10, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(2, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(500, i1_0)
                    T.reads(placeholder[i0_4, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_expsum_rf[i0_4, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_4, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_4, vi1_1] = T_softmax_expsum_rf[i0_4, vi1_1] + T.exp(placeholder[i0_4, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_1 in T.grid(1, 2):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(2, i1_1)
                    i0_6 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_6, vi1_1])
                    T.writes(T_softmax_expsum[i0_6])
                    with T.init():
                        T_softmax_expsum[i0_6] = T.float32(0)
                    T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_expsum_rf[i0_6, vi1_1]
            for i0_7, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_8, i1_2 = T.axis.remap("SS", [i0_7, i1])
                    T.reads(placeholder[i0_8, i1_2], T_softmax_maxelem[i0_8], T_softmax_expsum[i0_8])
                    T.writes(T_softmax_norm[i0_8, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_8, i1_2] = T.exp(placeholder[i0_8, i1_2] - T_softmax_maxelem[i0_8], dtype="float32") / T_softmax_expsum[i0_8]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
l11, l12 = sch.get_loops(block=b0)
v13, v14 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[100, 10])
l15, l16 = sch.split(loop=l12, factors=[v13, v14])
b17 = sch.rfactor(loop=l16, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
b19, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l20 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l20, preserve_unit_loops=True)
l21 = sch.sample_compute_location(block=b19, decision=-1)
sch.compute_at(block=b19, loop=l21, preserve_unit_loops=True)
l22 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l22, preserve_unit_loops=True)
b23, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l24 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l24, preserve_unit_loops=True)
l25 = sch.sample_compute_location(block=b23, decision=-1)
sch.compute_at(block=b23, loop=l25, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #5:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_rf = T.alloc_buffer([1, 2], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1_0, i1_1 in T.grid(1, 500, 2):
                with T.block("T_softmax_expsum_rf"):
                    vi1_1 = T.axis.spatial(2, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(500, i1_0)
                    T.reads(placeholder[i0_2, vi1_0 * 2 + vi1_1], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum_rf[i0_2, vi1_1])
                    with T.init():
                        T_softmax_expsum_rf[i0_2, vi1_1] = T.float32(0)
                    T_softmax_expsum_rf[i0_2, vi1_1] = T_softmax_expsum_rf[i0_2, vi1_1] + T.exp(placeholder[i0_2, vi1_0 * 2 + vi1_1] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1_1 in T.grid(1, 2):
                with T.block("T_softmax_expsum"):
                    vi1_1 = T.axis.reduce(2, i1_1)
                    i0_4 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_expsum_rf[i0_4, vi1_1])
                    T.writes(T_softmax_expsum[i0_4])
                    with T.init():
                        T_softmax_expsum[i0_4] = T.float32(0)
                    T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_expsum_rf[i0_4, vi1_1]
            for i0_5, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                    T.reads(placeholder[i0_6, i1_2], T_softmax_maxelem[i0_6], T_softmax_expsum[i0_6])
                    T.writes(T_softmax_norm[i0_6, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_6, i1_2] = T.exp(placeholder[i0_6, i1_2] - T_softmax_maxelem[i0_6], dtype="float32") / T_softmax_expsum[i0_6]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b2)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[500, 2])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
b12, = sch.get_producers(block=b2)
sch.unannotate(block_or_loop=b2, ann_key="meta_schedule.random_compute_producer")
l13 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l13, preserve_unit_loops=True)
l14 = sch.sample_compute_location(block=b12, decision=-1)
sch.compute_at(block=b12, loop=l14, preserve_unit_loops=True)
l15 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l16, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #6:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 100], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 100, 10):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_0 = T.axis.spatial(100, i1_0)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_1 = T.axis.reduce(10, i1_1)
                    T.reads(placeholder[i0_1, vi1_0 * 10 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_0])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_0] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_0] = T.max(T_softmax_maxelem_rf[i0_1, vi1_0], placeholder[i0_1, vi1_0 * 10 + vi1_1])
            for i0, i1_0 in T.grid(1, 100):
                with T.block("T_softmax_maxelem"):
                    vi1_0 = T.axis.reduce(100, i1_0)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_0])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_0])
            for i0_3, i1 in T.grid(1, 1000):
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_expsum"):
                        i0_4, k = T.axis.remap("SR", [ax0, ax1])
                        T.reads(placeholder[i0_4, k], T_softmax_maxelem[i0_4])
                        T.writes(T_softmax_expsum[i0_4])
                        with T.init():
                            T_softmax_expsum[i0_4] = T.float32(0)
                        T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T.exp(placeholder[i0_4, k] - T_softmax_maxelem[i0_4], dtype="float32")
                with T.block("T_softmax_norm"):
                    i0_5, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_5, i1_2], T_softmax_maxelem[i0_5], T_softmax_expsum[i0_5])
                    T.writes(T_softmax_norm[i0_5, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_5, i1_2] = T.exp(placeholder[i0_5, i1_2] - T_softmax_maxelem[i0_5], dtype="float32") / T_softmax_expsum[i0_5]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[100, 10])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l8, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=-1)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #7:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            T_softmax_maxelem_rf = T.alloc_buffer([1, 10], dtype="float32")
            for i0, i1_0, i1_1 in T.grid(1, 100, 10):
                with T.block("T_softmax_maxelem_rf"):
                    vi1_1 = T.axis.spatial(10, i1_1)
                    i0_1 = T.axis.spatial(1, 0)
                    vi1_0 = T.axis.reduce(100, i1_0)
                    T.reads(placeholder[i0_1, vi1_0 * 10 + vi1_1])
                    T.writes(T_softmax_maxelem_rf[i0_1, vi1_1])
                    with T.init():
                        T_softmax_maxelem_rf[i0_1, vi1_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem_rf[i0_1, vi1_1] = T.max(T_softmax_maxelem_rf[i0_1, vi1_1], placeholder[i0_1, vi1_0 * 10 + vi1_1])
            for i0, i1_1 in T.grid(1, 10):
                with T.block("T_softmax_maxelem"):
                    vi1_1 = T.axis.reduce(10, i1_1)
                    i0_2 = T.axis.spatial(1, 0)
                    T.reads(T_softmax_maxelem_rf[i0_2, vi1_1])
                    T.writes(T_softmax_maxelem[i0_2])
                    with T.init():
                        T_softmax_maxelem[i0_2] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_2] = T.max(T_softmax_maxelem[i0_2], T_softmax_maxelem_rf[i0_2, vi1_1])
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_exp"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4])
                    T.writes(T_softmax_exp[i0_4, i1_2])
                    T_softmax_exp[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32")
            for i0_5, i1_3 in T.grid(1, 1000):
                for ax0, ax1 in T.grid(1, 1000):
                    with T.block("T_softmax_expsum"):
                        i0_6, k = T.axis.remap("SR", [ax0, ax1])
                        T.reads(T_softmax_exp[i0_6, k])
                        T.writes(T_softmax_expsum[i0_6])
                        with T.init():
                            T_softmax_expsum[i0_6] = T.float32(0)
                        T_softmax_expsum[i0_6] = T_softmax_expsum[i0_6] + T_softmax_exp[i0_6, k]
                with T.block("T_softmax_norm"):
                    i0_7, i1_4 = T.axis.remap("SS", [i0_5, i1_3])
                    T.reads(T_softmax_exp[i0_7, i1_4], T_softmax_expsum[i0_7])
                    T.writes(T_softmax_norm[i0_7, i1_4])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_7, i1_4] = T_softmax_exp[i0_7, i1_4] / T_softmax_expsum[i0_7]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
l4, l5 = sch.get_loops(block=b0)
v6, v7 = sch.sample_perfect_tile(loop=l5, n=2, max_innermost_factor=64, decision=[100, 10])
l8, l9 = sch.split(loop=l5, factors=[v6, v7])
b10 = sch.rfactor(loop=l9, factor_axis=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer", ann_val=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
l12 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l12, preserve_unit_loops=True)
l13 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l13, preserve_unit_loops=True)
b14, = sch.get_producers(block=b0)
sch.unannotate(block_or_loop=b0, ann_key="meta_schedule.random_compute_producer")
l15 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l15, preserve_unit_loops=True)
l16 = sch.sample_compute_location(block=b14, decision=-1)
sch.compute_at(block=b14, loop=l16, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #8:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":128, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_4, i1_1 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_1], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_1] = T.exp(placeholder[i0_4, i1_1] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                             fused_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_avg_pool2d"
[16:59:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:00:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:00:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_avg_pool2d_1"
[17:00:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:00:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:00:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_layout_transform"
[17:00:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:00:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:00:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_multiply_add_nn_relu"
[17:00:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:01:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:01:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_reshape_transpose_reshape_layout_transform"
[17:01:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:01:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:01:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"
[17:01:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:02:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:02:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_layout_transform_1"
[17:02:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:02:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:02:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_multiply_add"
[17:02:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:02:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_layout_transform_2"
[17:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:02:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:03:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"
[17:03:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:03:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_max_pool2d"
[17:03:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:03:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:03:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_avg_pool2d_2"
[17:03:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:03:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_layout_transform_concatenate_nn_relu"
[17:04:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:04:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:04:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"
[17:04:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:04:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_layout_transform_3"
[17:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:04:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:05:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"
[17:05:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:05:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:05:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"
[17:05:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:06:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_reshape_transpose_reshape_layout_transform_1"
[17:06:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:06:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"
[17:06:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_layout_transform_4"
[17:07:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:07:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_nn_conv2d_multiply_add_1"
[17:07:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:07:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:08:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_concatenate_nn_relu"
[17:08:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:08:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:10:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"
[17:10:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:10:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_layout_transform_5"
[17:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:10:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:10:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[17:10:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:11:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:11:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"
[17:11:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:12:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:12:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_reshape_transpose_reshape_layout_transform_2"
[17:12:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:12:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:12:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"
[17:12:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:13:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:13:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_layout_transform_6"
[17:13:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:13:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_nn_conv2d_multiply_add_2"
[17:14:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:14:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_concatenate_nn_relu_1"
[17:15:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:15:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:15:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"
[17:15:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:15:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:15:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_reshape_transpose_reshape_layout_transform_3"
[17:15:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:15:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:16:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"
[17:16:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:16:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_layout_transform_7"
[17:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:16:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:17:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[17:17:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:17:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:17:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_nn_avg_pool2d_3"
[17:17:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:17:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:18:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_reshape"
[17:18:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:18:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_nn_dense_add"
[17:18:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:18:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:18:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_softmax"
[17:18:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[17:18:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #0: GFLOPs: 10.4903. Time: 0.0407 ms. Best GFLOPs: 10.4903
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #1: GFLOPs: 11.1323. Time: 0.0383 ms. Best GFLOPs: 11.1323
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #2: GFLOPs: 21.9969. Time: 0.0194 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #3: GFLOPs: 10.2659. Time: 0.0415 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #4: GFLOPs: 18.4242. Time: 0.0231 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #5: GFLOPs: 15.6487. Time: 0.0273 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #6: GFLOPs: 7.5878. Time: 0.0562 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #7: GFLOPs: 8.1932. Time: 0.0521 ms. Best GFLOPs: 21.9969
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #8: GFLOPs: 23.4028. Time: 0.0182 ms. Best GFLOPs: 23.4028
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #9: GFLOPs: 17.6174. Time: 0.0242 ms. Best GFLOPs: 23.4028
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #10: GFLOPs: 18.0324. Time: 0.0237 ms. Best GFLOPs: 23.4028
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #11: GFLOPs: 5.5567. Time: 0.0768 ms. Best GFLOPs: 23.4028
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #12: GFLOPs: 27.2423. Time: 0.0157 ms. Best GFLOPs: 27.2423
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #13: GFLOPs: 9.5396. Time: 0.0447 ms. Best GFLOPs: 27.2423
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #14: GFLOPs: 18.8838. Time: 0.0226 ms. Best GFLOPs: 27.2423
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #15: GFLOPs: 9.1648. Time: 0.0465 ms. Best GFLOPs: 27.2423
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #16: GFLOPs: 29.1549. Time: 0.0146 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #17: GFLOPs: 7.6763. Time: 0.0556 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #18: GFLOPs: 7.9297. Time: 0.0538 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #19: GFLOPs: 15.9259. Time: 0.0268 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #20: GFLOPs: 13.6362. Time: 0.0313 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #21: GFLOPs: 11.3955. Time: 0.0374 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #22: GFLOPs: 1.1542. Time: 0.3695 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #23: GFLOPs: 1.2918. Time: 0.3302 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #24: GFLOPs: 3.2009. Time: 0.1332 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #25: GFLOPs: 2.4407. Time: 0.1747 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #26: GFLOPs: 2.5863. Time: 0.1649 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #27: GFLOPs: 3.3785. Time: 0.1262 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #28: GFLOPs: 3.2311. Time: 0.1320 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #29: GFLOPs: 2.3949. Time: 0.1781 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #30: GFLOPs: 2.9746. Time: 0.1434 ms. Best GFLOPs: 29.1549
[17:18:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_avg_pool2d"] Trial #31: GFLOPs: 6.3128. Time: 0.0676 ms. Best GFLOPs: 29.1549
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[17:18:52] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_avg_pool2d"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                             fused_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 14.6286

[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #0: GFLOPs: 6.2087. Time: 0.1374 ms. Best GFLOPs: 6.2087
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #1: GFLOPs: 63.9200. Time: 0.0133 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #2: GFLOPs: 21.3031. Time: 0.0400 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #3: GFLOPs: 46.9667. Time: 0.0182 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #4: GFLOPs: 33.2501. Time: 0.0257 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #5: GFLOPs: 16.9754. Time: 0.0502 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #6: GFLOPs: 35.0040. Time: 0.0244 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #7: GFLOPs: 51.1178. Time: 0.0167 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #8: GFLOPs: 35.3000. Time: 0.0242 ms. Best GFLOPs: 63.9200
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #9: GFLOPs: 67.6931. Time: 0.0126 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #10: GFLOPs: 21.8782. Time: 0.0390 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #11: GFLOPs: 23.1175. Time: 0.0369 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #12: GFLOPs: 14.2075. Time: 0.0600 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #13: GFLOPs: 12.6511. Time: 0.0674 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #14: GFLOPs: 19.6275. Time: 0.0435 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #15: GFLOPs: 16.9144. Time: 0.0504 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #16: GFLOPs: 37.6921. Time: 0.0226 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #17: GFLOPs: 46.4401. Time: 0.0184 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #18: GFLOPs: 14.5287. Time: 0.0587 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #19: GFLOPs: 8.8169. Time: 0.0967 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #20: GFLOPs: 32.2325. Time: 0.0265 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #21: GFLOPs: 22.6112. Time: 0.0377 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #22: GFLOPs: 8.4331. Time: 0.1011 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #23: GFLOPs: 12.6356. Time: 0.0675 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #24: GFLOPs: 18.8774. Time: 0.0452 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #25: GFLOPs: 12.2953. Time: 0.0694 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #26: GFLOPs: 17.2344. Time: 0.0495 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #27: GFLOPs: 29.6832. Time: 0.0287 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #28: GFLOPs: 15.0877. Time: 0.0565 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #29: GFLOPs: 16.6457. Time: 0.0512 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #30: GFLOPs: 14.3533. Time: 0.0594 ms. Best GFLOPs: 67.6931
[17:18:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_avg_pool2d_1"] Trial #31: GFLOPs: 18.7368. Time: 0.0455 ms. Best GFLOPs: 67.6931
[17:18:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_avg_pool2d_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 27.2295

[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.0393 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0210 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.0155 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0271 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.0162 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0420 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #6: GFLOPs: 0.0000. Time: 0.0247 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0255 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 0.0241 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0213 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0169 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #11: GFLOPs: 0.0000. Time: 0.0154 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.0246 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0217 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #14: GFLOPs: 0.0000. Time: 0.0521 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.4146 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.1121 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.1002 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #18: GFLOPs: 0.0000. Time: 0.0930 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0694 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 0.1034 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0937 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0703 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0686 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #24: GFLOPs: 0.0000. Time: 0.0631 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0818 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.1038 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0243 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0095 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #29: GFLOPs: 0.0000. Time: 0.0094 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.0095 ms. Best GFLOPs: 0.0000
[17:18:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[17:18:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_layout_transform"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 36.6488

[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #0: GFLOPs: 16.9094. Time: 0.3116 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #1: GFLOPs: 3.6370. Time: 1.4486 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #2: GFLOPs: 6.7537. Time: 0.7801 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #3: GFLOPs: 10.5485. Time: 0.4995 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #4: GFLOPs: 2.8406. Time: 1.8547 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #5: GFLOPs: 12.5161. Time: 0.4209 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #6: GFLOPs: 11.3140. Time: 0.4657 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #7: GFLOPs: 8.2757. Time: 0.6366 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #8: GFLOPs: 12.1622. Time: 0.4332 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #9: GFLOPs: 14.6729. Time: 0.3591 ms. Best GFLOPs: 16.9094
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #10: GFLOPs: 39.8042. Time: 0.1324 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #11: GFLOPs: 15.0339. Time: 0.3504 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #12: GFLOPs: 14.0019. Time: 0.3763 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #13: GFLOPs: 10.1817. Time: 0.5174 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #14: GFLOPs: 17.3807. Time: 0.3031 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #15: GFLOPs: 9.8818. Time: 0.5331 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #16: GFLOPs: 6.0185. Time: 0.8754 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #17: GFLOPs: 19.0131. Time: 0.2771 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #18: GFLOPs: 11.7584. Time: 0.4481 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #19: GFLOPs: 8.9823. Time: 0.5865 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #20: GFLOPs: 17.1354. Time: 0.3075 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #21: GFLOPs: 1.5436. Time: 3.4131 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #22: GFLOPs: 9.0632. Time: 0.5813 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #23: GFLOPs: 4.3634. Time: 1.2074 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #24: GFLOPs: 9.2862. Time: 0.5673 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #25: GFLOPs: 8.9311. Time: 0.5899 ms. Best GFLOPs: 39.8042
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #26: GFLOPs: 40.8876. Time: 0.1289 ms. Best GFLOPs: 40.8876
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #27: GFLOPs: 14.6635. Time: 0.3593 ms. Best GFLOPs: 40.8876
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #28: GFLOPs: 12.6642. Time: 0.4160 ms. Best GFLOPs: 40.8876
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #29: GFLOPs: 18.9200. Time: 0.2785 ms. Best GFLOPs: 40.8876
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #30: GFLOPs: 14.3215. Time: 0.3679 ms. Best GFLOPs: 40.8876
[17:18:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_multiply_add_nn_relu"] Trial #31: GFLOPs: 23.7868. Time: 0.2215 ms. Best GFLOPs: 40.8876
[17:18:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_multiply_add_nn_relu"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 165.502

[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #0: GFLOPs: 0.0000. Time: 0.8168 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #1: GFLOPs: 0.0000. Time: 0.0780 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #2: GFLOPs: 0.0000. Time: 0.4150 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #3: GFLOPs: 0.0000. Time: 0.0547 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #4: GFLOPs: 0.0000. Time: 0.4002 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #5: GFLOPs: 0.0000. Time: 0.0511 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #6: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1568, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(56, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, i4)
                        ax2_1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                        ax3_1 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax4_1 = T.axis.spatial(56, i3)
                        T.reads(placeholder[0, (ax1_1 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_1 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56]
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_reshape_1"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(112, i0_i1_i2_fused // 56 * 4 + i4)
                        ax2_2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3_2 = T.axis.spatial(56, i3)
                        T.reads(T_reshape[0, ((ax3_2 // 56 + ax2_2) // 56 + ax1_2) % 4, ((ax3_2 // 56 + ax2_2) // 56 + ax1_2) % 112 // 4, (ax3_2 // 56 + ax2_2) % 56, ax3_2 % 56])
                        T.writes(T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2] = T_reshape[0, ((ax3_2 // 56 + ax2_2) // 56 + ax1_2) % 4, ((ax3_2 // 56 + ax2_2) // 56 + ax1_2) % 112 // 4, (ax3_2 // 56 + ax2_2) % 56, ax3_2 % 56]
                with T.block("T_layout_trans"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                    ax2_3 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                    ax3_3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_reshape_1[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                    T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                    T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 112 and ax2_3 < 56 and ax3_3 < 56, T_reshape_1[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16, l17, l18, l19, l20, l21 = sch.get_loops(block=b9)
l22 = sch.fuse(l12, l13, l14)
sch.parallel(loop=l22)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #7: GFLOPs: 0.0000. Time: 0.0411 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #8: GFLOPs: 0.0000. Time: 3.5512 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #9: GFLOPs: 0.0000. Time: 0.0675 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #10: GFLOPs: 0.0000. Time: 0.0379 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #11: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(6272, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i4_fused in T.vectorized(56):
                with T.block("T_reshape"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 1568)
                    ax2 = T.axis.spatial(28, i0_i1_i2_i3_fused % 1568 // 56)
                    ax3 = T.axis.spatial(56, i0_i1_i2_i3_fused % 56)
                    ax4 = T.axis.spatial(56, i4_fused)
                    T.reads(placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                    T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56]
        for i0_i1_i2_fused in T.parallel(1568, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(56, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                        ax2_1 = T.axis.spatial(4, i4)
                        ax3_1 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax4_1 = T.axis.spatial(56, i3)
                        T.reads(T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1]
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                    ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_transpose[0, (ax1 * 4 + (ax3 // 56 + ax2) // 56 + ax4) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax4) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 112 and ax2 < 56 and ax3 < 56, T_transpose[0, ((ax3 // 56 + ax2) // 56 + (ax1 * 4 + ax4)) % 112 // 4, ((ax3 // 56 + ax2) // 56 + (ax1 * 4 + ax4)) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16 = sch.get_loops(block=b9)
l17 = sch.fuse(l12, l13, l14, l15)
sch.parallel(loop=l17)
l18 = sch.fuse(l16)
sch.vectorize(loop=l18)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b10)
l29 = sch.fuse(l19, l20, l21)
sch.parallel(loop=l29)
sch.annotate(block_or_loop=l29, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l29, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #12: GFLOPs: 0.0000. Time: 0.7079 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #13: GFLOPs: 0.0000. Time: 0.0446 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #14: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(6272, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i4_fused in T.vectorized(56):
                with T.block("T_reshape"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(4, i0_i1_i2_i3_fused // 1568)
                    ax2 = T.axis.spatial(28, i0_i1_i2_i3_fused % 1568 // 56)
                    ax3 = T.axis.spatial(56, i0_i1_i2_i3_fused % 56)
                    ax4 = T.axis.spatial(56, i4_fused)
                    T.reads(placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56])
                    T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                    T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 28 + (ax4 // 56 + ax3) // 56 + ax2) % 112, (ax4 // 56 + ax3) % 56, ax4 % 56]
        for i0_i1_i2_fused in T.parallel(1568, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(56, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                        ax2_1 = T.axis.spatial(4, i4)
                        ax3_1 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax4_1 = T.axis.spatial(56, i3)
                        T.reads(T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1]
                with T.block("T_layout_trans"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                    ax2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                    ax3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_transpose[0, (ax1 * 4 + (ax3 // 56 + ax2) // 56 + ax4) % 112 // 4, ((ax3 // 56 + ax2) // 56 + ax4) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56])
                    T.writes(T_layout_trans[ax0, ax1, ax2, ax3, ax4])
                    T_layout_trans[ax0, ax1, ax2, ax3, ax4] = T.if_then_else(ax0 < 1 and ax1 * 4 + ax4 < 112 and ax2 < 56 and ax3 < 56, T_transpose[0, ((ax3 // 56 + ax2) // 56 + (ax1 * 4 + ax4)) % 112 // 4, ((ax3 // 56 + ax2) // 56 + (ax1 * 4 + ax4)) % 4, (ax3 // 56 + ax2) % 56, ax3 % 56], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16 = sch.get_loops(block=b9)
l17 = sch.fuse(l12, l13, l14, l15)
sch.parallel(loop=l17)
l18 = sch.fuse(l16)
sch.vectorize(loop=l18)
sch.annotate(block_or_loop=l17, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l17, ann_key="pragma_unroll_explicit", ann_val=1)
l19, l20, l21, l22, l23, l24, l25, l26, l27, l28 = sch.get_loops(block=b10)
l29 = sch.fuse(l19, l20, l21)
sch.parallel(loop=l29)
sch.annotate(block_or_loop=l29, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l29, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #15: GFLOPs: 0.0000. Time: 0.0397 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #16: GFLOPs: 0.0000. Time: 0.0259 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #17: GFLOPs: 0.0000. Time: 0.9650 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #18: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0_i1_i2_fused in T.parallel(1568, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i3 in T.serial(56):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_reshape"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2 = T.axis.spatial(4, ax1)
                            ax2_1 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                            ax3_1 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                            ax4_1 = T.axis.spatial(56, i3)
                            T.reads(placeholder[0, (ax1_2 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56])
                            T.writes(T_reshape[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T_reshape[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_2 * 28 + (ax4_1 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4_1 // 56 + ax3_1) % 56, ax4_1 % 56]
                    for ax2, ax3 in T.grid(1, 1):
                        with T.block("T_reshape_1"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3 = T.axis.spatial(112, i0_i1_i2_fused // 56 * 4 + ax1)
                            ax2_2 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                            ax3_2 = T.axis.spatial(56, i3)
                            T.reads(T_reshape[0, ((ax3_2 // 56 + ax2_2) // 56 + ax1_3) % 4, ((ax3_2 // 56 + ax2_2) // 56 + ax1_3) % 112 // 4, (ax3_2 // 56 + ax2_2) % 56, ax3_2 % 56])
                            T.writes(T_reshape_1[ax0_3, ax1_3, ax2_2, ax3_2])
                            T_reshape_1[ax0_3, ax1_3, ax2_2, ax3_2] = T_reshape[0, ((ax3_2 // 56 + ax2_2) // 56 + ax1_3) % 4, ((ax3_2 // 56 + ax2_2) // 56 + ax1_3) % 112 // 4, (ax3_2 // 56 + ax2_2) % 56, ax3_2 % 56]
                for i4 in T.serial(4):
                    with T.block("T_layout_trans"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4 = T.axis.spatial(28, i0_i1_i2_fused // 56)
                        ax2_3 = T.axis.spatial(56, i0_i1_i2_fused % 56)
                        ax3_3, ax4 = T.axis.remap("SS", [i3, i4])
                        T.reads(T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_3, ax3_3])
                        T.writes(T_layout_trans[ax0_4, ax1_4, ax2_3, ax3_3, ax4])
                        T_layout_trans[ax0_4, ax1_4, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 112 and ax2_3 < 56 and ax3_3 < 56, T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
l23 = sch.fuse(l12, l13, l14)
sch.parallel(loop=l23)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #19: GFLOPs: 0.0000. Time: 0.0448 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #20: GFLOPs: 0.0000. Time: 1.0783 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #21: GFLOPs: 0.0000. Time: 0.0452 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #22: GFLOPs: 0.0000. Time: 0.0938 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #23: GFLOPs: 0.0000. Time: 0.0519 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #24: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0_i1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 56):
                for ax4_fused in T.vectorized(56):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, ax3, ax4_fused])
                        T.reads(placeholder[0, (ax1_1 * 28 + (ax4 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4 // 56 + ax3_1) % 56, ax4 % 56])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = placeholder[0, (ax1_1 * 28 + (ax4 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4 // 56 + ax3_1) % 56, ax4 % 56]
            for ax0, ax1, ax2 in T.grid(1, 4, 56):
                for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_transpose"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2, ax2_2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax1, ax2, ax0_ax1_ax2_ax3_ax4_fused])
                        T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3, ax4])
                        T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3, ax4])
                        T_transpose[ax0_2, ax1_2, ax2_2, ax3, ax4] = T_reshape[ax0_2, ax2_2, ax1_2, ax3, ax4]
                for ax3_fused in T.vectorized(56):
                    with T.block("T_reshape_1"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3 = T.axis.spatial(112, i0_i1_fused * 4 + ax1)
                        ax2_3, ax3 = T.axis.remap("SS", [ax2, ax3_fused])
                        T.reads(T_transpose[0, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 112 // 4, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 4, (ax3 // 56 + ax2_3) % 56, ax3 % 56])
                        T.writes(T_reshape_1[ax0_3, ax1_3, ax2_3, ax3])
                        T_reshape_1[ax0_3, ax1_3, ax2_3, ax3] = T_transpose[0, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 112 // 4, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 4, (ax3 // 56 + ax2_3) % 56, ax3 % 56]
            for i2, i3, i4 in T.grid(56, 56, 4):
                with T.block("T_layout_trans"):
                    ax0_4 = T.axis.spatial(1, 0)
                    ax1_4, ax2_4, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3])
                    T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3, ax4])
                    T_layout_trans[ax0_4, ax1_4, ax2_4, ax3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 112 and ax2_4 < 56 and ax3 < 56, T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b10)
l31 = sch.fuse(l26, l27, l28, l29, l30)
sch.vectorize(loop=l31)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36 = sch.get_loops(block=b11)
l37 = sch.fuse(l36)
sch.vectorize(loop=l37)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l38, l39, l40, l41 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l38, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l38, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #25: GFLOPs: 0.0000. Time: 0.0514 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #26: GFLOPs: 0.0000. Time: 0.0552 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #27: GFLOPs: 0.0000. Time: 0.0301 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #28: GFLOPs: 0.0000. Time: 0.0149 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #29: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 56, 56), "float32"], T_layout_trans: T.Buffer[(1, 28, 56, 56, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 28, 56, 56], dtype="float32")
        T_transpose = T.alloc_buffer([1, 28, 4, 56, 56], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 112, 56, 56], dtype="float32")
        for i0_i1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 56):
                for ax4_fused in T.vectorized(56):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, ax3, ax4_fused])
                        T.reads(placeholder[0, (ax1_1 * 28 + (ax4 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4 // 56 + ax3_1) % 56, ax4 % 56])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = placeholder[0, (ax1_1 * 28 + (ax4 // 56 + ax3_1) // 56 + ax2_1) % 112, (ax4 // 56 + ax3_1) % 56, ax4 % 56]
            for ax0, ax1, ax2 in T.grid(1, 4, 56):
                for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(56):
                    with T.block("T_transpose"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2, ax2_2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax1, ax2, ax0_ax1_ax2_ax3_ax4_fused])
                        T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3, ax4])
                        T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3, ax4])
                        T_transpose[ax0_2, ax1_2, ax2_2, ax3, ax4] = T_reshape[ax0_2, ax2_2, ax1_2, ax3, ax4]
                for ax3_fused in T.vectorized(56):
                    with T.block("T_reshape_1"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3 = T.axis.spatial(112, i0_i1_fused * 4 + ax1)
                        ax2_3, ax3 = T.axis.remap("SS", [ax2, ax3_fused])
                        T.reads(T_transpose[0, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 112 // 4, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 4, (ax3 // 56 + ax2_3) % 56, ax3 % 56])
                        T.writes(T_reshape_1[ax0_3, ax1_3, ax2_3, ax3])
                        T_reshape_1[ax0_3, ax1_3, ax2_3, ax3] = T_transpose[0, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 112 // 4, ((ax3 // 56 + ax2_3) // 56 + ax1_3) % 4, (ax3 // 56 + ax2_3) % 56, ax3 % 56]
            for i2, i3, i4 in T.grid(56, 56, 4):
                with T.block("T_layout_trans"):
                    ax0_4 = T.axis.spatial(1, 0)
                    ax1_4, ax2_4, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3])
                    T.writes(T_layout_trans[ax0_4, ax1_4, ax2_4, ax3, ax4])
                    T_layout_trans[ax0_4, ax1_4, ax2_4, ax3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 112 and ax2_4 < 56 and ax3 < 56, T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2_4, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=1)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b10)
l31 = sch.fuse(l26, l27, l28, l29, l30)
sch.vectorize(loop=l31)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36 = sch.get_loops(block=b11)
l37 = sch.fuse(l36)
sch.vectorize(loop=l37)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l38, l39, l40, l41 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l38, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l38, ann_key="pragma_unroll_explicit", ann_val=1)
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #30: GFLOPs: 0.0000. Time: 0.3872 ms. Best GFLOPs: 0.0000
[17:18:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_reshape_transpose_reshape_layout_transform"] Trial #31: GFLOPs: 0.0000. Time: 0.1777 ms. Best GFLOPs: 0.0000
[17:18:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_reshape_transpose_reshape_layout_transform"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 180.386

[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #0: GFLOPs: 5.2898. Time: 0.3154 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #1: GFLOPs: 0.7416. Time: 2.2496 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #2: GFLOPs: 0.2286. Time: 7.2969 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #3: GFLOPs: 0.7800. Time: 2.1390 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #4: GFLOPs: 3.0370. Time: 0.5494 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #5: GFLOPs: 1.1498. Time: 1.4509 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #6: GFLOPs: 1.8383. Time: 0.9076 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #7: GFLOPs: 2.5008. Time: 0.6671 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #8: GFLOPs: 1.5090. Time: 1.1056 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #9: GFLOPs: 0.2489. Time: 6.7020 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #10: GFLOPs: 0.0404. Time: 41.3243 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #11: GFLOPs: 0.6748. Time: 2.4724 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #12: GFLOPs: 1.2565. Time: 1.3277 ms. Best GFLOPs: 5.2898
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #13: GFLOPs: 26.4859. Time: 0.0630 ms. Best GFLOPs: 26.4859
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #14: GFLOPs: 8.3366. Time: 0.2001 ms. Best GFLOPs: 26.4859
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #15: GFLOPs: 17.3072. Time: 0.0964 ms. Best GFLOPs: 26.4859
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #16: GFLOPs: 16.8990. Time: 0.0987 ms. Best GFLOPs: 26.4859
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #17: GFLOPs: 18.8310. Time: 0.0886 ms. Best GFLOPs: 26.4859
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #18: GFLOPs: 28.3995. Time: 0.0587 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #19: GFLOPs: 16.5822. Time: 0.1006 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #20: GFLOPs: 8.2559. Time: 0.2021 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #21: GFLOPs: 21.4012. Time: 0.0780 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #22: GFLOPs: 17.4925. Time: 0.0954 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #23: GFLOPs: 16.4448. Time: 0.1015 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #24: GFLOPs: 16.5125. Time: 0.1010 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #25: GFLOPs: 6.1205. Time: 0.2726 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #26: GFLOPs: 14.0457. Time: 0.1188 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #27: GFLOPs: 6.5610. Time: 0.2543 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #28: GFLOPs: 11.8146. Time: 0.1412 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #29: GFLOPs: 24.5749. Time: 0.0679 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #30: GFLOPs: 14.7181. Time: 0.1134 ms. Best GFLOPs: 28.3995
[17:18:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"] Trial #31: GFLOPs: 11.0591. Time: 0.1509 ms. Best GFLOPs: 28.3995
[17:18:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 239.132

[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0406 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0227 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0245 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0168 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0337 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0316 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0142 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0182 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0152 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0282 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0191 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0261 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0145 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0186 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0171 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0278 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0204 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0160 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0488 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0117 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0141 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0128 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0193 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0130 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #25: GFLOPs: 0.0000. Time: 0.1281 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0357 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0199 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0219 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0129 ms. Best GFLOPs: 0.0000
[17:18:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_layout_transform_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0284 ms. Best GFLOPs: 0.0000
[17:18:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_layout_transform_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 250.586

[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #0: GFLOPs: 25.3854. Time: 0.2006 ms. Best GFLOPs: 25.3854
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #1: GFLOPs: 3.4070. Time: 1.4948 ms. Best GFLOPs: 25.3854
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #2: GFLOPs: 14.2233. Time: 0.3581 ms. Best GFLOPs: 25.3854
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #3: GFLOPs: 7.2574. Time: 0.7017 ms. Best GFLOPs: 25.3854
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #4: GFLOPs: 28.7771. Time: 0.1770 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #5: GFLOPs: 9.2759. Time: 0.5490 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #6: GFLOPs: 20.4354. Time: 0.2492 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #7: GFLOPs: 14.0610. Time: 0.3622 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #8: GFLOPs: 16.2917. Time: 0.3126 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #9: GFLOPs: 12.6040. Time: 0.4041 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #10: GFLOPs: 27.7349. Time: 0.1836 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #11: GFLOPs: 6.8061. Time: 0.7483 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #12: GFLOPs: 12.0499. Time: 0.4226 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #13: GFLOPs: 16.5155. Time: 0.3084 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #14: GFLOPs: 5.6389. Time: 0.9032 ms. Best GFLOPs: 28.7771
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #15: GFLOPs: 33.1480. Time: 0.1536 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #16: GFLOPs: 14.4354. Time: 0.3528 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #17: GFLOPs: 11.8850. Time: 0.4285 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #18: GFLOPs: 12.9031. Time: 0.3947 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #19: GFLOPs: 8.9737. Time: 0.5675 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #20: GFLOPs: 18.2962. Time: 0.2784 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #21: GFLOPs: 10.8085. Time: 0.4712 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #22: GFLOPs: 15.2235. Time: 0.3345 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #23: GFLOPs: 14.3854. Time: 0.3540 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #24: GFLOPs: 19.6720. Time: 0.2589 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #25: GFLOPs: 2.6755. Time: 1.9035 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #26: GFLOPs: 26.8553. Time: 0.1896 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #27: GFLOPs: 28.6097. Time: 0.1780 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #28: GFLOPs: 3.8286. Time: 1.3302 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #29: GFLOPs: 6.8786. Time: 0.7404 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #30: GFLOPs: 11.1468. Time: 0.4569 ms. Best GFLOPs: 33.1480
[17:18:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_multiply_add"] Trial #31: GFLOPs: 7.7656. Time: 0.6558 ms. Best GFLOPs: 33.1480
[17:19:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_multiply_add"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 404.226

[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0181 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #1: GFLOPs: 0.0000. Time: 0.0185 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #2: GFLOPs: 0.0000. Time: 0.0207 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0258 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #4: GFLOPs: 0.0000. Time: 0.0177 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0180 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0152 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0182 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0110 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #9: GFLOPs: 0.0000. Time: 0.0161 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #10: GFLOPs: 0.0000. Time: 0.0309 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #11: GFLOPs: 0.0000. Time: 0.0444 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0206 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #13: GFLOPs: 0.0000. Time: 0.0206 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #14: GFLOPs: 0.0000. Time: 0.0195 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #15: GFLOPs: 0.0000. Time: 0.0146 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #16: GFLOPs: 0.0000. Time: 0.0201 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #17: GFLOPs: 0.0000. Time: 0.0148 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #18: GFLOPs: 0.0000. Time: 0.0201 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #19: GFLOPs: 0.0000. Time: 0.0202 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0203 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0200 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0151 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0193 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #24: GFLOPs: 0.0000. Time: 0.0209 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0194 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #26: GFLOPs: 0.0000. Time: 0.0201 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #27: GFLOPs: 0.0000. Time: 0.0194 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #28: GFLOPs: 0.0000. Time: 0.0202 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #29: GFLOPs: 0.0000. Time: 0.0201 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #30: GFLOPs: 0.0000. Time: 0.0202 ms. Best GFLOPs: 0.0000
[17:19:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_layout_transform_2"] Trial #31: GFLOPs: 0.0000. Time: 0.0209 ms. Best GFLOPs: 0.0000
[17:19:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_layout_transform_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 415.179

[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #0: GFLOPs: 5.3373. Time: 3.2151 ms. Best GFLOPs: 5.3373
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #1: GFLOPs: 1.7161. Time: 9.9994 ms. Best GFLOPs: 5.3373
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #2: GFLOPs: 2.8856. Time: 5.9468 ms. Best GFLOPs: 5.3373
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #3: GFLOPs: 42.6035. Time: 0.4028 ms. Best GFLOPs: 42.6035
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #4: GFLOPs: 8.5641. Time: 2.0037 ms. Best GFLOPs: 42.6035
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #5: GFLOPs: 35.6554. Time: 0.4813 ms. Best GFLOPs: 42.6035
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #6: GFLOPs: 35.0360. Time: 0.4898 ms. Best GFLOPs: 42.6035
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #7: GFLOPs: 12.7103. Time: 1.3501 ms. Best GFLOPs: 42.6035
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #8: GFLOPs: 84.6548. Time: 0.2027 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #9: GFLOPs: 19.3612. Time: 0.8863 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #10: GFLOPs: 41.2472. Time: 0.4160 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #11: GFLOPs: 52.3909. Time: 0.3275 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #12: GFLOPs: 41.8863. Time: 0.4097 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #13: GFLOPs: 13.0554. Time: 1.3144 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1, 224, 224, 3), "float32"], placeholder_1: T.Buffer[(6, 1, 3, 3, 3, 4), "float32"], placeholder_2: T.Buffer[(1, 6, 1, 1, 4), "float32"], placeholder_3: T.Buffer[(1, 6, 1, 1, 4), "float32"], T_relu: T.Buffer[(1, 6, 112, 112, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_pad = T.alloc_buffer([1, 1, 226, 226, 3], dtype="float32")
        conv2d_NCHWc = T.alloc_buffer([1, 6, 112, 112, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused in T.parallel(256, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 29, 29):
                for ax4_fused in T.vectorized(3):
                    with T.block("data_pad"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 256 // 32 * 28 + ax2)
                        i3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 4 * 28 + ax3)
                        i4 = T.axis.spatial(3, ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(data_pad[i0, i1, i2, i3, i4])
                        data_pad[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 225 and 1 <= i3 and i3 < 225, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i1_2_init, i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(3, 7, 2, 2, 14):
                with T.block("conv2d_NCHWc_init"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_2_init * 2 + i1_3_init)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 14 + i2_2_init * 2 + i2_3_init)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 4 * 14 + i3_3_init)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads()
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
            for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 3, 1, 3, 7, 1, 1, 1, 1, 1, 1, 2, 2, 14, 1):
                with T.block("conv2d_NCHWc_update"):
                    n = T.axis.spatial(1, 0)
                    oc_chunk = T.axis.spatial(6, i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 4 * 14 + i3_3)
                    oc_block = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    ic, kh, kw = T.axis.remap("RRR", [i5_0, i6_0, i7_0])
                    T.reads(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block], data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3], placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["conv2d_NCHWc.x86", ["TENSOR", [1, 1, 224, 224, 3], "float32"], ["TENSOR", [6, 1, 3, 3, 3, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW3c", "NCHW4c", "float32"]})
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + data_pad[n, ic // 3, oh * 2 + kh, ow * 2 + kw, ic % 3] * placeholder_1[oc_chunk, ic // 3, kh, kw, ic % 3, oc_block]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 6, 14, 14, 1):
                with T.block("T_relu"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1 = T.axis.spatial(6, ax1)
                    ax2_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused // 32 * 14 + ax2)
                    ax3_1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 32 // 4 * 14 + ax3)
                    ax4_1 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_i4_1_fused % 4)
                    T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1], placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1] + placeholder_3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="data_pad", func_name="main")
b1 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l5, l6, l7, l8, l9, l10, l11, l12 = sch.get_loops(block=b1)
v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l17, l18, l19, l20 = sch.split(loop=l5, factors=[v13, v14, v15, v16])
v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 3, 2])
l25, l26, l27, l28 = sch.split(loop=l6, factors=[v21, v22, v23, v24])
v29, v30, v31, v32 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[8, 1, 7, 2])
l33, l34, l35, l36 = sch.split(loop=l7, factors=[v29, v30, v31, v32])
v37, v38, v39, v40 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 8, 1, 14])
l41, l42, l43, l44 = sch.split(loop=l8, factors=[v37, v38, v39, v40])
v45, v46, v47, v48 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l49, l50, l51, l52 = sch.split(loop=l9, factors=[v45, v46, v47, v48])
v53, v54 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[3, 1])
l55, l56 = sch.split(loop=l10, factors=[v53, v54])
v57, v58 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[3, 1])
l59, l60 = sch.split(loop=l11, factors=[v57, v58])
v61, v62 = sch.sample_perfect_tile(loop=l12, n=2, max_innermost_factor=64, decision=[3, 1])
l63, l64 = sch.split(loop=l12, factors=[v61, v62])
sch.reorder(l17, l25, l33, l41, l49, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51, l56, l60, l64, l20, l28, l36, l44, l52)
b65, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b65, loop=l50, preserve_unit_loops=True)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.vectorize", ann_val=64)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
l67 = sch.sample_compute_location(block=b0, decision=9)
sch.compute_at(block=b0, loop=l67, preserve_unit_loops=True)
sch.enter_postproc()
b68 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b68, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b68, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b68, ann_key="meta_schedule.unroll_explicit")
b69, b70, b71 = sch.get_child_blocks(b68)
l72, l73, l74, l75, l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b69)
l87 = sch.fuse(l72, l73, l74, l75, l76, l77, l78, l79, l80, l81)
sch.parallel(loop=l87)
l88 = sch.fuse(l86)
sch.vectorize(loop=l88)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l106, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l106, ann_key="pragma_unroll_explicit", ann_val=1)
b112 = sch.get_block(name="conv2d_NCHWc", func_name="main")
l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129 = sch.get_loops(block=b112)
b130 = sch.decompose_reduction(block=b112, loop=l114)
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #15: GFLOPs: 13.6136. Time: 1.2605 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #16: GFLOPs: 28.5973. Time: 0.6001 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #17: GFLOPs: 12.4518. Time: 1.3781 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #18: GFLOPs: 47.9225. Time: 0.3581 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #19: GFLOPs: 31.1765. Time: 0.5504 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #20: GFLOPs: 26.0656. Time: 0.6583 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #21: GFLOPs: 6.4912. Time: 2.6436 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #22: GFLOPs: 29.2924. Time: 0.5858 ms. Best GFLOPs: 84.6548
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #23: GFLOPs: 99.1783. Time: 0.1730 ms. Best GFLOPs: 99.1783
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #24: GFLOPs: 14.9364. Time: 1.1489 ms. Best GFLOPs: 99.1783
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #25: GFLOPs: 26.9150. Time: 0.6376 ms. Best GFLOPs: 99.1783
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #26: GFLOPs: 109.2976. Time: 0.1570 ms. Best GFLOPs: 109.2976
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #27: GFLOPs: 9.5623. Time: 1.7946 ms. Best GFLOPs: 109.2976
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #28: GFLOPs: 18.9952. Time: 0.9034 ms. Best GFLOPs: 109.2976
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #29: GFLOPs: 37.9549. Time: 0.4521 ms. Best GFLOPs: 109.2976
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #30: GFLOPs: 28.9759. Time: 0.5922 ms. Best GFLOPs: 109.2976
[17:19:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"] Trial #31: GFLOPs: 48.9731. Time: 0.3504 ms. Best GFLOPs: 109.2976
[17:19:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 572.184

[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 11.8604. Time: 0.0571 ms. Best GFLOPs: 11.8604
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 3.7093. Time: 0.1826 ms. Best GFLOPs: 11.8604
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 9.7568. Time: 0.0694 ms. Best GFLOPs: 11.8604
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 7.1560. Time: 0.0947 ms. Best GFLOPs: 11.8604
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 15.6763. Time: 0.0432 ms. Best GFLOPs: 15.6763
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 8.8698. Time: 0.0764 ms. Best GFLOPs: 15.6763
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 13.6500. Time: 0.0496 ms. Best GFLOPs: 15.6763
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 13.1670. Time: 0.0514 ms. Best GFLOPs: 15.6763
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 9.9955. Time: 0.0678 ms. Best GFLOPs: 15.6763
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 26.4159. Time: 0.0256 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 10.6777. Time: 0.0634 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 15.1030. Time: 0.0449 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 8.8359. Time: 0.0767 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 7.1330. Time: 0.0950 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 9.7126. Time: 0.0697 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 16.6593. Time: 0.0407 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 13.9569. Time: 0.0485 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 10.6313. Time: 0.0637 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 10.9225. Time: 0.0620 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 15.3597. Time: 0.0441 ms. Best GFLOPs: 26.4159
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 26.6388. Time: 0.0254 ms. Best GFLOPs: 26.6388
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 32.5675. Time: 0.0208 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 25.8976. Time: 0.0262 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 15.5045. Time: 0.0437 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 25.3774. Time: 0.0267 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 19.8280. Time: 0.0342 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 9.4482. Time: 0.0717 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 11.3321. Time: 0.0598 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 19.3733. Time: 0.0350 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 15.3879. Time: 0.0440 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 10.9650. Time: 0.0618 ms. Best GFLOPs: 32.5675
[17:19:03] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 13.4228. Time: 0.0505 ms. Best GFLOPs: 32.5675
[17:19:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_max_pool2d"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 592.983

[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #0: GFLOPs: 3.3984. Time: 0.1772 ms. Best GFLOPs: 3.3984
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #1: GFLOPs: 22.4897. Time: 0.0268 ms. Best GFLOPs: 22.4897
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #2: GFLOPs: 27.2759. Time: 0.0221 ms. Best GFLOPs: 27.2759
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #3: GFLOPs: 31.7525. Time: 0.0190 ms. Best GFLOPs: 31.7525
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #4: GFLOPs: 33.8298. Time: 0.0178 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #5: GFLOPs: 26.7874. Time: 0.0225 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #6: GFLOPs: 13.1801. Time: 0.0457 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #7: GFLOPs: 28.1502. Time: 0.0214 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #8: GFLOPs: 30.6073. Time: 0.0197 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #9: GFLOPs: 33.4862. Time: 0.0180 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #10: GFLOPs: 28.8213. Time: 0.0209 ms. Best GFLOPs: 33.8298
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #11: GFLOPs: 35.2223. Time: 0.0171 ms. Best GFLOPs: 35.2223
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #12: GFLOPs: 1.9252. Time: 0.3128 ms. Best GFLOPs: 35.2223
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #13: GFLOPs: 1.5696. Time: 0.3836 ms. Best GFLOPs: 35.2223
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #14: GFLOPs: 4.2036. Time: 0.1432 ms. Best GFLOPs: 35.2223
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #15: GFLOPs: 53.9159. Time: 0.0112 ms. Best GFLOPs: 53.9159
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #16: GFLOPs: 89.0907. Time: 0.0068 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #17: GFLOPs: 5.4727. Time: 0.1100 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #18: GFLOPs: 27.1717. Time: 0.0222 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #19: GFLOPs: 50.7476. Time: 0.0119 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #20: GFLOPs: 28.3636. Time: 0.0212 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #21: GFLOPs: 32.0247. Time: 0.0188 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #22: GFLOPs: 7.6256. Time: 0.0790 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #23: GFLOPs: 4.4531. Time: 0.1352 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #24: GFLOPs: 8.5169. Time: 0.0707 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #25: GFLOPs: 23.4362. Time: 0.0257 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #26: GFLOPs: 13.2936. Time: 0.0453 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #27: GFLOPs: 33.6742. Time: 0.0179 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #28: GFLOPs: 38.4532. Time: 0.0157 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #29: GFLOPs: 25.0965. Time: 0.0240 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #30: GFLOPs: 43.2197. Time: 0.0139 ms. Best GFLOPs: 89.0907
[17:19:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_avg_pool2d_2"] Trial #31: GFLOPs: 7.7763. Time: 0.0774 ms. Best GFLOPs: 89.0907
[17:19:06] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_avg_pool2d_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 599.741

[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #0: GFLOPs: 3.2434. Time: 0.0329 ms. Best GFLOPs: 3.2434
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #1: GFLOPs: 6.1598. Time: 0.0173 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #2: GFLOPs: 4.2597. Time: 0.0250 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #3: GFLOPs: 3.7948. Time: 0.0281 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #4: GFLOPs: 3.8849. Time: 0.0274 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #5: GFLOPs: 5.8455. Time: 0.0182 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #6: GFLOPs: 3.4578. Time: 0.0308 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #7: GFLOPs: 2.6514. Time: 0.0402 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #8: GFLOPs: 2.6016. Time: 0.0410 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #9: GFLOPs: 3.3145. Time: 0.0322 ms. Best GFLOPs: 6.1598
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #10: GFLOPs: 7.6086. Time: 0.0140 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #11: GFLOPs: 6.4095. Time: 0.0166 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #12: GFLOPs: 5.1093. Time: 0.0209 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #13: GFLOPs: 4.2166. Time: 0.0253 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #14: GFLOPs: 3.6047. Time: 0.0296 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #15: GFLOPs: 2.3547. Time: 0.0453 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #16: GFLOPs: 3.7941. Time: 0.0281 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #17: GFLOPs: 7.2884. Time: 0.0146 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #18: GFLOPs: 3.1828. Time: 0.0335 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #19: GFLOPs: 2.6424. Time: 0.0404 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #20: GFLOPs: 3.6437. Time: 0.0293 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #21: GFLOPs: 6.0100. Time: 0.0177 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #22: GFLOPs: 2.3086. Time: 0.0462 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #23: GFLOPs: 4.9949. Time: 0.0213 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #24: GFLOPs: 3.8440. Time: 0.0277 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #25: GFLOPs: 3.3525. Time: 0.0318 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #26: GFLOPs: 3.4053. Time: 0.0313 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #27: GFLOPs: 2.5947. Time: 0.0411 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #28: GFLOPs: 6.0071. Time: 0.0177 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #29: GFLOPs: 5.6160. Time: 0.0190 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #30: GFLOPs: 2.7410. Time: 0.0389 ms. Best GFLOPs: 7.6086
[17:19:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_layout_transform_concatenate_nn_relu"] Trial #31: GFLOPs: 8.2273. Time: 0.0130 ms. Best GFLOPs: 8.2273
[17:19:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_layout_transform_concatenate_nn_relu"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 612.701

[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #0: GFLOPs: 3.3190. Time: 0.6104 ms. Best GFLOPs: 3.3190
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #1: GFLOPs: 1.1619. Time: 1.7436 ms. Best GFLOPs: 3.3190
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #2: GFLOPs: 2.3440. Time: 0.8643 ms. Best GFLOPs: 3.3190
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #3: GFLOPs: 27.4421. Time: 0.0738 ms. Best GFLOPs: 27.4421
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #4: GFLOPs: 98.6325. Time: 0.0205 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #5: GFLOPs: 43.7711. Time: 0.0463 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #6: GFLOPs: 24.9521. Time: 0.0812 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #7: GFLOPs: 32.3776. Time: 0.0626 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #8: GFLOPs: 30.1834. Time: 0.0671 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #9: GFLOPs: 26.6026. Time: 0.0762 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #10: GFLOPs: 48.2186. Time: 0.0420 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #11: GFLOPs: 4.4109. Time: 0.4593 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #12: GFLOPs: 15.8578. Time: 0.1278 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #13: GFLOPs: 16.8994. Time: 0.1199 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #14: GFLOPs: 10.0159. Time: 0.2023 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #15: GFLOPs: 16.8789. Time: 0.1200 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #16: GFLOPs: 36.2108. Time: 0.0559 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #17: GFLOPs: 41.3051. Time: 0.0490 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #18: GFLOPs: 3.5809. Time: 0.5657 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #19: GFLOPs: 22.0361. Time: 0.0919 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #20: GFLOPs: 5.6390. Time: 0.3593 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #21: GFLOPs: 41.2033. Time: 0.0492 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #22: GFLOPs: 39.7206. Time: 0.0510 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #23: GFLOPs: 38.9983. Time: 0.0519 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #24: GFLOPs: 6.6285. Time: 0.3056 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #25: GFLOPs: 41.1548. Time: 0.0492 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #26: GFLOPs: 12.6348. Time: 0.1603 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #27: GFLOPs: 16.2257. Time: 0.1249 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #28: GFLOPs: 22.6966. Time: 0.0893 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #29: GFLOPs: 15.6348. Time: 0.1296 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #30: GFLOPs: 26.5067. Time: 0.0764 ms. Best GFLOPs: 98.6325
[17:19:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"] Trial #31: GFLOPs: 26.2872. Time: 0.0771 ms. Best GFLOPs: 98.6325
[17:19:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 674.319

[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #0: GFLOPs: 0.0000. Time: 0.0121 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #1: GFLOPs: 0.0000. Time: 0.0118 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #2: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #3: GFLOPs: 0.0000. Time: 0.0111 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #4: GFLOPs: 0.0000. Time: 0.0533 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #5: GFLOPs: 0.0000. Time: 2.1716 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #6: GFLOPs: 0.0000. Time: 4.3074 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #7: GFLOPs: 0.0000. Time: 3.9998 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #8: GFLOPs: 0.0000. Time: 4.6153 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #9: GFLOPs: 0.0000. Time: 1.6616 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #10: GFLOPs: 0.0000. Time: 0.7861 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #11: GFLOPs: 0.0000. Time: 0.0847 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #12: GFLOPs: 0.0000. Time: 0.0921 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #13: GFLOPs: 0.0000. Time: 0.0681 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #14: GFLOPs: 0.0000. Time: 0.1159 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #15: GFLOPs: 0.0000. Time: 0.1015 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #16: GFLOPs: 0.0000. Time: 0.2216 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #17: GFLOPs: 0.0000. Time: 0.1439 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #18: GFLOPs: 0.0000. Time: 0.0962 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #19: GFLOPs: 0.0000. Time: 0.0112 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #20: GFLOPs: 0.0000. Time: 0.0116 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #21: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #22: GFLOPs: 0.0000. Time: 0.0131 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #23: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #24: GFLOPs: 0.0000. Time: 0.0114 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #25: GFLOPs: 0.0000. Time: 0.0111 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #26: GFLOPs: 0.0000. Time: 0.0263 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #27: GFLOPs: 0.0000. Time: 0.0115 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #28: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #29: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #30: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[17:19:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_layout_transform_3"] Trial #31: GFLOPs: 0.0000. Time: 0.0118 ms. Best GFLOPs: 0.0000
[17:19:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_layout_transform_3"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |            N/A |          N/A |                   N/A |      0 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 706.57

[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #0: GFLOPs: 4.9354. Time: 1.5555 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #1: GFLOPs: 0.9872. Time: 7.7765 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #2: GFLOPs: 0.8229. Time: 9.3290 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], placeholder_1: T.Buffer[(136, 34, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 136, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 136, 28, 28), "float32"], T_relu: T.Buffer[(1, 136, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 28, 2, 28], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
        output_unpack = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        conv_global = T.alloc_buffer([4, 1, 17, 28, 28, 2], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(7, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3, ax4 in T.grid(4, 1, 17, 28, 2):
                for ax5_fused in T.vectorized(4):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, ax0)
                        n = T.axis.spatial(1, 0)
                        C, h, c = T.axis.remap("SSS", [ax2, ax3, ax4])
                        w = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax5_fused)
                        T.reads(placeholder[n, g * 34 + C * 2 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 34 + C * 2 + c, h, w]
            for i5_0 in T.serial(1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(1, 1, 1, 1, 1, 2):
                    for i3_2_init, i4_2_init, i0_3_init, i2_3_init, i3_3_init in T.grid(4, 4, 4, 17, 7):
                        with T.block("conv_init"):
                            g = T.axis.spatial(4, i0_3_init)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(17, i2_3_init)
                            oh = T.axis.spatial(28, i3_2_init * 7 + i3_3_init)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i4_2_init)
                            oc_block = T.axis.spatial(2, i5_1)
                            T.reads()
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(2, 1, 1, 1, 1, 1, 4, 4, 1, 17, 1, 1, 4, 1, 17, 7, 1, 1):
                        with T.block("conv_update"):
                            g = T.axis.spatial(4, i0_3)
                            n = T.axis.spatial(1, 0)
                            oc_chunk = T.axis.spatial(17, i2_3)
                            oh = T.axis.spatial(28, i3_2 * 7 + i3_3)
                            ow = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + i4_2)
                            oc_block = T.axis.spatial(2, i5_1)
                            ic = T.axis.reduce(34, i6_0 * 17 + i6_1)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv_global[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw], placeholder_1[g * 34 + oc_chunk * 2 + oc_block, ic, kh, kw])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 2, oh + kh, ic % 2, ow + kw] * placeholder_1[g * 34 + oc_chunk * 2 + oc_block, ic // 2 * 2 + ic % 2, kh, kw]
                for ax0, ax1, ax2, ax3 in T.grid(4, 1, 17, 28):
                    for ax4_ax5_fused in T.vectorized(8):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, ax0)
                            v1 = T.axis.spatial(1, 0)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            v4 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 4 + ax4_ax5_fused // 2)
                            v5 = T.axis.spatial(2, ax4_ax5_fused % 2)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(28, 28):
                with T.block("output_unpack"):
                    n = T.axis.spatial(1, 0)
                    c, h, w = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(conv[c // 34, n, c % 34 // 2, h, w, c % 2])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 136, 28, 28], "float32"], ["TENSOR", [136, 34, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 34, n, c % 34 // 2, h, w, c % 2]
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(28):
                for i3_fused in T.vectorized(28):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 4, 7])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[7, 1, 4, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 2, 1, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[2, 17])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=-1)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
sch.enter_postproc()
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86, b87 = sch.get_child_blocks(b82)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b83)
l99 = sch.fuse(l88, l89, l90, l91, l92)
sch.parallel(loop=l99)
l100 = sch.fuse(l98)
sch.vectorize(loop=l100)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b85)
l135 = sch.fuse(l133, l134)
sch.vectorize(loop=l135)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l136, l137, l138, l139 = sch.get_loops(block=b86)
l140 = sch.fuse(l136, l137)
sch.parallel(loop=l140)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144 = sch.get_loops(block=b87)
l145 = sch.fuse(l141, l142)
sch.parallel(loop=l145)
l146 = sch.fuse(l144)
sch.vectorize(loop=l146)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
b147 = sch.get_block(name="conv", func_name="main")
l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173 = sch.get_loops(block=b147)
b174 = sch.decompose_reduction(block=b147, loop=l156)
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #4: GFLOPs: 0.9596. Time: 7.9999 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #5: GFLOPs: 1.0284. Time: 7.4653 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #6: GFLOPs: 0.6045. Time: 12.6996 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #7: GFLOPs: 1.1187. Time: 6.8621 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #8: GFLOPs: 0.9945. Time: 7.7190 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #9: GFLOPs: 0.8764. Time: 8.7595 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #10: GFLOPs: 1.0207. Time: 7.5209 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #11: GFLOPs: 1.3556. Time: 5.6632 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #12: GFLOPs: 1.1589. Time: 6.6242 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #13: GFLOPs: 0.8007. Time: 9.5872 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #14: GFLOPs: 0.7866. Time: 9.7593 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #15: GFLOPs: 0.9596. Time: 8.0004 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #16: GFLOPs: 0.3839. Time: 19.9979 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #17: GFLOPs: 3.2122. Time: 2.3900 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #18: GFLOPs: 1.4447. Time: 5.3140 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #19: GFLOPs: 4.1893. Time: 1.8325 ms. Best GFLOPs: 4.9354
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #20: GFLOPs: 9.8906. Time: 0.7762 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #21: GFLOPs: 2.3140. Time: 3.3176 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #22: GFLOPs: 3.3436. Time: 2.2960 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #23: GFLOPs: 4.0501. Time: 1.8955 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #24: GFLOPs: 3.8295. Time: 2.0047 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #25: GFLOPs: 5.7684. Time: 1.3309 ms. Best GFLOPs: 9.8906
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #26: GFLOPs: 28.3561. Time: 0.2707 ms. Best GFLOPs: 28.3561
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #27: GFLOPs: 10.5293. Time: 0.7291 ms. Best GFLOPs: 28.3561
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #28: GFLOPs: 26.4973. Time: 0.2897 ms. Best GFLOPs: 28.3561
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #29: GFLOPs: 28.0361. Time: 0.2738 ms. Best GFLOPs: 28.3561
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #30: GFLOPs: 76.3999. Time: 0.1005 ms. Best GFLOPs: 76.3999
[17:19:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"] Trial #31: GFLOPs: 20.4970. Time: 0.3745 ms. Best GFLOPs: 76.3999
[17:19:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 1008.02

[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #0: GFLOPs: 0.9429. Time: 8.0288 ms. Best GFLOPs: 0.9429
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #1: GFLOPs: 1.7971. Time: 4.2125 ms. Best GFLOPs: 1.7971
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #2: GFLOPs: 4.6122. Time: 1.6414 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #3: GFLOPs: 3.6386. Time: 2.0806 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #4: GFLOPs: 1.8927. Time: 3.9998 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #5: GFLOPs: 2.5993. Time: 2.9124 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #6: GFLOPs: 4.2798. Time: 1.7689 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #7: GFLOPs: 0.1456. Time: 51.9951 ms. Best GFLOPs: 4.6122
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #8: GFLOPs: 17.0380. Time: 0.4443 ms. Best GFLOPs: 17.0380
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #9: GFLOPs: 19.9696. Time: 0.3791 ms. Best GFLOPs: 19.9696
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #10: GFLOPs: 46.4955. Time: 0.1628 ms. Best GFLOPs: 46.4955
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #11: GFLOPs: 10.0354. Time: 0.7544 ms. Best GFLOPs: 46.4955
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #12: GFLOPs: 6.3371. Time: 1.1946 ms. Best GFLOPs: 46.4955
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #13: GFLOPs: 5.0969. Time: 1.4853 ms. Best GFLOPs: 46.4955
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #14: GFLOPs: 13.8658. Time: 0.5460 ms. Best GFLOPs: 46.4955
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #15: GFLOPs: 65.5622. Time: 0.1155 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #16: GFLOPs: 1.8716. Time: 4.0447 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #17: GFLOPs: 1.4758. Time: 5.1296 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #18: GFLOPs: 2.9968. Time: 2.5261 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #19: GFLOPs: 22.8832. Time: 0.3308 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #20: GFLOPs: 6.1334. Time: 1.2343 ms. Best GFLOPs: 65.5622
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #21: GFLOPs: 65.7743. Time: 0.1151 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #22: GFLOPs: 24.9562. Time: 0.3033 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #23: GFLOPs: 3.8056. Time: 1.9892 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #24: GFLOPs: 1.9928. Time: 3.7989 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #25: GFLOPs: 8.7045. Time: 0.8697 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #26: GFLOPs: 3.2146. Time: 2.3550 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #27: GFLOPs: 11.2552. Time: 0.6726 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #28: GFLOPs: 1.9928. Time: 3.7988 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #29: GFLOPs: 7.2297. Time: 1.0471 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #30: GFLOPs: 31.9078. Time: 0.2373 ms. Best GFLOPs: 65.7743
[17:19:14] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"] Trial #31: GFLOPs: 2.1063. Time: 3.5940 ms. Best GFLOPs: 65.7743
[17:19:17] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |            N/A |          N/A |                   N/A |      0 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 1468.4

[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #0: GFLOPs: 0.0000. Time: 1.2662 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0362 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #2: GFLOPs: 0.0000. Time: 0.2488 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #3: GFLOPs: 0.0000. Time: 0.2035 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0548 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #5: GFLOPs: 0.0000. Time: 0.2853 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #6: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        for i0_i1_i2_fused in T.parallel(952, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 1, 1):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(4, ax1)
                        ax2_1 = T.axis.spatial(34, i0_i1_i2_fused // 28)
                        ax3_1 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax4_1 = T.axis.spatial(28, i3)
                        T.reads(placeholder[0, (ax1_1 * 34 + (ax4_1 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4_1 // 28 + ax3_1) % 28, ax4_1 % 28])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_1 * 34 + (ax4_1 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4_1 // 28 + ax3_1) % 28, ax4_1 % 28]
                for i4 in T.serial(4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2 = T.axis.spatial(34, i0_i1_i2_fused // 28)
                            ax2_2 = T.axis.spatial(4, i4)
                            ax3_2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                            ax4_2 = T.axis.spatial(28, i3)
                            T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_2])
                            T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                            T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_2]
                    with T.block("T_layout_trans"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3 = T.axis.spatial(34, i0_i1_i2_fused // 28)
                        ax2_3 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3_3, ax4_3 = T.axis.remap("SS", [i3, i4])
                        T.reads(T_transpose[0, (ax1_3 * 4 + (ax3_3 // 28 + ax2_3) // 28 + ax4_3) % 136 // 4, ((ax3_3 // 28 + ax2_3) // 28 + ax4_3) % 4, (ax3_3 // 28 + ax2_3) % 28, ax3_3 % 28])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4_3 < 136 and ax2_3 < 28 and ax3_3 < 28, T_transpose[0, ((ax3_3 // 28 + ax2_3) // 28 + (ax1_3 * 4 + ax4_3)) % 136 // 4, ((ax3_3 // 28 + ax2_3) // 28 + (ax1_3 * 4 + ax4_3)) % 4, (ax3_3 // 28 + ax2_3) % 28, ax3_3 % 28], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b9)
l21 = sch.fuse(l12, l13, l14)
sch.parallel(loop=l21)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l30, l31, l32 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #7: GFLOPs: 0.0000. Time: 0.1387 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #8: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0_i1_i2_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3 in T.serial(28):
                for i4_fused in T.vectorized(28):
                    with T.block("T_reshape"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(4, i0_i1_i2_fused // 34)
                        ax2 = T.axis.spatial(34, i0_i1_i2_fused % 34)
                        ax3, ax4 = T.axis.remap("SS", [i3, i4_fused])
                        T.reads(placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28])
                        T.writes(T_reshape[ax0, ax1, ax2, ax3, ax4])
                        T_reshape[ax0, ax1, ax2, ax3, ax4] = placeholder[0, (ax1 * 34 + (ax4 // 28 + ax3) // 28 + ax2) % 136, (ax4 // 28 + ax3) % 28, ax4 % 28]
        for i0_i1_i2_fused in T.parallel(952, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4 in T.grid(28, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_transpose"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1 = T.axis.spatial(34, i0_i1_i2_fused // 28)
                        ax2_1 = T.axis.spatial(4, i4)
                        ax3_1 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax4_1 = T.axis.spatial(28, i3)
                        T.reads(T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1])
                        T.writes(T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_transpose[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T_reshape[ax0_1, ax2_1, ax1_1, ax3_1, ax4_1]
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("T_reshape_1"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2 = T.axis.spatial(136, i0_i1_i2_fused // 28 * 4 + i4)
                        ax2_2 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                        ax3_2 = T.axis.spatial(28, i3)
                        T.reads(T_transpose[0, ((ax3_2 // 28 + ax2_2) // 28 + ax1_2) % 136 // 4, ((ax3_2 // 28 + ax2_2) // 28 + ax1_2) % 4, (ax3_2 // 28 + ax2_2) % 28, ax3_2 % 28])
                        T.writes(T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2])
                        T_reshape_1[ax0_2, ax1_2, ax2_2, ax3_2] = T_transpose[0, ((ax3_2 // 28 + ax2_2) // 28 + ax1_2) % 136 // 4, ((ax3_2 // 28 + ax2_2) // 28 + ax1_2) % 4, (ax3_2 // 28 + ax2_2) % 28, ax3_2 % 28]
                with T.block("T_layout_trans"):
                    ax0_3 = T.axis.spatial(1, 0)
                    ax1_3 = T.axis.spatial(34, i0_i1_i2_fused // 28)
                    ax2_3 = T.axis.spatial(28, i0_i1_i2_fused % 28)
                    ax3_3, ax4 = T.axis.remap("SS", [i3, i4])
                    T.reads(T_reshape_1[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3])
                    T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4])
                    T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4 < 136 and ax2_3 < 28 and ax3_3 < 28, T_reshape_1[ax0_3, ax1_3 * 4 + ax4, ax2_3, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17 = sch.get_loops(block=b9)
l18 = sch.fuse(l13, l14, l15)
sch.parallel(loop=l18)
l19 = sch.fuse(l17)
sch.vectorize(loop=l19)
sch.annotate(block_or_loop=l18, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l18, ann_key="pragma_unroll_explicit", ann_val=1)
l20, l21, l22, l23, l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b10)
l30 = sch.fuse(l20, l21, l22)
sch.parallel(loop=l30)
sch.annotate(block_or_loop=l30, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l30, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
l38, l39, l40 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l38, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l38, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0334 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0148 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #11: GFLOPs: 0.0000. Time: 0.2049 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0119 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #13: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0_i1_fused in T.parallel(34, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                for ax4_fused in T.vectorized(28):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, ax3, ax4_fused])
                        T.reads(placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28]
            for i2, i3 in T.grid(28, 28):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_2, ax1_2, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3, ax2_2, ax3_2, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, ax1, i2, i3])
                            T.reads(T_reshape[ax0_3, ax2_2, ax1_3, ax3_2, ax4_1])
                            T.writes(T_transpose[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1])
                            T_transpose[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1] = T_reshape[ax0_3, ax2_2, ax1_3, ax3_2, ax4_1]
                    for ax2_3, ax3_3 in T.grid(1, 1):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, 0)
                            ax1_4 = T.axis.spatial(136, i0_i1_fused * 4 + ax1)
                            ax2_4, ax3_4 = T.axis.remap("SS", [i2, i3])
                            T.reads(T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28])
                            T.writes(T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4])
                            T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4] = T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28]
                for i4 in T.serial(4):
                    with T.block("T_layout_trans"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5, ax2_5, ax3_5, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5])
                        T.writes(T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                        T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 136 and ax2_5 < 28 and ax3_5 < 28, T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #14: GFLOPs: 0.0000. Time: 0.1179 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0178 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #16: GFLOPs: 0.0000. Time: 0.2100 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #18: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0_i1_fused in T.parallel(34, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                for ax4_fused in T.vectorized(28):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, ax3, ax4_fused])
                        T.reads(placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28]
            for i2, i3, i4 in T.grid(28, 28, 4):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                    with T.block("T_transpose"):
                        ax0_2 = T.axis.spatial(1, 0)
                        ax1_2, ax2_2, ax3_2, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, i4, i2, i3])
                        T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_1])
                        T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1])
                        T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_1] = T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_1]
                for ax0_3, ax1_3, ax2_3, ax3_3 in T.grid(1, 1, 1, 1):
                    with T.block("T_reshape_1"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4 = T.axis.spatial(136, i0_i1_fused * 4 + i4)
                        ax2_4, ax3_4 = T.axis.remap("SS", [i2, i3])
                        T.reads(T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28])
                        T.writes(T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4])
                        T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4] = T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28]
                with T.block("T_layout_trans"):
                    ax0_5 = T.axis.spatial(1, 0)
                    ax1_5, ax2_5, ax3_5, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                    T.reads(T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5])
                    T.writes(T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                    T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 136 and ax2_5 < 28 and ax3_5 < 28, T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0215 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0157 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #21: GFLOPs: 0.0000. Time: 0.2028 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0589 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0126 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0430 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #25: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 136, 28, 28), "float32"], T_layout_trans: T.Buffer[(1, 34, 28, 28, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 34, 28, 28], dtype="float32")
        T_transpose = T.alloc_buffer([1, 34, 4, 28, 28], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 136, 28, 28], dtype="float32")
        for i0_i1_fused in T.parallel(34, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                for ax4_fused in T.vectorized(28):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, ax3, ax4_fused])
                        T.reads(placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = placeholder[0, (ax1_1 * 34 + (ax4 // 28 + ax3_1) // 28 + ax2_1) % 136, (ax4 // 28 + ax3_1) % 28, ax4 % 28]
            for i2, i3 in T.grid(28, 28):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_2, ax1_2, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3, ax2_2, ax3_2, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, ax1, i2, i3])
                            T.reads(T_reshape[ax0_3, ax2_2, ax1_3, ax3_2, ax4_1])
                            T.writes(T_transpose[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1])
                            T_transpose[ax0_3, ax1_3, ax2_2, ax3_2, ax4_1] = T_reshape[ax0_3, ax2_2, ax1_3, ax3_2, ax4_1]
                    for ax2_3, ax3_3 in T.grid(1, 1):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, 0)
                            ax1_4 = T.axis.spatial(136, i0_i1_fused * 4 + ax1)
                            ax2_4, ax3_4 = T.axis.remap("SS", [i2, i3])
                            T.reads(T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28])
                            T.writes(T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4])
                            T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_4] = T_transpose[0, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 136 // 4, ((ax3_4 // 28 + ax2_4) // 28 + ax1_4) % 4, (ax3_4 // 28 + ax2_4) % 28, ax3_4 % 28]
                for i4 in T.serial(4):
                    with T.block("T_layout_trans"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5, ax2_5, ax3_5, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5])
                        T.writes(T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4])
                        T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_5, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 136 and ax2_5 < 28 and ax3_5 < 28, T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_5], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37, l38 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #26: GFLOPs: 0.0000. Time: 0.2053 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0253 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0110 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0211 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0289 ms. Best GFLOPs: 0.0000
[17:19:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_transpose_reshape_layout_transform_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0138 ms. Best GFLOPs: 0.0000
[17:19:19] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_reshape_transpose_reshape_layout_transform_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |            N/A |          N/A |                   N/A |      0 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 1503.01

[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #0: GFLOPs: 1.0471. Time: 0.4837 ms. Best GFLOPs: 1.0471
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 34, 1, 1, 1):
                for i2_2_init, i4_2_init, i3_3_init in T.grid(14, 2, 14):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco, oh, ow = T.axis.remap("SSS", [i1_1, i2_2_init, i3_3_init])
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_2_init)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0 in T.serial(3):
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 27, 29):
                        for ax4_fused in T.vectorized(2):
                            with T.block("PaddedInput"):
                                i0 = T.axis.spatial(1, ax0)
                                i1 = T.axis.spatial(34, i1_1 + ax1)
                                i2 = T.axis.spatial(30, i5_0 + ax2)
                                i3 = T.axis.spatial(30, ax3)
                                i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 2 * 2 + ax4_fused)
                                T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                                T.writes(PaddedInput[i0, i1, i2, i3, i4])
                                PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 14, 1, 2, 1, 1, 1, 1, 1, 14, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco, oh, ow = T.axis.remap("SSS", [i1_1, i2_2, i3_3])
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_2)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3 in T.grid(1, 34, 14, 14):
                for ax4_fused in T.vectorized(2):
                    with T.block("T_add"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1 = T.axis.remap("SSS", [ax1, ax2, ax3])
                        ax4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax4_fused)
                        T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4], placeholder_2[ax0_1, ax1_1, 0, 0, ax4])
                        T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4])
                        T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 34, 1, 1])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 14, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b62)
l81 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l81)
l82 = sch.fuse(l80)
sch.vectorize(loop=l82)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l83, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l83, ann_key="pragma_unroll_explicit", ann_val=1)
l103, l104, l105, l106, l107, l108 = sch.get_loops(block=b64)
l109 = sch.fuse(l108)
sch.vectorize(loop=l109)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
b110 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b110)
b131 = sch.decompose_reduction(block=b110, loop=l117)
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #2: GFLOPs: 4.2922. Time: 0.1180 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #3: GFLOPs: 2.1105. Time: 0.2400 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #4: GFLOPs: 1.6092. Time: 0.3147 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #5: GFLOPs: 2.1404. Time: 0.2366 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 34, 29, 29):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0 = T.axis.spatial(1, 0)
                        i1 = T.axis.spatial(34, ax1)
                        i2 = T.axis.spatial(30, ax2)
                        i3 = T.axis.spatial(30, ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 1, 14, 2):
                for i1_2_init, i2_2_init, i1_3_init, i2_3_init in T.grid(17, 2, 2, 7):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(34, i1_2_init * 2 + i1_3_init)
                        oh = T.axis.spatial(14, i2_2_init * 7 + i2_3_init)
                        ow = T.axis.spatial(14, i3_1)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_1)
                        T.reads()
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 3, 1, 17, 2, 1, 1, 1, 1, 1, 2, 7, 1, 1):
                    with T.block("DepthwiseConv2d_update"):
                        b = T.axis.spatial(1, 0)
                        oco = T.axis.spatial(34, i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(14, i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(14, i3_1)
                        oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused * 2 + i4_1)
                        kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                        T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                        T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                        DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(476, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(34, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 17, 2])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 2, 7])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 14, 1, 1])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 2, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
sch.enter_postproc()
b60 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.unroll_explicit")
b61, b62, b63 = sch.get_child_blocks(b60)
l64, l65, l66, l67, l68, l69, l70, l71, l72, l73 = sch.get_loops(block=b61)
l74 = sch.fuse(l64, l65, l66, l67, l68)
sch.parallel(loop=l74)
l75 = sch.fuse(l73)
sch.vectorize(loop=l75)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l76, l77, l78, l79, l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b62)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100 = sch.get_loops(block=b63)
l101 = sch.fuse(l96, l97, l98)
sch.parallel(loop=l101)
l102 = sch.fuse(l99, l100)
sch.vectorize(loop=l102)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b103 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123 = sch.get_loops(block=b103)
b124 = sch.decompose_reduction(block=b103, loop=l110)
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #7: GFLOPs: 1.8142. Time: 0.2792 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #8: GFLOPs: 2.6767. Time: 0.1892 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #9: GFLOPs: 3.0047. Time: 0.1686 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #10: GFLOPs: 2.4373. Time: 0.2078 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_fused in T.parallel(4, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1 in T.grid(1, 1, 1, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 29, 5, 1):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, ax2)
                        i3 = T.axis.spatial(30, i3_1 * 4 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused % 4 + ax4)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
                for i4_1, i5_0 in T.grid(1, 1):
                    for i2_2_init, i1_3_init, i2_3_init, i3_3_init in T.grid(7, 34, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_3_init)
                            oh = T.axis.spatial(14, i2_2_init * 2 + i2_3_init)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_fused)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                    for i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(3, 1, 1, 7, 1, 1, 3, 1, 1, 34, 2, 2, 1):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_3)
                            oh = T.axis.spatial(14, i2_2 * 2 + i2_3)
                            ow = T.axis.spatial(14, i3_1 * 2 + i3_3)
                            oci, kh, kw = T.axis.remap("SRR", [i0_0_i1_0_i2_0_i3_0_i4_0_fused, i5_1, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
            for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 34, 14, 14, 1):
                with T.block("T_add"):
                    ax0_1 = T.axis.spatial(1, 0)
                    ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSS", [ax1, ax2, ax3, i0_0_i1_0_i2_0_i3_0_i4_0_fused])
                    T.reads(DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1])
                    T.writes(T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                    T_add[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = DepthwiseConv2d[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + placeholder_2[ax0_1, ax1_1, 0, 0, ax4_1]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 34])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 2])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 2])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[4, 1, 1, 1])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[1, 3])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
b58, = sch.get_consumers(block=b1)
sch.reverse_compute_at(block=b58, loop=l46, preserve_unit_loops=True)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v59 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v59)
l60 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l60, preserve_unit_loops=True)
sch.enter_postproc()
b61 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b61, ann_key="meta_schedule.unroll_explicit")
b62, b63, b64 = sch.get_child_blocks(b61)
l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77, l78 = sch.get_loops(block=b62)
l79 = sch.fuse(l65, l66, l67, l68, l69)
sch.parallel(loop=l79)
sch.annotate(block_or_loop=l79, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l79, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b63)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b64)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b106)
b127 = sch.decompose_reduction(block=b106, loop=l114)
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #12: GFLOPs: 3.7921. Time: 0.1336 ms. Best GFLOPs: 4.2922
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #13: GFLOPs: 12.6435. Time: 0.0401 ms. Best GFLOPs: 12.6435
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #14: GFLOPs: 16.7308. Time: 0.0303 ms. Best GFLOPs: 16.7308
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #15: GFLOPs: 19.4225. Time: 0.0261 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #16: GFLOPs: 0.3634. Time: 1.3936 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #17: GFLOPs: 0.3182. Time: 1.5914 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #18: GFLOPs: 19.0944. Time: 0.0265 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #19: GFLOPs: 18.9786. Time: 0.0267 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #20: GFLOPs: 12.1038. Time: 0.0418 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #21: GFLOPs: 12.3561. Time: 0.0410 ms. Best GFLOPs: 19.4225
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #22: GFLOPs: 21.8011. Time: 0.0232 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #23: GFLOPs: 14.1270. Time: 0.0359 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #24: GFLOPs: 2.4599. Time: 0.2059 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #25: GFLOPs: 12.2640. Time: 0.0413 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #26: GFLOPs: 11.3619. Time: 0.0446 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #27: GFLOPs: 5.8232. Time: 0.0870 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #28: GFLOPs: 6.3483. Time: 0.0798 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #29: GFLOPs: 19.2407. Time: 0.0263 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #30: GFLOPs: 12.3930. Time: 0.0409 ms. Best GFLOPs: 21.8011
[17:19:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 34, 28, 28, 4), "float32"], placeholder_1: T.Buffer[(34, 1, 3, 3, 1, 4), "float32"], placeholder_2: T.Buffer[(1, 34, 1, 1, 4), "float32"], T_add: T.Buffer[(1, 34, 14, 14, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 34, 30, 30, 4], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 34, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused in T.parallel(28, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 34, 5, 15):
                for ax4_fused in T.vectorized(2):
                    with T.block("PaddedInput"):
                        i0, i1 = T.axis.remap("SS", [ax0, ax1])
                        i2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 28 // 4 * 4 + ax2)
                        i3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                        i4 = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax4_fused)
                        T.reads(placeholder[i0, i1, i2 - 1, i3 - 1, i4])
                        T.writes(PaddedInput[i0, i1, i2, i3, i4])
                        PaddedInput[i0, i1, i2, i3, i4] = T.if_then_else(1 <= i2 and i2 < 29 and 1 <= i3 and i3 < 29, placeholder[i0, i1, i2 - 1, i3 - 1, i4], T.float32(0), dtype="float32")
            for i4_1 in T.serial(1):
                for i2_2_init, i1_3_init, i3_3_init in T.grid(2, 34, 7):
                    for i4_3_fused_init in T.vectorized(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_3_init)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i2_2_init)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3_init)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused_init)
                            T.reads()
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = T.float32(0)
                for i5_0, i6_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(3, 3, 1, 1, 2, 1, 1, 1, 1, 1, 34, 1, 7):
                    for i4_3_fused in T.vectorized(2):
                        with T.block("DepthwiseConv2d_update"):
                            b = T.axis.spatial(1, 0)
                            oco = T.axis.spatial(34, i1_3)
                            oh = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i2_2)
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                            oci = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_i4_0_i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i4_3_fused)
                            kh, kw = T.axis.remap("RR", [i5_0, i6_0])
                            T.reads(DepthwiseConv2d[b, oco, oh, ow, oci], PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4], placeholder_1[oco, 0, kh, kw, 0, oci])
                            T.writes(DepthwiseConv2d[b, oco, oh, ow, oci])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS", "workload":["depthwise_conv2d_NCHWc.x86", ["TENSOR", [1, 34, 28, 28, 4], "float32"], ["TENSOR", [34, 1, 3, 3, 1, 4], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "NCHW4c", "NCHW4c", "float32"]})
                            DepthwiseConv2d[b, oco, oh, ow, oci] = DepthwiseConv2d[b, oco, oh, ow, oci] + PaddedInput[b, oci // 4 + oco, oh * 2 + kh, ow * 2 + kw, oci % 4] * placeholder_1[oco, 0, kh, kw, 0, oci]
        for i0_i1_i2_fused in T.parallel(476, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3_i4_fused in T.vectorized(56):
                with T.block("T_add"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(34, i0_i1_i2_fused // 14)
                    ax2 = T.axis.spatial(14, i0_i1_i2_fused % 14)
                    ax3 = T.axis.spatial(14, i3_i4_fused // 4)
                    ax4 = T.axis.spatial(4, i3_i4_fused % 4)
                    T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4], placeholder_2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                    T_add[ax0, ax1, ax2, ax3, ax4] = DepthwiseConv2d[ax0, ax1, ax2, ax3, ax4] + placeholder_2[ax0, ax1, 0, 0, ax4]
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l14, l15, l16, l17 = sch.split(loop=l3, factors=[v10, v11, v12, v13])
v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 34])
l22, l23, l24, l25 = sch.split(loop=l4, factors=[v18, v19, v20, v21])
v26, v27, v28, v29 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 2, 1])
l30, l31, l32, l33 = sch.split(loop=l5, factors=[v26, v27, v28, v29])
v34, v35, v36, v37 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 2, 1, 7])
l38, l39, l40, l41 = sch.split(loop=l6, factors=[v34, v35, v36, v37])
v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 1, 2])
l46, l47, l48, l49 = sch.split(loop=l7, factors=[v42, v43, v44, v45])
v50, v51 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[3, 1])
l52, l53 = sch.split(loop=l8, factors=[v50, v51])
v54, v55 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[3, 1])
l56, l57 = sch.split(loop=l9, factors=[v54, v55])
sch.reorder(l14, l22, l30, l38, l46, l15, l23, l31, l39, l47, l52, l56, l16, l24, l32, l40, l48, l53, l57, l17, l25, l33, l41, l49)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v58 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v58)
l59 = sch.sample_compute_location(block=b0, decision=8)
sch.compute_at(block=b0, loop=l59, preserve_unit_loops=True)
sch.enter_postproc()
b60 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b60, ann_key="meta_schedule.unroll_explicit")
b61, b62, b63 = sch.get_child_blocks(b60)
l64, l65, l66, l67, l68, l69, l70, l71, l72, l73, l74, l75, l76, l77 = sch.get_loops(block=b61)
l78 = sch.fuse(l64, l65, l66, l67, l68, l69, l70, l71, l72)
sch.parallel(loop=l78)
l79 = sch.fuse(l77)
sch.vectorize(loop=l79)
sch.annotate(block_or_loop=l78, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l78, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b62)
l96 = sch.fuse(l95)
sch.vectorize(loop=l96)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b63)
l102 = sch.fuse(l97, l98, l99)
sch.parallel(loop=l102)
l103 = sch.fuse(l100, l101)
sch.vectorize(loop=l103)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b104)
b121 = sch.decompose_reduction(block=b104, loop=l107)
[17:19:21] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 1526.24

[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #0: GFLOPs: 0.0000. Time: 0.0173 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #1: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #2: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #3: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #4: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #5: GFLOPs: 0.0000. Time: 0.0080 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #6: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #7: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #8: GFLOPs: 0.0000. Time: 0.0100 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #9: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #10: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #11: GFLOPs: 0.0000. Time: 0.0080 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #12: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #13: GFLOPs: 0.0000. Time: 0.0082 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #14: GFLOPs: 0.0000. Time: 0.0080 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #15: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #16: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #17: GFLOPs: 0.0000. Time: 0.0108 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #18: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #19: GFLOPs: 0.0000. Time: 0.0081 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #20: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #21: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #22: GFLOPs: 0.0000. Time: 0.0082 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #23: GFLOPs: 0.0000. Time: 0.0086 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #24: GFLOPs: 0.0000. Time: 0.0082 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #25: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #26: GFLOPs: 0.0000. Time: 0.0078 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #27: GFLOPs: 0.0000. Time: 0.0099 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #28: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #29: GFLOPs: 0.0000. Time: 0.0089 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #30: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_layout_transform_4"] Trial #31: GFLOPs: 0.0000. Time: 0.0079 ms. Best GFLOPs: 0.0000
[17:19:23] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_layout_transform_4"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 1533.69

[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #0: GFLOPs: 0.4815. Time: 3.8753 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #1: GFLOPs: 0.4010. Time: 4.6535 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #2: GFLOPs: 0.2166. Time: 8.6151 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #3: GFLOPs: 0.2247. Time: 8.3048 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #4: GFLOPs: 0.2808. Time: 6.6457 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #5: GFLOPs: 0.2328. Time: 8.0160 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #6: GFLOPs: 0.3643. Time: 5.1222 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #7: GFLOPs: 0.1037. Time: 17.9991 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #8: GFLOPs: 0.3704. Time: 5.0381 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #9: GFLOPs: 0.2333. Time: 7.9996 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #10: GFLOPs: 0.2105. Time: 8.8628 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #11: GFLOPs: 0.4364. Time: 4.2760 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #12: GFLOPs: 0.2464. Time: 7.5742 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #13: GFLOPs: 0.2799. Time: 6.6660 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #14: GFLOPs: 0.4175. Time: 4.4690 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #15: GFLOPs: 0.2871. Time: 6.4996 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #16: GFLOPs: 0.1749. Time: 10.6660 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #17: GFLOPs: 0.3257. Time: 5.7289 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #18: GFLOPs: 0.3344. Time: 5.5801 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #19: GFLOPs: 0.4154. Time: 4.4916 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #20: GFLOPs: 0.3347. Time: 5.5748 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #21: GFLOPs: 0.2536. Time: 7.3571 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #22: GFLOPs: 0.2859. Time: 6.5260 ms. Best GFLOPs: 0.4815
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #23: GFLOPs: 0.5903. Time: 3.1610 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #24: GFLOPs: 0.3361. Time: 5.5515 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #25: GFLOPs: 0.3500. Time: 5.3307 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #26: GFLOPs: 0.2200. Time: 8.4812 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #27: GFLOPs: 0.4487. Time: 4.1590 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #28: GFLOPs: 0.3665. Time: 5.0907 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #29: GFLOPs: 0.2086. Time: 8.9453 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #30: GFLOPs: 0.2597. Time: 7.1842 ms. Best GFLOPs: 0.5903
[17:19:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_multiply_add_1"] Trial #31: GFLOPs: 0.2302. Time: 8.1062 ms. Best GFLOPs: 0.5903
[17:19:26] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_conv2d_multiply_add_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 4694.67

[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #0: GFLOPs: 0.2696. Time: 0.1977 ms. Best GFLOPs: 0.2696
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #1: GFLOPs: 0.4907. Time: 0.1086 ms. Best GFLOPs: 0.4907
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #2: GFLOPs: 0.5934. Time: 0.0898 ms. Best GFLOPs: 0.5934
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #3: GFLOPs: 0.3270. Time: 0.1630 ms. Best GFLOPs: 0.5934
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #4: GFLOPs: 0.3053. Time: 0.1746 ms. Best GFLOPs: 0.5934
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #5: GFLOPs: 0.6595. Time: 0.0808 ms. Best GFLOPs: 0.6595
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #6: GFLOPs: 6.4811. Time: 0.0082 ms. Best GFLOPs: 6.4811
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #7: GFLOPs: 7.0318. Time: 0.0076 ms. Best GFLOPs: 7.0318
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #8: GFLOPs: 7.1043. Time: 0.0075 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #9: GFLOPs: 1.9312. Time: 0.0276 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #10: GFLOPs: 0.8272. Time: 0.0644 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #11: GFLOPs: 1.9723. Time: 0.0270 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #12: GFLOPs: 2.2544. Time: 0.0236 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #13: GFLOPs: 7.0669. Time: 0.0075 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #14: GFLOPs: 2.8444. Time: 0.0187 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #15: GFLOPs: 6.2065. Time: 0.0086 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #16: GFLOPs: 6.6774. Time: 0.0080 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #17: GFLOPs: 0.0978. Time: 0.5450 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #18: GFLOPs: 0.3508. Time: 0.1520 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #19: GFLOPs: 0.5829. Time: 0.0915 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #20: GFLOPs: 0.0706. Time: 0.7549 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #21: GFLOPs: 0.0704. Time: 0.7577 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #22: GFLOPs: 0.0124. Time: 4.2855 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #23: GFLOPs: 0.0721. Time: 0.7391 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #24: GFLOPs: 0.0902. Time: 0.5914 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #25: GFLOPs: 0.1190. Time: 0.4479 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #26: GFLOPs: 0.4583. Time: 0.1163 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #27: GFLOPs: 5.9158. Time: 0.0090 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #28: GFLOPs: 6.5210. Time: 0.0082 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #29: GFLOPs: 4.5979. Time: 0.0116 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #30: GFLOPs: 6.1471. Time: 0.0087 ms. Best GFLOPs: 7.1043
[17:19:26] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_concatenate_nn_relu"] Trial #31: GFLOPs: 6.4769. Time: 0.0082 ms. Best GFLOPs: 7.1043
[17:19:28] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_concatenate_nn_relu"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |            N/A |          N/A |                   N/A |      0 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 4702.18

[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #0: GFLOPs: 1.7048. Time: 0.5942 ms. Best GFLOPs: 1.7048
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #1: GFLOPs: 1.9079. Time: 0.5309 ms. Best GFLOPs: 1.9079
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #2: GFLOPs: 0.7562. Time: 1.3394 ms. Best GFLOPs: 1.9079
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #3: GFLOPs: 2.3690. Time: 0.4276 ms. Best GFLOPs: 2.3690
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #4: GFLOPs: 1.6566. Time: 0.6114 ms. Best GFLOPs: 2.3690
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #5: GFLOPs: 17.4201. Time: 0.0581 ms. Best GFLOPs: 17.4201
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #6: GFLOPs: 20.6435. Time: 0.0491 ms. Best GFLOPs: 20.6435
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #7: GFLOPs: 18.2811. Time: 0.0554 ms. Best GFLOPs: 20.6435
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #8: GFLOPs: 37.0426. Time: 0.0273 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #9: GFLOPs: 36.8593. Time: 0.0275 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #10: GFLOPs: 17.0516. Time: 0.0594 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #11: GFLOPs: 31.0295. Time: 0.0326 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #12: GFLOPs: 26.5302. Time: 0.0382 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #13: GFLOPs: 30.9204. Time: 0.0328 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #14: GFLOPs: 16.5844. Time: 0.0611 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #15: GFLOPs: 26.0130. Time: 0.0389 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #16: GFLOPs: 26.4002. Time: 0.0384 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #17: GFLOPs: 7.4511. Time: 0.1359 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #18: GFLOPs: 12.8291. Time: 0.0790 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #19: GFLOPs: 10.9743. Time: 0.0923 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #20: GFLOPs: 7.0891. Time: 0.1429 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #21: GFLOPs: 2.1318. Time: 0.4752 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #22: GFLOPs: 22.2062. Time: 0.0456 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #23: GFLOPs: 17.8672. Time: 0.0567 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #24: GFLOPs: 23.7395. Time: 0.0427 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #25: GFLOPs: 12.4438. Time: 0.0814 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #26: GFLOPs: 29.4863. Time: 0.0344 ms. Best GFLOPs: 37.0426
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #27: GFLOPs: 55.4040. Time: 0.0183 ms. Best GFLOPs: 55.4040
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #28: GFLOPs: 29.2873. Time: 0.0346 ms. Best GFLOPs: 55.4040
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #29: GFLOPs: 6.7516. Time: 0.1500 ms. Best GFLOPs: 55.4040
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #30: GFLOPs: 18.1719. Time: 0.0557 ms. Best GFLOPs: 55.4040
[17:19:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"] Trial #31: GFLOPs: 6.6458. Time: 0.1524 ms. Best GFLOPs: 55.4040
[17:19:31] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |            N/A |          N/A |                   N/A |      0 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 4830.16

[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #0: GFLOPs: 0.0000. Time: 0.0200 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #1: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #2: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #3: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #4: GFLOPs: 0.0000. Time: 0.0090 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #5: GFLOPs: 0.0000. Time: 0.0096 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #6: GFLOPs: 0.0000. Time: 0.0092 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #7: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #8: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #9: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #10: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #11: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #12: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #13: GFLOPs: 0.0000. Time: 0.0090 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #14: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #15: GFLOPs: 0.0000. Time: 0.0092 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #16: GFLOPs: 0.0000. Time: 0.0095 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #17: GFLOPs: 0.0000. Time: 0.0088 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #18: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #19: GFLOPs: 0.0000. Time: 0.0088 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #20: GFLOPs: 0.0000. Time: 0.0093 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #21: GFLOPs: 0.0000. Time: 0.0088 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #22: GFLOPs: 0.0000. Time: 0.0089 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #23: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #24: GFLOPs: 0.0000. Time: 0.0089 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #25: GFLOPs: 0.0000. Time: 0.0090 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #26: GFLOPs: 0.0000. Time: 0.0145 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #27: GFLOPs: 0.0000. Time: 0.0312 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #28: GFLOPs: 0.0000. Time: 0.0090 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #29: GFLOPs: 0.0000. Time: 0.0089 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #30: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[17:19:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_layout_transform_5"] Trial #31: GFLOPs: 0.0000. Time: 0.0091 ms. Best GFLOPs: 0.0000
[17:19:33] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_layout_transform_5"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |            N/A |          N/A |                   N/A |      0 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 4891.05

[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #0: GFLOPs: 1.3075. Time: 5.7082 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #1: GFLOPs: 0.4666. Time: 15.9959 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #2: GFLOPs: 1.0630. Time: 7.0216 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #3: GFLOPs: 0.6021. Time: 12.3953 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #4: GFLOPs: 0.4605. Time: 16.2072 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #5: GFLOPs: 0.8204. Time: 9.0978 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #6: GFLOPs: 1.0888. Time: 6.8552 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #7: GFLOPs: 0.7379. Time: 10.1142 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #8: GFLOPs: 0.8472. Time: 8.8101 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #9: GFLOPs: 0.5017. Time: 14.8766 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #10: GFLOPs: 0.9114. Time: 8.1893 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #11: GFLOPs: 0.6489. Time: 11.5028 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #12: GFLOPs: 0.8338. Time: 8.9514 ms. Best GFLOPs: 1.3075
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #13: GFLOPs: 1.3888. Time: 5.3740 ms. Best GFLOPs: 1.3888
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #14: GFLOPs: 2.3607. Time: 3.1617 ms. Best GFLOPs: 2.3607
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #15: GFLOPs: 0.5238. Time: 14.2489 ms. Best GFLOPs: 2.3607
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #16: GFLOPs: 2.3873. Time: 3.1264 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_4: T.Buffer[(1, 272, 14, 14), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        for i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused in T.parallel(2, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(4, 1, 1, 1, 7):
                for ax0, ax1, ax2, ax3, ax4, ax5, ax6 in T.grid(1, 17, 17, 1, 1, 4, 4):
                    with T.block("kernel_vec"):
                        g, out_channel, in_channel = T.axis.remap("SSS", [i0_1, ax1, ax2])
                        h = T.axis.spatial(1, 0)
                        w = T.axis.spatial(1, 0)
                        ci, co = T.axis.remap("SS", [ax5, ax6])
                        T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                        T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                        kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
                for ax0, ax1, ax2, ax3, ax4, ax5 in T.grid(1, 1, 17, 14, 4, 1):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_1)
                        n = T.axis.spatial(1, 0)
                        C, h, c = T.axis.remap("SSS", [ax2, ax3, ax4])
                        w = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 7 + i4_1)
                        T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
                for i5_1 in T.serial(2):
                    for i5_2_init, i2_3_init, i3_3_init in T.grid(2, 17, 14):
                        with T.block("conv_init"):
                            g = T.axis.spatial(4, i0_1)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh = T.axis.remap("SS", [i2_3_init, i3_3_init])
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 7 + i4_1)
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2_init)
                            T.reads()
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3, i4_3, i5_3 in T.grid(68, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 17, 14, 1, 1):
                        with T.block("conv_update"):
                            g = T.axis.spatial(4, i0_1)
                            n = T.axis.spatial(1, 0)
                            oc_chunk, oh = T.axis.remap("SS", [i2_3, i3_3])
                            ow = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 7 + i4_1)
                            oc_block = T.axis.spatial(4, i5_1 * 2 + i5_2)
                            ic = T.axis.reduce(68, i6_0)
                            kh = T.axis.reduce(1, 0)
                            kw = T.axis.reduce(1, 0)
                            T.reads(conv_global[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                            T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                            T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                            conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for ax0, ax1, ax2, ax3 in T.grid(4, 1, 17, 14):
                for ax4_ax5_fused in T.vectorized(28):
                    with T.block("conv_global"):
                        v0 = T.axis.spatial(4, ax0)
                        v1 = T.axis.spatial(1, 0)
                        v2, v3 = T.axis.remap("SS", [ax2, ax3])
                        v4 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_i4_0_i5_0_fused * 7 + ax4_ax5_fused // 4)
                        v5 = T.axis.spatial(4, ax4_ax5_fused % 4)
                        T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                        T.writes(conv[v0, v1, v2, v3, v4, v5])
                        conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
        for i0_i1_fused in T.parallel(272, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                with T.block("output_unpack"):
                    n = T.axis.spatial(1, 0)
                    c, h, w = T.axis.remap("SSS", [i0_i1_fused, ax2, ax3])
                    T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                    T.writes(output_unpack[n, c, h, w])
                    T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                    output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
            for i2 in T.serial(14):
                for i3_fused in T.vectorized(14):
                    with T.block("T_relu"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3_fused])
                        T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0], placeholder_4[ax0, ax1, ax2, ax3])
                        T.writes(T_relu[ax0, ax1, ax2, ax3])
                        T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0] + placeholder_4[ax0, ax1, ax2, ax3], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l8, l9, l10, l11, l12, l13, l14, l15, l16 = sch.get_loops(block=b2)
v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 4, 1, 1])
l21, l22, l23, l24 = sch.split(loop=l8, factors=[v17, v18, v19, v20])
v25, v26, v27, v28 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l29, l30, l31, l32 = sch.split(loop=l9, factors=[v25, v26, v27, v28])
v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l37, l38, l39, l40 = sch.split(loop=l10, factors=[v33, v34, v35, v36])
v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l45, l46, l47, l48 = sch.split(loop=l11, factors=[v41, v42, v43, v44])
v49, v50, v51, v52 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[2, 7, 1, 1])
l53, l54, l55, l56 = sch.split(loop=l12, factors=[v49, v50, v51, v52])
v57, v58, v59, v60 = sch.sample_perfect_tile(loop=l13, n=4, max_innermost_factor=64, decision=[1, 2, 2, 1])
l61, l62, l63, l64 = sch.split(loop=l13, factors=[v57, v58, v59, v60])
v65, v66 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[68, 1])
l67, l68 = sch.split(loop=l14, factors=[v65, v66])
v69, v70 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l71, l72 = sch.split(loop=l15, factors=[v69, v70])
v73, v74 = sch.sample_perfect_tile(loop=l16, n=2, max_innermost_factor=64, decision=[1, 1])
l75, l76 = sch.split(loop=l16, factors=[v73, v74])
sch.reorder(l21, l29, l37, l45, l53, l61, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63, l68, l72, l76, l24, l32, l40, l48, l56, l64)
b77 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b77, loop=l61, preserve_unit_loops=True)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.vectorize", ann_val=64)
v78 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b7, ann_key="meta_schedule.unroll_explicit", ann_val=v78)
l79 = sch.sample_compute_location(block=b3, decision=1)
sch.compute_at(block=b3, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b1, decision=10)
sch.compute_at(block=b1, loop=l80, preserve_unit_loops=True)
l81 = sch.sample_compute_location(block=b0, decision=10)
sch.compute_at(block=b0, loop=l81, preserve_unit_loops=True)
sch.enter_postproc()
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86, b87, b88 = sch.get_child_blocks(b82)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98, l99, l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b83)
l107 = sch.fuse(l89, l90, l91, l92, l93, l94)
sch.parallel(loop=l107)
sch.annotate(block_or_loop=l107, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l107, ann_key="pragma_unroll_explicit", ann_val=1)
l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l108, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l108, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l145, l146, l147, l148, l149, l150, l151 = sch.get_loops(block=b86)
l152 = sch.fuse(l150, l151)
sch.vectorize(loop=l152)
sch.annotate(block_or_loop=l145, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l145, ann_key="pragma_unroll_explicit", ann_val=1)
l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b87)
l159 = sch.fuse(l153, l154)
sch.parallel(loop=l159)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162 = sch.get_loops(block=b88)
l163 = sch.fuse(l162)
sch.vectorize(loop=l163)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b164 = sch.get_block(name="conv", func_name="main")
l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b164)
b190 = sch.decompose_reduction(block=b164, loop=l172)
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #18: GFLOPs: 1.1526. Time: 6.4755 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #19: GFLOPs: 1.0655. Time: 7.0049 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #20: GFLOPs: 2.1379. Time: 3.4911 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #21: GFLOPs: 1.1749. Time: 6.3524 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #22: GFLOPs: 0.6304. Time: 11.8391 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #23: GFLOPs: 0.5882. Time: 12.6899 ms. Best GFLOPs: 2.3873
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #24: GFLOPs: 2.8103. Time: 2.6558 ms. Best GFLOPs: 2.8103
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #25: GFLOPs: 3.9838. Time: 1.8735 ms. Best GFLOPs: 3.9838
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #26: GFLOPs: 1.1405. Time: 6.5443 ms. Best GFLOPs: 3.9838
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #27: GFLOPs: 11.3652. Time: 0.6567 ms. Best GFLOPs: 11.3652
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #28: GFLOPs: 9.1650. Time: 0.8144 ms. Best GFLOPs: 11.3652
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #29: GFLOPs: 2.5228. Time: 2.9585 ms. Best GFLOPs: 11.3652
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #30: GFLOPs: 5.7232. Time: 1.3041 ms. Best GFLOPs: 11.3652
[17:19:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"] Trial #31: GFLOPs: 4.1691. Time: 1.7902 ms. Best GFLOPs: 11.3652
[17:19:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |            N/A |          N/A |                   N/A |      0 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 9488.04

[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #0: GFLOPs: 0.6715. Time: 11.0348 ms. Best GFLOPs: 0.6715
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #1: GFLOPs: 1.3236. Time: 5.5985 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #2: GFLOPs: 0.6955. Time: 10.6550 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #3: GFLOPs: 1.2576. Time: 5.8923 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #4: GFLOPs: 0.9088. Time: 8.1543 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #5: GFLOPs: 0.8748. Time: 8.4705 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #6: GFLOPs: 1.0588. Time: 6.9986 ms. Best GFLOPs: 1.3236
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #7: GFLOPs: 1.9074. Time: 3.8850 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #8: GFLOPs: 0.6882. Time: 10.7674 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #9: GFLOPs: 1.0987. Time: 6.7448 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 14, 14), "float32"], placeholder_1: T.Buffer[(272, 68, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 1, 1), "float32"], T_relu: T.Buffer[(1, 272, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        data_vec = T.alloc_buffer([4, 1, 17, 14, 4, 14], dtype="float32")
        kernel_vec = T.alloc_buffer([4, 17, 17, 1, 1, 4, 4], dtype="float32")
        conv = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        output_unpack = T.alloc_buffer([1, 272, 14, 14], dtype="float32")
        conv_global = T.alloc_buffer([4, 1, 17, 14, 14, 4], dtype="float32")
        for i0_i1_i2_i3_fused in T.parallel(952, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i4 in T.serial(4):
                for i5_fused in T.vectorized(14):
                    with T.block("data_vec"):
                        g = T.axis.spatial(4, i0_i1_i2_i3_fused // 238)
                        n = T.axis.spatial(1, 0)
                        C = T.axis.spatial(17, i0_i1_i2_i3_fused % 238 // 14)
                        h = T.axis.spatial(14, i0_i1_i2_i3_fused % 14)
                        c, w = T.axis.remap("SS", [i4, i5_fused])
                        T.reads(placeholder[n, g * 68 + C * 4 + c, h, w])
                        T.writes(data_vec[g, n, C, h, c, w])
                        data_vec[g, n, C, h, c, w] = placeholder[n, g * 68 + C * 4 + c, h, w]
        for i0_i1_i2_fused in T.parallel(1156, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i3, i4, i5, i6 in T.grid(1, 1, 4, 4):
                with T.block("kernel_vec"):
                    g = T.axis.spatial(4, i0_i1_i2_fused // 289)
                    out_channel = T.axis.spatial(17, i0_i1_i2_fused % 289 // 17)
                    in_channel = T.axis.spatial(17, i0_i1_i2_fused % 17)
                    h = T.axis.spatial(1, 0)
                    w = T.axis.spatial(1, 0)
                    ci, co = T.axis.remap("SS", [i5, i6])
                    T.reads(placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w])
                    T.writes(kernel_vec[g, out_channel, in_channel, h, w, ci, co])
                    kernel_vec[g, out_channel, in_channel, h, w, ci, co] = placeholder_1[g * 68 + out_channel * 4 + co, in_channel * 4 + ci, h, w]
        for i0_0 in T.serial(1, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i1_0, i2_0, i3_0, i4_0, i5_0 in T.grid(1, 1, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_1 in T.grid(2, 1, 1, 1, 1, 1):
                    for i0_3_init, i2_3_init, i3_3_init in T.grid(2, 17, 14):
                        for i4_3_i5_3_fused_init in T.vectorized(56):
                            with T.block("conv_init"):
                                g = T.axis.spatial(4, i0_1 * 2 + i0_3_init)
                                n = T.axis.spatial(1, 0)
                                oc_chunk, oh = T.axis.remap("SS", [i2_3_init, i3_3_init])
                                ow = T.axis.spatial(14, i4_3_i5_3_fused_init // 4)
                                oc_block = T.axis.spatial(4, i4_3_i5_3_fused_init % 4)
                                T.reads()
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    for i6_0, i7_0, i8_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_2, i6_1, i7_1, i8_1, i0_3, i1_3, i2_3, i3_3 in T.grid(34, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 17, 14):
                        for i4_3_i5_3_fused in T.vectorized(56):
                            with T.block("conv_update"):
                                g = T.axis.spatial(4, i0_1 * 2 + i0_3)
                                n = T.axis.spatial(1, 0)
                                oc_chunk, oh = T.axis.remap("SS", [i2_3, i3_3])
                                ow = T.axis.spatial(14, i4_3_i5_3_fused // 4)
                                oc_block = T.axis.spatial(4, i4_3_i5_3_fused % 4)
                                ic = T.axis.reduce(68, i6_0 * 2 + i6_1)
                                kh = T.axis.reduce(1, 0)
                                kw = T.axis.reduce(1, 0)
                                T.reads(conv_global[g, n, oc_chunk, oh, ow, oc_block], data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw], kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                                T.writes(conv_global[g, n, oc_chunk, oh, ow, oc_block])
                                T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                                conv_global[g, n, oc_chunk, oh, ow, oc_block] = conv_global[g, n, oc_chunk, oh, ow, oc_block] + data_vec[g, n, ic // 4, oh + kh, ic % 4, ow + kw] * kernel_vec[g, oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3 in T.grid(4, 1, 17, 14):
                    for ax4_ax5_fused in T.vectorized(56):
                        with T.block("conv_global"):
                            v0 = T.axis.spatial(4, ax0)
                            v1 = T.axis.spatial(1, 0)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            v4 = T.axis.spatial(14, ax4_ax5_fused // 4)
                            v5 = T.axis.spatial(4, ax4_ax5_fused % 4)
                            T.reads(conv_global[v0, v1, v2, v3, v4, v5])
                            T.writes(conv[v0, v1, v2, v3, v4, v5])
                            conv[v0, v1, v2, v3, v4, v5] = conv_global[v0, v1, v2, v3, v4, v5]
        for i0_i1_fused in T.parallel(272, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(14, 14):
                for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                    with T.block("output_unpack"):
                        n = T.axis.spatial(1, 0)
                        c, h, w = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                        T.reads(conv[c // 68, n, c % 68 // 4, h, w, c % 4])
                        T.writes(output_unpack[n, c, h, w])
                        T.block_attr({"workload":["group_conv2d_nchw.x86", ["TENSOR", [1, 272, 14, 14], "float32"], ["TENSOR", [272, 68, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], 4, "float32"]})
                        output_unpack[n, c, h, w] = conv[c // 68, n, c % 68 // 4, h, w, c % 4]
                with T.block("T_relu"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1, ax2, ax3 = T.axis.remap("SSS", [i0_i1_fused, i2, i3])
                    T.reads(output_unpack[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0], placeholder_3[ax0, ax1, 0, 0])
                    T.writes(T_relu[ax0, ax1, ax2, ax3])
                    T_relu[ax0, ax1, ax2, ax3] = T.max(output_unpack[ax0, ax1, ax2, ax3] * placeholder_2[ax0, ax1, 0, 0] + placeholder_3[ax0, ax1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="data_vec", func_name="main")
b1 = sch.get_block(name="kernel_vec", func_name="main")
b2 = sch.get_block(name="conv", func_name="main")
b3 = sch.get_block(name="output_unpack", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l7, l8, l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b2)
v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 2])
l20, l21, l22, l23 = sch.split(loop=l7, factors=[v16, v17, v18, v19])
v24, v25, v26, v27 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l28, l29, l30, l31 = sch.split(loop=l8, factors=[v24, v25, v26, v27])
v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l9, n=4, max_innermost_factor=64, decision=[1, 1, 1, 17])
l36, l37, l38, l39 = sch.split(loop=l9, factors=[v32, v33, v34, v35])
v40, v41, v42, v43 = sch.sample_perfect_tile(loop=l10, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l44, l45, l46, l47 = sch.split(loop=l10, factors=[v40, v41, v42, v43])
v48, v49, v50, v51 = sch.sample_perfect_tile(loop=l11, n=4, max_innermost_factor=64, decision=[1, 1, 1, 14])
l52, l53, l54, l55 = sch.split(loop=l11, factors=[v48, v49, v50, v51])
v56, v57, v58, v59 = sch.sample_perfect_tile(loop=l12, n=4, max_innermost_factor=64, decision=[1, 1, 1, 4])
l60, l61, l62, l63 = sch.split(loop=l12, factors=[v56, v57, v58, v59])
v64, v65 = sch.sample_perfect_tile(loop=l13, n=2, max_innermost_factor=64, decision=[34, 2])
l66, l67 = sch.split(loop=l13, factors=[v64, v65])
v68, v69 = sch.sample_perfect_tile(loop=l14, n=2, max_innermost_factor=64, decision=[1, 1])
l70, l71 = sch.split(loop=l14, factors=[v68, v69])
v72, v73 = sch.sample_perfect_tile(loop=l15, n=2, max_innermost_factor=64, decision=[1, 1])
l74, l75 = sch.split(loop=l15, factors=[v72, v73])
sch.reorder(l20, l28, l36, l44, l52, l60, l21, l29, l37, l45, l53, l61, l66, l70, l74, l22, l30, l38, l46, l54, l62, l67, l71, l75, l23, l31, l39, l47, l55, l63)
b76 = sch.cache_write(block=b2, write_buffer_index=0, storage_scope="global")
sch.reverse_compute_at(block=b76, loop=l60, preserve_unit_loops=True)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.vectorize", ann_val=64)
v77 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v77)
l78 = sch.sample_compute_location(block=b3, decision=3)
sch.compute_at(block=b3, loop=l78, preserve_unit_loops=True)
l79 = sch.sample_compute_location(block=b1, decision=-1)
sch.compute_at(block=b1, loop=l79, preserve_unit_loops=True)
l80 = sch.sample_compute_location(block=b0, decision=-1)
sch.compute_at(block=b0, loop=l80, preserve_unit_loops=True)
sch.enter_postproc()
b81 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b81, ann_key="meta_schedule.unroll_explicit")
b82, b83, b84, b85, b86, b87 = sch.get_child_blocks(b81)
l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b82)
l94 = sch.fuse(l88, l89, l90, l91)
sch.parallel(loop=l94)
l95 = sch.fuse(l93)
sch.vectorize(loop=l95)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l96, l97, l98, l99, l100, l101, l102 = sch.get_loops(block=b83)
l103 = sch.fuse(l96, l97, l98)
sch.parallel(loop=l103)
sch.annotate(block_or_loop=l103, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l103, ann_key="pragma_unroll_explicit", ann_val=1)
l104, l105, l106, l107, l108, l109, l110, l111, l112, l113, l114, l115, l116, l117, l118, l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b84)
l134 = sch.fuse(l132, l133)
sch.vectorize(loop=l134)
sch.annotate(block_or_loop=l104, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l104, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146 = sch.get_loops(block=b85)
l147 = sch.fuse(l145, l146)
sch.vectorize(loop=l147)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154, l155 = sch.get_loops(block=b86)
l156 = sch.fuse(l148, l149)
sch.parallel(loop=l156)
sch.annotate(block_or_loop=l156, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l156, ann_key="pragma_unroll_explicit", ann_val=1)
l157, l158, l159 = sch.get_loops(block=b87)
sch.annotate(block_or_loop=l157, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l157, ann_key="pragma_unroll_explicit", ann_val=1)
b160 = sch.get_block(name="conv", func_name="main")
l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b160)
b190 = sch.decompose_reduction(block=b160, loop=l173)
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #11: GFLOPs: 0.8606. Time: 8.6106 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #12: GFLOPs: 1.1160. Time: 6.6403 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #13: GFLOPs: 0.6863. Time: 10.7977 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #14: GFLOPs: 0.9576. Time: 7.7383 ms. Best GFLOPs: 1.9074
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #15: GFLOPs: 2.4299. Time: 3.0496 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #16: GFLOPs: 0.8026. Time: 9.2329 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #17: GFLOPs: 0.5368. Time: 13.8046 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #18: GFLOPs: 1.3763. Time: 5.3843 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #19: GFLOPs: 0.5520. Time: 13.4258 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #20: GFLOPs: 0.7303. Time: 10.1466 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #21: GFLOPs: 0.6325. Time: 11.7160 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #22: GFLOPs: 1.0344. Time: 7.1642 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #23: GFLOPs: 0.9117. Time: 8.1278 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #24: GFLOPs: 0.8920. Time: 8.3077 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #25: GFLOPs: 1.8528. Time: 3.9995 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #26: GFLOPs: 1.1878. Time: 6.2385 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #27: GFLOPs: 1.9884. Time: 3.7267 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #28: GFLOPs: 1.2362. Time: 5.9944 ms. Best GFLOPs: 2.4299
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #29: GFLOPs: 3.7931. Time: 1.9537 ms. Best GFLOPs: 3.7931
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #30: GFLOPs: 3.6074. Time: 2.0542 ms. Best GFLOPs: 3.7931
[17:19:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"] Trial #31: GFLOPs: 3.7684. Time: 1.9665 ms. Best GFLOPs: 3.7931
[17:19:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |            N/A |          N/A |                   N/A |      0 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 25117.3

[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #0: GFLOPs: 0.0000. Time: 0.0634 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #1: GFLOPs: 0.0000. Time: 0.1503 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #2: GFLOPs: 0.0000. Time: 0.3169 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #3: GFLOPs: 0.0000. Time: 0.0415 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #4: GFLOPs: 0.0000. Time: 1.9739 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #5: GFLOPs: 0.0000. Time: 0.0897 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #6: GFLOPs: 0.0000. Time: 0.0355 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #7: GFLOPs: 0.0000. Time: 0.0542 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #8: GFLOPs: 0.0000. Time: 0.0700 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #9: GFLOPs: 0.0000. Time: 1.4629 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #10: GFLOPs: 0.0000. Time: 0.1206 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #11: GFLOPs: 0.0000. Time: 0.2258 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #12: GFLOPs: 0.0000. Time: 0.0623 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #13: GFLOPs: 0.0000. Time: 1.1089 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #14: GFLOPs: 0.0000. Time: 0.2836 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #15: GFLOPs: 0.0000. Time: 1.2901 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #16: GFLOPs: 0.0000. Time: 4.6383 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #17: GFLOPs: 0.0000. Time: 0.3838 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #18: GFLOPs: 0.0000. Time: 2.2944 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #19: GFLOPs: 0.0000. Time: 0.1352 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #20: GFLOPs: 0.0000. Time: 0.0156 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #21: GFLOPs: 0.0000. Time: 0.0152 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #22: GFLOPs: 0.0000. Time: 0.0301 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #23: GFLOPs: 0.0000. Time: 0.0184 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #24: GFLOPs: 0.0000. Time: 0.1331 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #25: GFLOPs: 0.0000. Time: 0.0189 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #26: GFLOPs: 0.0000. Time: 0.1367 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #27: GFLOPs: 0.0000. Time: 0.0178 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #28: GFLOPs: 0.0000. Time: 0.0177 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #29: GFLOPs: 0.0000. Time: 0.0183 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #30: GFLOPs: 0.0000. Time: 0.1336 ms. Best GFLOPs: 0.0000
[17:19:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_reshape_transpose_reshape_layout_transform_2"] Trial #31: GFLOPs: 0.0000. Time: 0.2490 ms. Best GFLOPs: 0.0000
[17:19:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_reshape_transpose_reshape_layout_transform_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |            N/A |          N/A |                   N/A |      0 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 864
Total latency (us): 25238.6

[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #0: GFLOPs: 0.5721. Time: 0.4426 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #1: GFLOPs: 0.2512. Time: 1.0081 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #2: GFLOPs: 0.0730. Time: 3.4685 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #3: GFLOPs: 0.0376. Time: 6.7414 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #4: GFLOPs: 0.0577. Time: 4.3866 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #5: GFLOPs: 0.0616. Time: 4.1119 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #6: GFLOPs: 0.0609. Time: 4.1570 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #7: GFLOPs: 0.0284. Time: 8.9228 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #8: GFLOPs: 0.1057. Time: 2.3961 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #9: GFLOPs: 0.0430. Time: 5.8877 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #10: GFLOPs: 0.0699. Time: 3.6205 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #11: GFLOPs: 0.1659. Time: 1.5262 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #12: GFLOPs: 0.0935. Time: 2.7093 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #13: GFLOPs: 0.1440. Time: 1.7581 ms. Best GFLOPs: 0.5721
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #14: GFLOPs: 3.0809. Time: 0.0822 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #15: GFLOPs: 1.1248. Time: 0.2251 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #16: GFLOPs: 1.4789. Time: 0.1712 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #17: GFLOPs: 2.7134. Time: 0.0933 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #18: GFLOPs: 1.3702. Time: 0.1848 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #19: GFLOPs: 0.8908. Time: 0.2843 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #20: GFLOPs: 1.0407. Time: 0.2433 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #21: GFLOPs: 0.7259. Time: 0.3488 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #22: GFLOPs: 2.7245. Time: 0.0929 ms. Best GFLOPs: 3.0809
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #23: GFLOPs: 4.5018. Time: 0.0563 ms. Best GFLOPs: 4.5018
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #24: GFLOPs: 5.4226. Time: 0.0467 ms. Best GFLOPs: 5.4226
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #25: GFLOPs: 6.4897. Time: 0.0390 ms. Best GFLOPs: 6.4897
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #26: GFLOPs: 7.6958. Time: 0.0329 ms. Best GFLOPs: 7.6958
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #27: GFLOPs: 9.8295. Time: 0.0258 ms. Best GFLOPs: 9.8295
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #28: GFLOPs: 1.5177. Time: 0.1669 ms. Best GFLOPs: 9.8295
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #29: GFLOPs: 18.0563. Time: 0.0140 ms. Best GFLOPs: 18.0563
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #30: GFLOPs: 6.0481. Time: 0.0419 ms. Best GFLOPs: 18.0563
[17:19:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"] Trial #31: GFLOPs: 1.2226. Time: 0.2071 ms. Best GFLOPs: 18.0563
[17:19:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 896
Total latency (us): 25252.6

[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #0: GFLOPs: 0.0000. Time: 0.0442 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #1: GFLOPs: 0.0000. Time: 0.0168 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #2: GFLOPs: 0.0000. Time: 0.0909 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #3: GFLOPs: 0.0000. Time: 0.0512 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #4: GFLOPs: 0.0000. Time: 0.0388 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #5: GFLOPs: 0.0000. Time: 0.0294 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #6: GFLOPs: 0.0000. Time: 0.0264 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #7: GFLOPs: 0.0000. Time: 0.0455 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #8: GFLOPs: 0.0000. Time: 0.0650 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #9: GFLOPs: 0.0000. Time: 0.0183 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #10: GFLOPs: 0.0000. Time: 0.0228 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #11: GFLOPs: 0.0000. Time: 0.0733 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #12: GFLOPs: 0.0000. Time: 0.4590 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #13: GFLOPs: 0.0000. Time: 4.2262 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #14: GFLOPs: 0.0000. Time: 0.5823 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #15: GFLOPs: 0.0000. Time: 0.2299 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #16: GFLOPs: 0.0000. Time: 0.0547 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #17: GFLOPs: 0.0000. Time: 0.0966 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #18: GFLOPs: 0.0000. Time: 0.0259 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #19: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #20: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #21: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #22: GFLOPs: 0.0000. Time: 0.0070 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #23: GFLOPs: 0.0000. Time: 0.0073 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #24: GFLOPs: 0.0000. Time: 0.0073 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #25: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #26: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #27: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #28: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #29: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #30: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_layout_transform_6"] Trial #31: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:19:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_layout_transform_6"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 928
Total latency (us): 25259.7

[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #0: GFLOPs: 0.6127. Time: 3.0019 ms. Best GFLOPs: 0.6127
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #1: GFLOPs: 0.5077. Time: 3.6226 ms. Best GFLOPs: 0.6127
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #2: GFLOPs: 1.1021. Time: 1.6688 ms. Best GFLOPs: 1.1021
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #3: GFLOPs: 0.5402. Time: 3.4047 ms. Best GFLOPs: 1.1021
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #4: GFLOPs: 0.6642. Time: 2.7689 ms. Best GFLOPs: 1.1021
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #5: GFLOPs: 0.7058. Time: 2.6059 ms. Best GFLOPs: 1.1021
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #6: GFLOPs: 0.5277. Time: 3.4854 ms. Best GFLOPs: 1.1021
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #7: GFLOPs: 1.1142. Time: 1.6508 ms. Best GFLOPs: 1.1142
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #8: GFLOPs: 0.4632. Time: 3.9704 ms. Best GFLOPs: 1.1142
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #9: GFLOPs: 0.9993. Time: 1.8406 ms. Best GFLOPs: 1.1142
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #10: GFLOPs: 0.6025. Time: 3.0525 ms. Best GFLOPs: 1.1142
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #11: GFLOPs: 1.8385. Time: 1.0004 ms. Best GFLOPs: 1.8385
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #12: GFLOPs: 1.4338. Time: 1.2828 ms. Best GFLOPs: 1.8385
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #13: GFLOPs: 0.9617. Time: 1.9124 ms. Best GFLOPs: 1.8385
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #14: GFLOPs: 18.4678. Time: 0.0996 ms. Best GFLOPs: 18.4678
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #15: GFLOPs: 12.7108. Time: 0.1447 ms. Best GFLOPs: 18.4678
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #16: GFLOPs: 24.0586. Time: 0.0764 ms. Best GFLOPs: 24.0586
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #17: GFLOPs: 5.6776. Time: 0.3240 ms. Best GFLOPs: 24.0586
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #18: GFLOPs: 10.3514. Time: 0.1777 ms. Best GFLOPs: 24.0586
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #19: GFLOPs: 16.0984. Time: 0.1143 ms. Best GFLOPs: 24.0586
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #20: GFLOPs: 28.1264. Time: 0.0654 ms. Best GFLOPs: 28.1264
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #21: GFLOPs: 20.7395. Time: 0.0887 ms. Best GFLOPs: 28.1264
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #22: GFLOPs: 28.6769. Time: 0.0641 ms. Best GFLOPs: 28.6769
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #23: GFLOPs: 30.3081. Time: 0.0607 ms. Best GFLOPs: 30.3081
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #24: GFLOPs: 9.9348. Time: 0.1851 ms. Best GFLOPs: 30.3081
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #25: GFLOPs: 8.4332. Time: 0.2181 ms. Best GFLOPs: 30.3081
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #26: GFLOPs: 32.5093. Time: 0.0566 ms. Best GFLOPs: 32.5093
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #27: GFLOPs: 12.3860. Time: 0.1485 ms. Best GFLOPs: 32.5093
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #28: GFLOPs: 22.5462. Time: 0.0816 ms. Best GFLOPs: 32.5093
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #29: GFLOPs: 6.9876. Time: 0.2632 ms. Best GFLOPs: 32.5093
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #30: GFLOPs: 23.6023. Time: 0.0779 ms. Best GFLOPs: 32.5093
[17:19:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_multiply_add_2"] Trial #31: GFLOPs: 17.2336. Time: 0.1067 ms. Best GFLOPs: 32.5093
[17:19:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_nn_conv2d_multiply_add_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 960
Total latency (us): 25316.3

[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #0: GFLOPs: 0.8677. Time: 0.0307 ms. Best GFLOPs: 0.8677
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #1: GFLOPs: 1.1770. Time: 0.0226 ms. Best GFLOPs: 1.1770
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #2: GFLOPs: 0.6019. Time: 0.0443 ms. Best GFLOPs: 1.1770
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #3: GFLOPs: 3.7824. Time: 0.0070 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #4: GFLOPs: 1.1395. Time: 0.0234 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #5: GFLOPs: 1.7304. Time: 0.0154 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #6: GFLOPs: 3.0266. Time: 0.0088 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #7: GFLOPs: 3.6084. Time: 0.0074 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #8: GFLOPs: 3.5869. Time: 0.0074 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #9: GFLOPs: 3.7589. Time: 0.0071 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #10: GFLOPs: 0.0710. Time: 0.3755 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #11: GFLOPs: 0.1768. Time: 0.1508 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #12: GFLOPs: 0.2916. Time: 0.0914 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #13: GFLOPs: 0.2257. Time: 0.1181 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #14: GFLOPs: 3.7422. Time: 0.0071 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #15: GFLOPs: 3.3000. Time: 0.0081 ms. Best GFLOPs: 3.7824
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #16: GFLOPs: 3.8068. Time: 0.0070 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #17: GFLOPs: 0.0122. Time: 2.1830 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #18: GFLOPs: 0.0064. Time: 4.1479 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #19: GFLOPs: 0.0270. Time: 0.9868 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #20: GFLOPs: 0.0277. Time: 0.9610 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #21: GFLOPs: 0.0278. Time: 0.9587 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #22: GFLOPs: 0.0079. Time: 3.3941 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #23: GFLOPs: 0.0459. Time: 0.5808 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #24: GFLOPs: 1.7001. Time: 0.0157 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #25: GFLOPs: 0.6957. Time: 0.0383 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #26: GFLOPs: 3.6105. Time: 0.0074 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #27: GFLOPs: 1.7252. Time: 0.0155 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #28: GFLOPs: 3.5936. Time: 0.0074 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #29: GFLOPs: 1.6334. Time: 0.0163 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #30: GFLOPs: 3.5338. Time: 0.0075 ms. Best GFLOPs: 3.8068
[17:19:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_concatenate_nn_relu_1"] Trial #31: GFLOPs: 3.1908. Time: 0.0084 ms. Best GFLOPs: 3.8068
[17:19:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_concatenate_nn_relu_1"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 992
Total latency (us): 25323.3

[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #0: GFLOPs: 6.2277. Time: 1.1771 ms. Best GFLOPs: 6.2277
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #1: GFLOPs: 10.5128. Time: 0.6973 ms. Best GFLOPs: 10.5128
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #2: GFLOPs: 23.1670. Time: 0.3164 ms. Best GFLOPs: 23.1670
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #3: GFLOPs: 24.6780. Time: 0.2970 ms. Best GFLOPs: 24.6780
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #4: GFLOPs: 11.9879. Time: 0.6115 ms. Best GFLOPs: 24.6780
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #5: GFLOPs: 10.3993. Time: 0.7049 ms. Best GFLOPs: 24.6780
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #6: GFLOPs: 20.8693. Time: 0.3513 ms. Best GFLOPs: 24.6780
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #7: GFLOPs: 22.2713. Time: 0.3291 ms. Best GFLOPs: 24.6780
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #8: GFLOPs: 37.7106. Time: 0.1944 ms. Best GFLOPs: 37.7106
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #9: GFLOPs: 14.3751. Time: 0.5099 ms. Best GFLOPs: 37.7106
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #10: GFLOPs: 48.1168. Time: 0.1523 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #11: GFLOPs: 4.1846. Time: 1.7517 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #12: GFLOPs: 3.6481. Time: 2.0094 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #13: GFLOPs: 1.6619. Time: 4.4109 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #14: GFLOPs: 8.5802. Time: 0.8543 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #15: GFLOPs: 2.3682. Time: 3.0953 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #16: GFLOPs: 3.1898. Time: 2.2980 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #17: GFLOPs: 1.1973. Time: 6.1225 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #18: GFLOPs: 1.3885. Time: 5.2794 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #19: GFLOPs: 0.9995. Time: 7.3343 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #20: GFLOPs: 1.1129. Time: 6.5869 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #21: GFLOPs: 0.8300. Time: 8.8316 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #22: GFLOPs: 0.7226. Time: 10.1444 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #23: GFLOPs: 0.5338. Time: 13.7317 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #24: GFLOPs: 1.2474. Time: 5.8763 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #25: GFLOPs: 0.9280. Time: 7.8992 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #26: GFLOPs: 1.2736. Time: 5.7559 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #27: GFLOPs: 0.5498. Time: 13.3327 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #28: GFLOPs: 0.9164. Time: 7.9994 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #29: GFLOPs: 0.9165. Time: 7.9982 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #30: GFLOPs: 0.9165. Time: 7.9984 ms. Best GFLOPs: 48.1168
[17:19:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"] Trial #31: GFLOPs: 1.0134. Time: 7.2338 ms. Best GFLOPs: 48.1168
[17:19:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1024
Total latency (us): 25780.3

[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #0: GFLOPs: 0.0000. Time: 0.2975 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #1: GFLOPs: 0.0000. Time: 0.1806 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #2: GFLOPs: 0.0000. Time: 0.2103 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #3: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for ax0, ax1 in T.grid(1, 4):
                for ax2_ax3_ax4_fused in T.vectorized(49):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2 = T.axis.remap("SS", [ax1, i0_i1_fused])
                        ax3 = T.axis.spatial(7, ax2_ax3_ax4_fused // 7)
                        ax4 = T.axis.spatial(7, ax2_ax3_ax4_fused % 7)
                        T.reads(placeholder[0, (ax1_1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2, ax3, ax4])
                        T_reshape[ax0_1, ax1_1, ax2, ax3, ax4] = placeholder[0, (ax1_1 * 136 + (ax4 // 7 + ax3) // 7 + ax2) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
            for i2 in T.serial(7):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(7):
                        with T.block("T_transpose"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax1, i2, ax0_ax1_ax2_ax3_ax4_fused])
                            T.reads(T_reshape[ax0_2, ax2, ax1_2, ax3, ax4])
                            T.writes(T_transpose[ax0_2, ax1_2, ax2, ax3, ax4])
                            T_transpose[ax0_2, ax1_2, ax2, ax3, ax4] = T_reshape[ax0_2, ax2, ax1_2, ax3, ax4]
                    for ax2_ax3_fused in T.vectorized(7):
                        with T.block("T_reshape_1"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3 = T.axis.spatial(544, i0_i1_fused * 4 + ax1)
                            ax2, ax3 = T.axis.remap("SS", [i2, ax2_ax3_fused])
                            T.reads(T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1_3) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1_3) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                            T.writes(T_reshape_1[ax0_3, ax1_3, ax2, ax3])
                            T_reshape_1[ax0_3, ax1_3, ax2, ax3] = T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1_3) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1_3) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7]
                for i3, i4 in T.grid(7, 4):
                    with T.block("T_layout_trans"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0_4, ax1_4, ax2, ax3, ax4])
                        T_layout_trans[ax0_4, ax1_4, ax2, ax3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 544 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=1)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b9)
l20 = sch.fuse(l13, l14)
sch.parallel(loop=l20)
l21 = sch.fuse(l17, l18, l19)
sch.vectorize(loop=l21)
sch.annotate(block_or_loop=l20, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l20, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b10)
l31 = sch.fuse(l26, l27, l28, l29, l30)
sch.vectorize(loop=l31)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b11)
l38 = sch.fuse(l36, l37)
sch.vectorize(loop=l38)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l39, l40, l41, l42 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l39, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l39, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #4: GFLOPs: 0.0000. Time: 0.1148 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #5: GFLOPs: 0.0000. Time: 0.0574 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #6: GFLOPs: 0.0000. Time: 0.2085 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #7: GFLOPs: 0.0000. Time: 0.0295 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #8: GFLOPs: 0.0000. Time: 0.0084 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #9: GFLOPs: 0.0000. Time: 0.0166 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #10: GFLOPs: 0.0000. Time: 0.0159 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #11: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #12: GFLOPs: 0.0000. Time: 0.0078 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #13: GFLOPs: 0.0000. Time: 0.0087 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #14: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for ax0, ax1, ax2 in T.grid(1, 4, 1):
                    for ax3_ax4_fused in T.vectorized(7):
                        with T.block("T_reshape"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, i2, ax3_ax4_fused])
                            T.reads(placeholder[0, (ax1_1 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                            T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3, ax4])
                            T_reshape[ax0_1, ax1_1, ax2_1, ax3, ax4] = placeholder[0, (ax1_1 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
                for i3, i4 in T.grid(7, 4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2, ax2_2, ax3_1, ax4_1 = T.axis.remap("SSSS", [i0_i1_fused, i4, i2, i3])
                            T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3_1, ax4_1])
                            T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3_1, ax4_1])
                            T_transpose[ax0_2, ax1_2, ax2_2, ax3_1, ax4_1] = T_reshape[ax0_2, ax2_2, ax1_2, ax3_1, ax4_1]
                    for ax0_3, ax1_3, ax2_3, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, 0)
                            ax1_4 = T.axis.spatial(544, i0_i1_fused * 4 + i4)
                            ax2_4, ax3_2 = T.axis.remap("SS", [i2, i3])
                            T.reads(T_transpose[0, ((ax3_2 // 7 + ax2_4) // 7 + ax1_4) % 544 // 4, ((ax3_2 // 7 + ax2_4) // 7 + ax1_4) % 4, (ax3_2 // 7 + ax2_4) % 7, ax3_2 % 7])
                            T.writes(T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_2])
                            T_reshape_1[ax0_4, ax1_4, ax2_4, ax3_2] = T_transpose[0, ((ax3_2 // 7 + ax2_4) // 7 + ax1_4) % 544 // 4, ((ax3_2 // 7 + ax2_4) // 7 + ax1_4) % 4, (ax3_2 // 7 + ax2_4) % 7, ax3_2 % 7]
                    with T.block("T_layout_trans"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5, ax2_5, ax3_3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_3])
                        T.writes(T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_3, ax4])
                        T_layout_trans[ax0_5, ax1_5, ax2_5, ax3_3, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 544 and ax2_5 < 7 and ax3_3 < 7, T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2_5, ax3_3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=4)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=2)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b9)
l21 = sch.fuse(l13, l14)
sch.parallel(loop=l21)
l22 = sch.fuse(l19, l20)
sch.vectorize(loop=l22)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l23, l24, l25, l26, l27, l28, l29, l30, l31 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l32, l33, l34, l35, l36, l37, l38, l39 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l32, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l32, ann_key="pragma_unroll_explicit", ann_val=1)
l40, l41, l42, l43 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l40, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l40, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #15: GFLOPs: 0.0000. Time: 0.2104 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #16: GFLOPs: 0.0000. Time: 0.1012 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #17: GFLOPs: 0.0000. Time: 0.2154 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #18: GFLOPs: 0.0000. Time: 0.0155 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #19: GFLOPs: 0.0000. Time: 0.1003 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #20: GFLOPs: 0.0000. Time: 0.1180 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #21: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #22: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'i4'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(7, 7):
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 1, 1, 1):
                    with T.block("T_reshape"):
                        ax0_1 = T.axis.spatial(1, 0)
                        ax1_1, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSS", [ax1, i0_i1_fused, i2, i3])
                        T.reads(placeholder[0, (ax1_1 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax2_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7])
                        T.writes(T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_reshape[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_1 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax2_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7]
                for i4 in T.serial(4):
                    for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2, ax2_2, ax3_2, ax4_2 = T.axis.remap("SSSS", [i0_i1_fused, i4, i2, i3])
                            T.reads(T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_2])
                            T.writes(T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2])
                            T_transpose[ax0_2, ax1_2, ax2_2, ax3_2, ax4_2] = T_reshape[ax0_2, ax2_2, ax1_2, ax3_2, ax4_2]
                    with T.block("T_layout_trans"):
                        ax0_3 = T.axis.spatial(1, 0)
                        ax1_3, ax2_3, ax3_3, ax4_3 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_transpose[0, (ax1_3 * 4 + (ax3_3 // 7 + ax2_3) // 7 + ax4_3) % 544 // 4, ((ax3_3 // 7 + ax2_3) // 7 + ax4_3) % 4, (ax3_3 // 7 + ax2_3) % 7, ax3_3 % 7])
                        T.writes(T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3])
                        T_layout_trans[ax0_3, ax1_3, ax2_3, ax3_3, ax4_3] = T.if_then_else(ax0_3 < 1 and ax1_3 * 4 + ax4_3 < 544 and ax2_3 < 7 and ax3_3 < 7, T_transpose[0, ((ax3_3 // 7 + ax2_3) // 7 + (ax1_3 * 4 + ax4_3)) % 544 // 4, ((ax3_3 // 7 + ax2_3) // 7 + (ax1_3 * 4 + ax4_3)) % 4, (ax3_3 // 7 + ax2_3) % 7, ax3_3 % 7], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=-2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=3)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16, l17, l18, l19, l20 = sch.get_loops(block=b9)
l21 = sch.fuse(l12, l13)
sch.parallel(loop=l21)
sch.annotate(block_or_loop=l21, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l21, ann_key="pragma_unroll_explicit", ann_val=1)
l22, l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33, l34 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #23: GFLOPs: 0.0000. Time: 0.1973 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #24: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2, i3 in T.grid(7, 7):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_reshape"):
                            ax0_2 = T.axis.spatial(1, 0)
                            ax1_2, ax2_1, ax3_1, ax4_1 = T.axis.remap("SSSS", [ax1, i0_i1_fused, i2, i3])
                            T.reads(placeholder[0, (ax1_2 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax2_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7])
                            T.writes(T_reshape[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1])
                            T_reshape[ax0_2, ax1_2, ax2_1, ax3_1, ax4_1] = placeholder[0, (ax1_2 * 136 + (ax4_1 // 7 + ax3_1) // 7 + ax2_1) % 544, (ax4_1 // 7 + ax3_1) % 7, ax4_1 % 7]
                    for ax0_3, ax1_3, ax2, ax3, ax4 in T.grid(1, 1, 1, 1, 1):
                        with T.block("T_transpose"):
                            ax0_4 = T.axis.spatial(1, 0)
                            ax1_4, ax2_2, ax3_2, ax4_2 = T.axis.remap("SSSS", [i0_i1_fused, ax1, i2, i3])
                            T.reads(T_reshape[ax0_4, ax2_2, ax1_4, ax3_2, ax4_2])
                            T.writes(T_transpose[ax0_4, ax1_4, ax2_2, ax3_2, ax4_2])
                            T_transpose[ax0_4, ax1_4, ax2_2, ax3_2, ax4_2] = T_reshape[ax0_4, ax2_2, ax1_4, ax3_2, ax4_2]
                    for ax2_3, ax3_3 in T.grid(1, 1):
                        with T.block("T_reshape_1"):
                            ax0_5 = T.axis.spatial(1, 0)
                            ax1_5 = T.axis.spatial(544, i0_i1_fused * 4 + ax1)
                            ax2_4, ax3_4 = T.axis.remap("SS", [i2, i3])
                            T.reads(T_transpose[0, ((ax3_4 // 7 + ax2_4) // 7 + ax1_5) % 544 // 4, ((ax3_4 // 7 + ax2_4) // 7 + ax1_5) % 4, (ax3_4 // 7 + ax2_4) % 7, ax3_4 % 7])
                            T.writes(T_reshape_1[ax0_5, ax1_5, ax2_4, ax3_4])
                            T_reshape_1[ax0_5, ax1_5, ax2_4, ax3_4] = T_transpose[0, ((ax3_4 // 7 + ax2_4) // 7 + ax1_5) % 544 // 4, ((ax3_4 // 7 + ax2_4) // 7 + ax1_5) % 4, (ax3_4 // 7 + ax2_4) % 7, ax3_4 % 7]
                for i4 in T.serial(4):
                    with T.block("T_layout_trans"):
                        ax0_6 = T.axis.spatial(1, 0)
                        ax1_6, ax2_5, ax3_5, ax4_3 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_6, ax1_6 * 4 + ax4_3, ax2_5, ax3_5])
                        T.writes(T_layout_trans[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3])
                        T_layout_trans[ax0_6, ax1_6, ax2_5, ax3_5, ax4_3] = T.if_then_else(ax0_6 < 1 and ax1_6 * 4 + ax4_3 < 544 and ax2_5 < 7 and ax3_5 < 7, T_reshape_1[ax0_6, ax1_6 * 4 + ax4_3, ax2_5, ax3_5], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=3)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=5)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=5)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22, l23 = sch.get_loops(block=b9)
l24 = sch.fuse(l13, l14)
sch.parallel(loop=l24)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26, l27, l28, l29, l30, l31, l32, l33, l34 = sch.get_loops(block=b10)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l35, l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l35, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l35, ann_key="pragma_unroll_explicit", ann_val=1)
l42, l43, l44, l45 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l42, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l42, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #25: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2 in T.grid(1, 1, 1):
                        for ax3_ax4_fused in T.vectorized(7):
                            with T.block("T_reshape"):
                                ax0_2 = T.axis.spatial(1, 0)
                                ax1_2, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, i2, ax3_ax4_fused])
                                T.reads(placeholder[0, (ax1_2 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                                T.writes(T_reshape[ax0_2, ax1_2, ax2_1, ax3, ax4])
                                T_reshape[ax0_2, ax1_2, ax2_1, ax3, ax4] = placeholder[0, (ax1_2 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
                    for ax2_ax3_fused in T.vectorized(7):
                        with T.block("T_reshape_1"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3 = T.axis.spatial(544, i0_i1_fused * 4 + ax1)
                            ax2, ax3 = T.axis.remap("SS", [i2, ax2_ax3_fused])
                            T.reads(T_reshape[0, ((ax3 // 7 + ax2) // 7 + ax1_3) % 4, ((ax3 // 7 + ax2) // 7 + ax1_3) % 544 // 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                            T.writes(T_reshape_1[ax0_3, ax1_3, ax2, ax3])
                            T_reshape_1[ax0_3, ax1_3, ax2, ax3] = T_reshape[0, ((ax3 // 7 + ax2) // 7 + ax1_3) % 4, ((ax3 // 7 + ax2) // 7 + ax1_3) % 544 // 4, (ax3 // 7 + ax2) % 7, ax3 % 7]
                for i3, i4 in T.grid(7, 4):
                    with T.block("T_layout_trans"):
                        ax0_4 = T.axis.spatial(1, 0)
                        ax1_4, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0_4, ax1_4, ax2, ax3, ax4])
                        T_layout_trans[ax0_4, ax1_4, ax2, ax3, ax4] = T.if_then_else(ax0_4 < 1 and ax1_4 * 4 + ax4 < 544 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0_4, ax1_4 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=-2)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11 = sch.get_child_blocks(b8)
l12, l13, l14, l15, l16, l17, l18, l19, l20, l21 = sch.get_loops(block=b9)
l22 = sch.fuse(l12, l13)
sch.parallel(loop=l22)
l23 = sch.fuse(l20, l21)
sch.vectorize(loop=l23)
sch.annotate(block_or_loop=l22, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l22, ann_key="pragma_unroll_explicit", ann_val=1)
l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b10)
l30 = sch.fuse(l28, l29)
sch.vectorize(loop=l30)
sch.annotate(block_or_loop=l24, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l24, ann_key="pragma_unroll_explicit", ann_val=1)
l31, l32, l33, l34 = sch.get_loops(block=b11)
sch.annotate(block_or_loop=l31, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l31, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #26: Error in building: LocalBuilder: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 154, in <lambda>
    lambda x: _worker_func(*x),
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 212, in _worker_func
    rt_mod: Module = f_build(mod, target, _deserialize_params(params))
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  3: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:534
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 81, in cfun
    rv = local_pyfunc(*pyargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/builder/local_builder.py", line 240, in default_build
    return tvm_build(mod, target=target)
  File "/home/yj/tvm/python/tvm/driver/build_module.py", line 278, in build
    rt_mod_host = _driver_ffi.tir_to_runtime(annotated_mods, target_host)
...
        at /home/yj/tvm/src/ir/transform.cc:267
  10: tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::IRModule, tvm::transform::PassContext)>::operator()(tvm::IRModule, tvm::transform::PassContext) const
        at /home/yj/tvm/src/ir/transform.cc:359
  9: tvm::IRModule tvm::runtime::detail::typed_packed_call_dispatcher<tvm::IRModule>::run<tvm::IRModule, tvm::transform::PassContext>(tvm::runtime::PackedFunc const&, tvm::IRModule&&, tvm::transform::PassContext&&)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1750
  8: tvm::runtime::TVMRetValue tvm::runtime::PackedFunc::operator()<tvm::IRModule, tvm::transform::PassContext>(tvm::IRModule&&, tvm::transform::PassContext&&) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1694
  7: Call
  6: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  5: unpack_call<tvm::IRModule, 2, tvm::tir::transform::MakePackedAPI(int)::<lambda(tvm::IRModule, tvm::transform::PassContext)> >
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1744
  4: run<>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1671
  3: run<tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  2: run<tvm::runtime::TVMMovableArgValueWithContext_, tvm::runtime::TVMMovableArgValueWithContext_>
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1631
  1: operator()
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1646
  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:353
  File "/home/yj/tvm/src/tir/transforms/make_packed_api.cc", line 329
        at /home/yj/tvm/src/tir/transforms/make_packed_api.cc:329
TVMError: Not all Vars are passed in api_args:  'ax1'  is not bound to any variables
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 544, 7, 7), "float32"], T_layout_trans: T.Buffer[(1, 136, 7, 7, 4), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 4, 136, 7, 7], dtype="float32")
        T_transpose = T.alloc_buffer([1, 136, 4, 7, 7], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 544, 7, 7], dtype="float32")
        for i0_i1_fused in T.parallel(136, annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i2 in T.serial(7):
                for ax0, ax1 in T.grid(1, 4):
                    for ax0_1, ax1_1, ax2 in T.grid(1, 1, 1):
                        for ax3_ax4_fused in T.vectorized(7):
                            with T.block("T_reshape"):
                                ax0_2 = T.axis.spatial(1, 0)
                                ax1_2, ax2_1, ax3, ax4 = T.axis.remap("SSSS", [ax1, i0_i1_fused, i2, ax3_ax4_fused])
                                T.reads(placeholder[0, (ax1_2 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7])
                                T.writes(T_reshape[ax0_2, ax1_2, ax2_1, ax3, ax4])
                                T_reshape[ax0_2, ax1_2, ax2_1, ax3, ax4] = placeholder[0, (ax1_2 * 136 + (ax4 // 7 + ax3) // 7 + ax2_1) % 544, (ax4 // 7 + ax3) % 7, ax4 % 7]
                    for ax0_ax1_ax2_ax3_ax4_fused in T.vectorized(7):
                        with T.block("T_transpose"):
                            ax0_3 = T.axis.spatial(1, 0)
                            ax1_3, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, ax1, i2, ax0_ax1_ax2_ax3_ax4_fused])
                            T.reads(T_reshape[ax0_3, ax2, ax1_3, ax3, ax4])
                            T.writes(T_transpose[ax0_3, ax1_3, ax2, ax3, ax4])
                            T_transpose[ax0_3, ax1_3, ax2, ax3, ax4] = T_reshape[ax0_3, ax2, ax1_3, ax3, ax4]
                    for ax2_ax3_fused in T.vectorized(7):
                        with T.block("T_reshape_1"):
                            ax0_4 = T.axis.spatial(1, 0)
                            ax1_4 = T.axis.spatial(544, i0_i1_fused * 4 + ax1)
                            ax2, ax3 = T.axis.remap("SS", [i2, ax2_ax3_fused])
                            T.reads(T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1_4) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1_4) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7])
                            T.writes(T_reshape_1[ax0_4, ax1_4, ax2, ax3])
                            T_reshape_1[ax0_4, ax1_4, ax2, ax3] = T_transpose[0, ((ax3 // 7 + ax2) // 7 + ax1_4) % 544 // 4, ((ax3 // 7 + ax2) // 7 + ax1_4) % 4, (ax3 // 7 + ax2) % 7, ax3 % 7]
                for i3, i4 in T.grid(7, 4):
                    with T.block("T_layout_trans"):
                        ax0_5 = T.axis.spatial(1, 0)
                        ax1_5, ax2, ax3, ax4 = T.axis.remap("SSSS", [i0_i1_fused, i2, i3, i4])
                        T.reads(T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2, ax3])
                        T.writes(T_layout_trans[ax0_5, ax1_5, ax2, ax3, ax4])
                        T_layout_trans[ax0_5, ax1_5, ax2, ax3, ax4] = T.if_then_else(ax0_5 < 1 and ax1_5 * 4 + ax4 < 544 and ax2 < 7 and ax3 < 7, T_reshape_1[ax0_5, ax1_5 * 4 + ax4, ax2, ax3], T.float32(0), dtype="float32")
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=128)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
l5 = sch.sample_compute_location(block=b2, decision=2)
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
l6 = sch.sample_compute_location(block=b1, decision=4)
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
l7 = sch.sample_compute_location(block=b0, decision=4)
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.enter_postproc()
b8 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.parallel")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.vectorize")
sch.unannotate(block_or_loop=b8, ann_key="meta_schedule.unroll_explicit")
b9, b10, b11, b12 = sch.get_child_blocks(b8)
l13, l14, l15, l16, l17, l18, l19, l20, l21, l22 = sch.get_loops(block=b9)
l23 = sch.fuse(l13, l14)
sch.parallel(loop=l23)
l24 = sch.fuse(l21, l22)
sch.vectorize(loop=l24)
sch.annotate(block_or_loop=l23, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l23, ann_key="pragma_unroll_explicit", ann_val=1)
l25, l26, l27, l28, l29, l30, l31, l32, l33 = sch.get_loops(block=b10)
l34 = sch.fuse(l29, l30, l31, l32, l33)
sch.vectorize(loop=l34)
sch.annotate(block_or_loop=l25, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l25, ann_key="pragma_unroll_explicit", ann_val=1)
l35, l36, l37, l38, l39, l40 = sch.get_loops(block=b11)
l41 = sch.fuse(l39, l40)
sch.vectorize(loop=l41)
sch.annotate(block_or_loop=l35, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l35, ann_key="pragma_unroll_explicit", ann_val=1)
l42, l43, l44, l45 = sch.get_loops(block=b12)
sch.annotate(block_or_loop=l42, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l42, ann_key="pragma_unroll_explicit", ann_val=1)
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #27: GFLOPs: 0.0000. Time: 0.0167 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #28: GFLOPs: 0.0000. Time: 0.1565 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #29: GFLOPs: 0.0000. Time: 1.6217 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #30: GFLOPs: 0.0000. Time: 5.2685 ms. Best GFLOPs: 0.0000
[17:19:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_reshape_transpose_reshape_layout_transform_3"] Trial #31: GFLOPs: 0.0000. Time: 2.0430 ms. Best GFLOPs: 0.0000
[17:20:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_reshape_transpose_reshape_layout_transform_3"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |            N/A |          N/A |                   N/A |      0 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1056
Total latency (us): 25802.7

[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #0: GFLOPs: 1.8819. Time: 0.2691 ms. Best GFLOPs: 1.8819
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #1: GFLOPs: 3.1646. Time: 0.1600 ms. Best GFLOPs: 3.1646
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #2: GFLOPs: 1.2243. Time: 0.4137 ms. Best GFLOPs: 3.1646
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #3: GFLOPs: 7.4741. Time: 0.0678 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #4: GFLOPs: 4.3411. Time: 0.1167 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #5: GFLOPs: 0.6017. Time: 0.8418 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #6: GFLOPs: 2.7630. Time: 0.1833 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #7: GFLOPs: 1.1527. Time: 0.4394 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #8: GFLOPs: 2.7030. Time: 0.1874 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #9: GFLOPs: 0.9102. Time: 0.5564 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #10: GFLOPs: 1.5368. Time: 0.3296 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #11: GFLOPs: 0.6491. Time: 0.7803 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #12: GFLOPs: 1.1147. Time: 0.4543 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #13: GFLOPs: 2.9231. Time: 0.1733 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #14: GFLOPs: 1.0931. Time: 0.4633 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #15: GFLOPs: 0.9397. Time: 0.5390 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #16: GFLOPs: 2.4943. Time: 0.2030 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #17: GFLOPs: 0.9824. Time: 0.5155 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #18: GFLOPs: 2.3201. Time: 0.2183 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #19: GFLOPs: 1.2192. Time: 0.4154 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #20: GFLOPs: 1.2512. Time: 0.4048 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #21: GFLOPs: 0.6392. Time: 0.7923 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #22: GFLOPs: 0.4485. Time: 1.1293 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #23: GFLOPs: 0.3561. Time: 1.4223 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #24: GFLOPs: 1.9101. Time: 0.2652 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #25: GFLOPs: 0.5523. Time: 0.9170 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #26: GFLOPs: 3.7349. Time: 0.1356 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #27: GFLOPs: 0.6121. Time: 0.8274 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #28: GFLOPs: 4.8593. Time: 0.1042 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #29: GFLOPs: 1.1303. Time: 0.4481 ms. Best GFLOPs: 7.4741
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #30: GFLOPs: 24.9125. Time: 0.0203 ms. Best GFLOPs: 24.9125
[17:20:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"] Trial #31: GFLOPs: 21.7811. Time: 0.0233 ms. Best GFLOPs: 24.9125
[17:20:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |            N/A |          N/A |                   N/A |      0 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1088
Total latency (us): 25863.7

[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #0: GFLOPs: 0.0000. Time: 0.0080 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #1: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #2: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #3: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #4: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #5: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #6: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #7: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #8: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #9: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #10: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #11: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #12: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #13: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #14: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #15: GFLOPs: 0.0000. Time: 0.0074 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #16: GFLOPs: 0.0000. Time: 0.0076 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #17: GFLOPs: 0.0000. Time: 0.0081 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #18: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #19: GFLOPs: 0.0000. Time: 0.0075 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #20: GFLOPs: 0.0000. Time: 0.0081 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #21: GFLOPs: 0.0000. Time: 0.0077 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #22: GFLOPs: 0.0000. Time: 0.0135 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #23: GFLOPs: 0.0000. Time: 0.2001 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #24: GFLOPs: 0.0000. Time: 0.0321 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #25: GFLOPs: 0.0000. Time: 0.0737 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #26: GFLOPs: 0.0000. Time: 0.0344 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #27: GFLOPs: 0.0000. Time: 0.2871 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #28: GFLOPs: 0.0000. Time: 0.2609 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #29: GFLOPs: 0.0000. Time: 0.6553 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #30: GFLOPs: 0.0000. Time: 2.3211 ms. Best GFLOPs: 0.0000
[17:20:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #34: "fused_layout_transform_7"] Trial #31: GFLOPs: 0.0000. Time: 0.8524 ms. Best GFLOPs: 0.0000
[17:20:07] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #34: "fused_layout_transform_7"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |            N/A |          N/A |                   N/A |      0 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1120
Total latency (us): 25886

[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #0: GFLOPs: 2.6898. Time: 2.7352 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #1: GFLOPs: 1.2472. Time: 5.8990 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #2: GFLOPs: 1.1956. Time: 6.1532 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #3: GFLOPs: 0.6587. Time: 11.1689 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #4: GFLOPs: 2.2771. Time: 3.2308 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #5: GFLOPs: 0.5972. Time: 12.3184 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #6: GFLOPs: 0.9077. Time: 8.1052 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #7: GFLOPs: 0.9196. Time: 8.0004 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #8: GFLOPs: 1.1527. Time: 6.3824 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #9: GFLOPs: 0.5480. Time: 13.4259 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #10: GFLOPs: 0.5912. Time: 12.4434 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #11: GFLOPs: 0.9332. Time: 7.8838 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #12: GFLOPs: 0.8177. Time: 8.9977 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #13: GFLOPs: 0.7420. Time: 9.9158 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #14: GFLOPs: 0.5450. Time: 13.4982 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #15: GFLOPs: 1.2894. Time: 5.7058 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #16: GFLOPs: 0.5853. Time: 12.5697 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #17: GFLOPs: 0.5709. Time: 12.8877 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #18: GFLOPs: 0.6612. Time: 11.1260 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #19: GFLOPs: 0.7527. Time: 9.7740 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #20: GFLOPs: 0.8230. Time: 8.9395 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #21: GFLOPs: 1.3438. Time: 5.4746 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #22: GFLOPs: 0.8047. Time: 9.1425 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #23: GFLOPs: 0.5519. Time: 13.3310 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #24: GFLOPs: 0.4905. Time: 14.9993 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #25: GFLOPs: 0.6813. Time: 10.7988 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #26: GFLOPs: 0.5608. Time: 13.1181 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #27: GFLOPs: 0.9504. Time: 7.7411 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #28: GFLOPs: 0.7431. Time: 9.9008 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #29: GFLOPs: 0.5519. Time: 13.3305 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #30: GFLOPs: 0.4953. Time: 14.8548 ms. Best GFLOPs: 2.6898
[17:20:07] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"] Trial #31: GFLOPs: 0.8920. Time: 8.2483 ms. Best GFLOPs: 2.6898
[17:20:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |         2.6898 |    2735.1856 |             8205.5568 |     32 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |            N/A |          N/A |                   N/A |      0 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1152
Total latency (us): 34091.5

[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #0: GFLOPs: 0.0604. Time: 0.5583 ms. Best GFLOPs: 0.0604
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #1: GFLOPs: 0.2603. Time: 0.1296 ms. Best GFLOPs: 0.2603
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #2: GFLOPs: 0.2005. Time: 0.1682 ms. Best GFLOPs: 0.2603
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #3: GFLOPs: 0.2940. Time: 0.1147 ms. Best GFLOPs: 0.2940
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #4: GFLOPs: 0.6623. Time: 0.0509 ms. Best GFLOPs: 0.6623
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #5: GFLOPs: 0.6827. Time: 0.0494 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #6: GFLOPs: 0.5584. Time: 0.0604 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #7: GFLOPs: 0.3719. Time: 0.0907 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #8: GFLOPs: 0.5398. Time: 0.0625 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #9: GFLOPs: 0.5985. Time: 0.0564 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #10: GFLOPs: 0.2629. Time: 0.1283 ms. Best GFLOPs: 0.6827
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #11: GFLOPs: 2.1989. Time: 0.0153 ms. Best GFLOPs: 2.1989
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #12: GFLOPs: 2.2020. Time: 0.0153 ms. Best GFLOPs: 2.2020
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #13: GFLOPs: 1.5246. Time: 0.0221 ms. Best GFLOPs: 2.2020
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #14: GFLOPs: 2.2787. Time: 0.0148 ms. Best GFLOPs: 2.2787
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #15: GFLOPs: 2.2686. Time: 0.0149 ms. Best GFLOPs: 2.2787
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #16: GFLOPs: 2.2957. Time: 0.0147 ms. Best GFLOPs: 2.2957
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #17: GFLOPs: 4.5240. Time: 0.0075 ms. Best GFLOPs: 4.5240
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #18: GFLOPs: 4.2410. Time: 0.0080 ms. Best GFLOPs: 4.5240
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #19: GFLOPs: 4.3128. Time: 0.0078 ms. Best GFLOPs: 4.5240
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #20: GFLOPs: 4.3477. Time: 0.0078 ms. Best GFLOPs: 4.5240
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #21: GFLOPs: 4.6104. Time: 0.0073 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #22: GFLOPs: 2.1469. Time: 0.0157 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #23: GFLOPs: 2.0871. Time: 0.0162 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #24: GFLOPs: 2.1110. Time: 0.0160 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #25: GFLOPs: 3.5928. Time: 0.0094 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #26: GFLOPs: 3.7111. Time: 0.0091 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #27: GFLOPs: 2.1917. Time: 0.0154 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #28: GFLOPs: 3.6468. Time: 0.0092 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #29: GFLOPs: 4.4941. Time: 0.0075 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #30: GFLOPs: 4.3029. Time: 0.0078 ms. Best GFLOPs: 4.6104
[17:20:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #36: "fused_nn_avg_pool2d_3"] Trial #31: GFLOPs: 2.1808. Time: 0.0155 ms. Best GFLOPs: 4.6104
[17:20:11] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #36: "fused_nn_avg_pool2d_3"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |         2.6898 |    2735.1856 |             8205.5568 |     32 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |         4.6104 |       7.3156 |                7.3156 |     32 |            
 37 |                                      fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1184
Total latency (us): 34098.8

[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0072 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0065 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0069 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0070 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0071 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0065 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0067 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0069 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0067 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0069 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0067 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0322 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.1178 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0421 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0146 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0234 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0485 ms. Best GFLOPs: 0.0000
[17:20:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #37: "fused_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0322 ms. Best GFLOPs: 0.0000
[17:20:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #37: "fused_reshape"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |         2.6898 |    2735.1856 |             8205.5568 |     32 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |         4.6104 |       7.3156 |                7.3156 |     32 |            
 37 |                                      fused_reshape |        1 |      1 |         0.0002 |       6.5069 |                6.5069 |     32 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1216
Total latency (us): 34105.4

[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #0: GFLOPs: 3.6290. Time: 0.3001 ms. Best GFLOPs: 3.6290
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #1: GFLOPs: 8.6321. Time: 0.1262 ms. Best GFLOPs: 8.6321
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #2: GFLOPs: 46.0426. Time: 0.0237 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #3: GFLOPs: 26.8173. Time: 0.0406 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #4: GFLOPs: 30.4646. Time: 0.0357 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #5: GFLOPs: 6.0256. Time: 0.1807 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #6: GFLOPs: 27.8472. Time: 0.0391 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #7: GFLOPs: 20.8751. Time: 0.0522 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #8: GFLOPs: 7.2638. Time: 0.1499 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #9: GFLOPs: 26.8214. Time: 0.0406 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #10: GFLOPs: 43.0025. Time: 0.0253 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #11: GFLOPs: 29.2347. Time: 0.0373 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #12: GFLOPs: 20.7344. Time: 0.0525 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #13: GFLOPs: 26.2386. Time: 0.0415 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #14: GFLOPs: 30.1251. Time: 0.0361 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #15: GFLOPs: 13.5125. Time: 0.0806 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #16: GFLOPs: 2.7074. Time: 0.4022 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #17: GFLOPs: 1.3053. Time: 0.8343 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #18: GFLOPs: 3.5827. Time: 0.3040 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #19: GFLOPs: 0.8852. Time: 1.2302 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #20: GFLOPs: 0.2074. Time: 5.2516 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #21: GFLOPs: 0.2616. Time: 4.1634 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #22: GFLOPs: 0.2082. Time: 5.2300 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #23: GFLOPs: 0.6232. Time: 1.7475 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #24: GFLOPs: 1.2453. Time: 0.8745 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #25: GFLOPs: 0.8777. Time: 1.2408 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #26: GFLOPs: 1.4739. Time: 0.7389 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #27: GFLOPs: 3.0755. Time: 0.3541 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #28: GFLOPs: 8.2331. Time: 0.1323 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #29: GFLOPs: 9.9754. Time: 0.1092 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #30: GFLOPs: 4.3824. Time: 0.2485 ms. Best GFLOPs: 46.0426
[17:20:12] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #38: "fused_nn_dense_add"] Trial #31: GFLOPs: 11.4988. Time: 0.0947 ms. Best GFLOPs: 46.0426
[17:20:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #38: "fused_nn_dense_add"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |         2.6898 |    2735.1856 |             8205.5568 |     32 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |         4.6104 |       7.3156 |                7.3156 |     32 |            
 37 |                                      fused_reshape |        1 |      1 |         0.0002 |       6.5069 |                6.5069 |     32 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |        46.0426 |      23.6520 |               23.6520 |     32 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1248
Total latency (us): 34129

[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #0: GFLOPs: 0.1808. Time: 0.0221 ms. Best GFLOPs: 0.1808
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #1: GFLOPs: 0.0481. Time: 0.0832 ms. Best GFLOPs: 0.1808
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #2: GFLOPs: 0.0001. Time: 57.0270 ms. Best GFLOPs: 0.1808
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #3: GFLOPs: 0.0015. Time: 2.6949 ms. Best GFLOPs: 0.1808
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #4: GFLOPs: 0.0678. Time: 0.0590 ms. Best GFLOPs: 0.1808
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #5: GFLOPs: 0.2718. Time: 0.0147 ms. Best GFLOPs: 0.2718
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #6: GFLOPs: 0.2410. Time: 0.0166 ms. Best GFLOPs: 0.2718
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #7: GFLOPs: 0.2760. Time: 0.0145 ms. Best GFLOPs: 0.2760
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #8: GFLOPs: 0.3190. Time: 0.0125 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #9: GFLOPs: 0.0078. Time: 0.5142 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #10: GFLOPs: 0.0089. Time: 0.4474 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #11: GFLOPs: 0.1838. Time: 0.0218 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #12: GFLOPs: 0.3147. Time: 0.0127 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #13: GFLOPs: 0.0463. Time: 0.0864 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #14: GFLOPs: 0.1700. Time: 0.0235 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #15: GFLOPs: 0.0333. Time: 0.1201 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #16: GFLOPs: 0.1977. Time: 0.0202 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #17: GFLOPs: 0.2404. Time: 0.0166 ms. Best GFLOPs: 0.3190
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #18: GFLOPs: 0.3414. Time: 0.0117 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #19: GFLOPs: 0.0010. Time: 4.1066 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #20: GFLOPs: 0.2742. Time: 0.0146 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #21: GFLOPs: 0.0089. Time: 0.4517 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #22: GFLOPs: 0.3278. Time: 0.0122 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #23: GFLOPs: 0.0308. Time: 0.1299 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #24: GFLOPs: 0.0787. Time: 0.0508 ms. Best GFLOPs: 0.3414
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #25: GFLOPs: 0.3530. Time: 0.0113 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #26: GFLOPs: 0.0044. Time: 0.9003 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #27: GFLOPs: 0.0762. Time: 0.0525 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #28: GFLOPs: 0.0215. Time: 0.1861 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #29: GFLOPs: 0.1445. Time: 0.0277 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #30: GFLOPs: 0.0155. Time: 0.2585 ms. Best GFLOPs: 0.3530
[17:20:15] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #39: "fused_nn_softmax"] Trial #31: GFLOPs: 0.0336. Time: 0.1192 ms. Best GFLOPs: 0.3530
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #39: "fused_nn_softmax"
 ID |                                               Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
-----------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                fused_nn_avg_pool2d |   426496 |      1 |        29.1549 |      14.6286 |               14.6286 |     32 |            
  1 |                              fused_nn_avg_pool2d_1 |   852992 |      1 |        67.6931 |      12.6009 |               12.6009 |     32 |            
  2 |                             fused_layout_transform |        1 |      1 |         0.0001 |       9.4193 |                9.4193 |     32 |            
  3 |               fused_nn_conv2d_multiply_add_nn_relu |  5268480 |      1 |        40.8876 |     128.8528 |              128.8528 |     32 |            
  4 |   fused_reshape_transpose_reshape_layout_transform |        1 |      1 |         0.0001 |      14.8845 |               14.8845 |     32 |            
  5 |        fused_nn_contrib_depthwise_conv2d_NCHWc_add |  1668352 |      1 |        28.3995 |      58.7458 |               58.7458 |     32 |            
  6 |                           fused_layout_transform_1 |        1 |      1 |         0.0001 |      11.4545 |               11.4545 |     32 |            
  7 |                       fused_nn_conv2d_multiply_add |  5092864 |      1 |        33.1480 |     153.6399 |              153.6399 |     32 |            
  8 |                           fused_layout_transform_2 |        1 |      1 |         0.0001 |      10.9528 |               10.9528 |     32 |            
  9 |      fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu | 17160192 |      1 |       109.2976 |     157.0043 |              157.0043 |     32 |            
 10 |                                fused_nn_max_pool2d |   677376 |      1 |        32.5675 |      20.7991 |               20.7991 |     32 |            
 11 |                              fused_nn_avg_pool2d_2 |   602112 |      1 |        89.0907 |       6.7584 |                6.7584 |     32 |            
 12 |         fused_layout_transform_concatenate_nn_relu |   106624 |      1 |         8.2273 |      12.9597 |               12.9597 |     32 |            
 13 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_1 |  2025856 |      3 |        98.6325 |      20.5394 |               61.6183 |     32 |            
 14 |                           fused_layout_transform_3 |        1 |      3 |         0.0001 |      10.7503 |               32.2510 |     32 |            
 15 |           fused_nn_conv2d_multiply_add_add_nn_relu |  7676928 |      3 |        76.3999 |     100.4835 |              301.4506 |     32 |            
 16 |             fused_nn_conv2d_multiply_add_nn_relu_1 |  7570304 |      4 |        65.7743 |     115.0951 |              460.3804 |     32 |            
 17 | fused_reshape_transpose_reshape_layout_transform_1 |        1 |      4 |         0.0001 |       8.6523 |               34.6091 |     32 |            
 18 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_2 |   506464 |      1 |        21.8011 |      23.2311 |               23.2311 |     32 |            
 19 |                           fused_layout_transform_4 |        1 |      1 |         0.0001 |       7.4501 |                7.4501 |     32 |            
 20 |                     fused_nn_conv2d_multiply_add_1 |  1865920 |      1 |         0.5903 |    3160.9833 |             3160.9833 |     32 |            
 21 |                          fused_concatenate_nn_relu |    53312 |      1 |         7.1043 |       7.5042 |                7.5042 |     32 |            
 22 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_3 |  1012928 |      7 |        55.4040 |      18.2826 |              127.9782 |     32 |            
 23 |                           fused_layout_transform_5 |        1 |      7 |         0.0001 |       8.6991 |               60.8940 |     32 |            
 24 |         fused_nn_conv2d_multiply_add_add_nn_relu_1 |  7463680 |      7 |        11.3652 |     656.7124 |             4596.9871 |     32 |            
 25 |             fused_nn_conv2d_multiply_add_nn_relu_2 |  7410368 |      8 |         3.7931 |    1953.6540 |            15629.2317 |     32 |            
 26 | fused_reshape_transpose_reshape_layout_transform_2 |        1 |      8 |         0.0001 |      15.1689 |              121.3513 |     32 |            
 27 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_4 |   253232 |      1 |        18.0563 |      14.0246 |               14.0246 |     32 |            
 28 |                           fused_layout_transform_6 |        1 |      1 |         0.0001 |       7.0435 |                7.0435 |     32 |            
 29 |                     fused_nn_conv2d_multiply_add_2 |  1839264 |      1 |        32.5093 |      56.5766 |               56.5766 |     32 |            
 30 |                        fused_concatenate_nn_relu_1 |    26656 |      1 |         3.8068 |       7.0022 |                7.0022 |     32 |            
 31 |             fused_nn_conv2d_multiply_add_nn_relu_3 |  7330400 |      3 |        48.1168 |     152.3460 |              457.0381 |     32 |            
 32 | fused_reshape_transpose_reshape_layout_transform_3 |        1 |      3 |         0.0001 |       7.4714 |               22.4143 |     32 |            
 33 |      fused_nn_contrib_depthwise_conv2d_NCHWc_add_5 |   506464 |      3 |        24.9125 |      20.3297 |               60.9891 |     32 |            
 34 |                           fused_layout_transform_7 |        1 |      3 |         0.0001 |       7.4211 |               22.2633 |     32 |            
 35 |         fused_nn_conv2d_multiply_add_add_nn_relu_2 |  7357056 |      3 |         2.6898 |    2735.1856 |             8205.5568 |     32 |            
 36 |                              fused_nn_avg_pool2d_3 |    33728 |      1 |         4.6104 |       7.3156 |                7.3156 |     32 |            
 37 |                                      fused_reshape |        1 |      1 |         0.0002 |       6.5069 |                6.5069 |     32 |            
 38 |                                 fused_nn_dense_add |  1089000 |      1 |        46.0426 |      23.6520 |               23.6520 |     32 |            
 39 |                                   fused_nn_softmax |     4000 |      1 |         0.3530 |      11.3302 |               11.3302 |     32 |            
-----------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1280
Total latency (us): 34140.3

[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_nn_conv2d_multiply_add_nn_relu_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 39
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #35: "fused_nn_conv2d_multiply_add_add_nn_relu_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #35 has finished. Remaining task(s): 38
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_nn_conv2d_multiply_add_add_nn_relu_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 37
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_nn_conv2d_multiply_add_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 36
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_conv2d_multiply_add_nn_relu_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 35
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #31: "fused_nn_conv2d_multiply_add_nn_relu_3"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #31 has finished. Remaining task(s): 34
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_nn_conv2d_multiply_add_add_nn_relu"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 33
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 32
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_multiply_add"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 31
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_multiply_add_nn_relu"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 30
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_3"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 29
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #26: "fused_reshape_transpose_reshape_layout_transform_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #26 has finished. Remaining task(s): 28
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 27
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #33: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_5"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #33 has finished. Remaining task(s): 26
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_layout_transform_5"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 25
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_contrib_depthwise_conv2d_NCHWc_add"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 24
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #29: "fused_nn_conv2d_multiply_add_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #29 has finished. Remaining task(s): 23
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_reshape_transpose_reshape_layout_transform_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 22
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_layout_transform_3"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 21
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #38: "fused_nn_dense_add"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #38 has finished. Remaining task(s): 20
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 19
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #32: "fused_reshape_transpose_reshape_layout_transform_3"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #32 has finished. Remaining task(s): 18
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #34: "fused_layout_transform_7"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #34 has finished. Remaining task(s): 17
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_nn_max_pool2d"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 16
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_reshape_transpose_reshape_layout_transform"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 15
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_avg_pool2d"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 14
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #27: "fused_nn_contrib_depthwise_conv2d_NCHWc_add_4"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #27 has finished. Remaining task(s): 13
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_layout_transform_concatenate_nn_relu"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 12
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_avg_pool2d_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 11
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_layout_transform_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 10
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #39: "fused_nn_softmax"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #39 has finished. Remaining task(s): 9
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_layout_transform_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 8
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_layout_transform"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 7
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_concatenate_nn_relu"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 6
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_layout_transform_4"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 5
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #36: "fused_nn_avg_pool2d_3"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #36 has finished. Remaining task(s): 4
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #28: "fused_layout_transform_6"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #28 has finished. Remaining task(s): 3
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #30: "fused_concatenate_nn_relu_1"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #30 has finished. Remaining task(s): 2
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_avg_pool2d_2"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 1
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #37: "fused_reshape"
[17:20:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #37 has finished. Remaining task(s): 0
[[2.65310928e-27 7.80934500e-29 6.42505376e-33 3.80880106e-30
  1.72494268e-35 1.53741104e-29 6.54108136e-35 2.24100342e-28
  2.60202916e-29 2.01549076e-30 9.81468460e-27 3.67234870e-21
  3.03370784e-25 6.77914013e-27 3.05189330e-27 7.15809227e-28
  5.81803594e-31 3.71049094e-31 2.19271848e-20 1.70633772e-26
  8.75393097e-36 3.72717841e-36 8.23774876e-36 1.67117890e-25
  1.29632696e-26 2.07715018e-34 2.47523240e-29 2.01713987e-27
  2.17436867e-35 9.99821800e-36 1.10174099e-33 1.13478686e-24
  1.68167829e-34 1.02182087e-32 6.65887919e-28 4.18009206e-35
  1.20055031e-32 2.14415827e-33 1.09865260e-17 5.67707431e-20
  2.56816430e-15 3.52304263e-26 4.34506861e-19 7.08658634e-27
  1.88767289e-31 2.99772666e-23 3.70989658e-16 6.72206679e-22
  8.66234766e-21 3.83955779e-43 9.69961981e-38 6.51300842e-31
  5.77442670e-26 1.19149057e-30 1.08519365e-26 4.14678114e-23
  1.80946944e-28 2.88824429e-39 9.65584297e-29 8.56693794e-31
  1.00055495e-19 4.65452359e-19 1.65128782e-24 1.69071225e-23
  1.49089673e-26 5.93383985e-27 1.52897067e-30 3.18819406e-29
  1.25260120e-32 1.33776832e-25 1.20060157e-23 6.59371126e-29
  1.06813948e-29 8.92837081e-18 3.45506528e-30 1.66898520e-20
  2.51035769e-15 4.45178230e-16 9.25274973e-25 7.61204672e-29
  1.19907253e-31 9.92189208e-35 7.69450074e-27 3.26075360e-35
  1.69397633e-26 4.98597413e-29 5.10142000e-25 1.06420818e-26
  7.39703575e-30 1.18368550e-29 6.07455096e-29 5.53868714e-30
  5.00435969e-30 1.30327323e-34 1.10197352e-23 8.96093093e-39
  1.09595489e-28 6.36053184e-24 6.25307930e-27 1.49061505e-30
  2.10622771e-32 2.16618613e-30 5.64926410e-30 1.54775585e-29
  5.54982998e-31 1.59540140e-29 1.24382643e-31 2.04210513e-23
  2.45609882e-31 1.81883567e-25 7.62860055e-33 7.04776152e-28
  9.25382496e-29 4.93751450e-31 1.33378557e-22 1.88185925e-28
  6.96706640e-34 2.22171206e-31 6.27601136e-30 2.08058919e-29
  4.64685633e-28 1.96802957e-29 4.99036202e-32 4.85610492e-33
  1.08448489e-28 1.98311712e-29 5.37448571e-31 3.67227043e-33
  2.46415442e-28 9.91718967e-37 5.58353494e-37 1.11378430e-33
  3.01109628e-34 1.15002428e-35 1.79750390e-30 2.23484404e-39
  1.82287351e-30 1.13443766e-29 5.37203395e-28 9.03749351e-37
  2.17199985e-33 5.20632096e-34 2.30294031e-37 2.99107476e-33
  2.56489029e-33 5.86441778e-27 4.13305401e-32 2.47452359e-37
  1.04158396e-34 4.18175084e-35 2.77613027e-28 4.68387961e-18
  1.81031634e-23 6.46045275e-23 1.60854481e-23 3.21006565e-20
  5.95971918e-21 5.17925144e-23 2.28099383e-23 6.06188093e-24
  1.59414364e-27 4.62727665e-27 2.95126244e-23 1.07675845e-23
  7.62863600e-24 1.80668672e-24 3.98446217e-26 2.39492624e-29
  9.05318311e-24 2.66983342e-25 7.49798279e-28 4.97298238e-21
  4.80618650e-23 5.45698807e-27 2.15267837e-21 4.98424603e-28
  9.92714963e-21 3.19253422e-32 6.69589574e-19 8.55220989e-24
  1.16161716e-25 7.51013183e-25 5.13575722e-21 8.63984730e-27
  1.11439969e-27 1.56865385e-23 9.26867745e-26 1.50974776e-20
  4.63295960e-29 3.21774786e-23 7.84963154e-30 1.85338395e-20
  7.52438026e-26 9.92392429e-30 2.61765624e-24 3.39042592e-21
  1.66528143e-23 3.24891833e-29 8.06548954e-27 2.45390646e-24
  8.88509689e-23 1.03977902e-24 1.09640371e-18 8.95547575e-21
  1.40375933e-21 3.44015046e-23 1.12061883e-28 6.26625544e-21
  2.99288029e-24 2.55444900e-28 4.23220505e-28 1.72247345e-24
  5.51794011e-21 2.32503856e-26 2.38432937e-25 7.65565226e-26
  1.42762120e-24 1.05853742e-23 8.85925299e-20 6.98191921e-17
  1.19568649e-26 2.48153405e-25 1.03569473e-22 1.27517874e-25
  1.25153037e-26 1.12807173e-23 1.26444456e-25 2.21080098e-28
  6.98178078e-26 2.42495009e-22 4.35861822e-25 2.64093572e-24
  4.78831821e-26 3.25209603e-27 7.77441570e-30 1.46443260e-26
  9.74612010e-25 1.54143831e-23 2.80726137e-26 2.28206848e-27
  3.26370044e-29 9.57846663e-30 2.42897129e-27 1.27517381e-25
  1.67636012e-29 4.17127716e-23 7.88422916e-31 2.00887836e-26
  1.46849453e-25 2.36213671e-28 7.33414152e-26 1.48201277e-24
  3.14735859e-24 2.64897717e-24 2.23863629e-21 2.27358375e-23
  8.52311284e-27 1.41552202e-25 7.29403462e-22 1.66745619e-22
  8.11089238e-23 6.39488121e-25 1.78767004e-23 1.06576070e-25
  3.86586381e-28 1.10332325e-24 8.42885180e-22 6.47007931e-28
  1.67730379e-25 1.49013262e-29 1.02088057e-27 8.06164877e-26
  5.07944500e-30 5.52436213e-29 6.12255535e-33 1.81215282e-32
  3.07800349e-33 1.41898102e-34 5.01666871e-30 8.04700128e-24
  6.27995914e-31 5.61426446e-31 5.01227409e-28 1.65188627e-24
  1.55139206e-28 4.47914167e-30 4.44715310e-29 6.44208520e-33
  9.11268010e-23 6.40045683e-29 7.08010680e-33 6.09220166e-32
  4.21070023e-31 8.45286864e-23 5.42293146e-28 1.47905522e-29
  6.95941488e-27 1.68843100e-30 7.21751337e-27 5.13746086e-29
  1.47992893e-23 3.70443221e-19 5.63139995e-17 1.38336829e-20
  1.25795218e-21 3.31888648e-25 3.75862907e-23 3.87322677e-22
  1.80227808e-20 7.94426381e-27 2.23084973e-24 1.51275486e-23
  4.81383046e-26 2.31435726e-19 1.59022565e-20 3.77412528e-19
  1.56063467e-27 1.88870773e-26 4.64582244e-23 2.38614505e-21
  4.45634828e-18 3.52078837e-32 8.50174657e-27 5.05457124e-29
  6.23332944e-29 3.02875170e-37 6.62413824e-30 3.28763452e-26
  5.41188210e-18 1.78526038e-35 2.76458592e-31 1.70451961e-37
  1.31759015e-26 2.06922985e-30 4.17342283e-32 3.94351700e-32
  6.74035162e-36 9.31347480e-34 1.60035600e-30 3.00719631e-39
  1.56451155e-24 5.21850015e-30 9.63448321e-33 2.07985749e-38
  4.81248275e-29 2.58095339e-36 0.00000000e+00 1.12722653e-34
  6.43719395e-34 1.57108838e-34 1.53151115e-36 2.81499853e-37
  7.57768063e-38 4.33708651e-34 1.14654100e-29 3.94351309e-34
  2.75640154e-25 8.10897867e-22 1.20240560e-26 7.02224410e-27
  5.83860316e-23 2.50125893e-29 8.18320510e-32 1.09433370e-21
  7.80897918e-31 3.46172676e-23 7.19782276e-28 7.93232862e-28
  3.72135199e-22 1.08597849e-22 8.28158913e-33 4.73539454e-30
  6.74239889e-24 5.77346313e-24 1.14313459e-29 3.19905803e-31
  2.51171144e-35 3.40914799e-24 1.59400209e-29 4.52213128e-30
  1.76878451e-24 1.60878259e-25 1.60156497e-23 1.78306028e-22
  3.83783840e-38 8.84624883e-28 3.09422623e-33 4.26770578e-30
  2.46452844e-22 4.33067665e-32 1.13761773e-23 4.34078419e-31
  2.37959613e-17 9.90866433e-33 2.48262556e-38 6.60122133e-26
  2.71790050e-28 1.03638289e-21 1.83921379e-27 2.10171675e-25
  2.04889324e-31 1.28263380e-13 6.64282281e-27 1.39945353e-31
  6.00948221e-30 5.46682777e-25 1.28281463e-32 1.38865635e-26
  4.74602318e-32 1.00182445e-16 3.34017787e-15 5.80499172e-19
  3.10316704e-15 1.56063281e-22 2.80512265e-17 6.74652730e-28
  2.00285324e-29 8.60320918e-27 3.73110307e-22 7.61297424e-22
  3.83148881e-31 5.20458927e-28 9.14341657e-25 1.11799394e-26
  2.43595024e-26 1.20382593e-30 7.54635806e-29 4.50847913e-27
  1.32408370e-28 9.43508767e-20 5.73307790e-10 1.30412405e-26
  8.94686392e-30 3.32586647e-27 2.79797069e-26 2.80342720e-28
  6.60635017e-29 2.48446002e-33 1.61550312e-19 8.57822242e-23
  6.21746580e-29 3.50463494e-28 2.26966222e-27 1.19289219e-21
  1.54213198e-31 6.22264498e-27 1.23174625e-19 6.63135644e-26
  4.75412050e-33 4.71142988e-30 5.30714893e-32 1.49302122e-24
  1.42715137e-26 3.09553251e-28 4.31031021e-30 9.28387655e-22
  1.54601746e-26 5.20996861e-22 1.18288383e-26 6.95001961e-28
  3.99146724e-30 1.05413735e-30 2.49025315e-29 4.21938032e-25
  8.61670619e-20 1.30490521e-26 1.10215768e-32 1.35129119e-28
  2.29321589e-24 3.23795216e-24 1.93093598e-22 6.41931556e-27
  5.87966052e-31 2.77797503e-27 1.00210062e-29 1.85037403e-28
  1.63129358e-40 6.93405777e-26 9.27007740e-29 8.06967913e-26
  5.73042098e-29 2.13072430e-15 2.13957128e-06 5.40651178e-35
  3.09493745e-31 1.34409443e-12 7.80327803e-27 4.21914910e-20
  1.32962087e-28 1.07598356e-21 2.37242298e-20 9.58065700e-23
  2.55856332e-29 3.24438251e-31 3.10791133e-34 3.23610068e-31
  4.77626445e-24 2.68006058e-23 3.04045063e-25 1.22161567e-26
  3.85312756e-36 6.11300526e-27 8.98078512e-30 6.03008714e-28
  3.88692476e-26 4.49247103e-23 7.12876934e-21 7.67669814e-21
  6.73489288e-24 1.27882348e-31 5.16150429e-29 2.71395187e-29
  6.58229040e-25 3.69603724e-32 2.76423007e-29 7.17186594e-21
  8.74903505e-26 2.02391905e-23 3.54055911e-25 1.27325191e-28
  5.31583658e-26 3.22176663e-29 5.12006866e-21 1.81926771e-23
  1.61725585e-32 6.54033890e-33 9.32026464e-26 7.15172764e-22
  2.11754312e-28 6.15677488e-25 8.20815795e-16 2.13092693e-23
  3.73271647e-27 1.70446592e-24 5.44185853e-33 4.67832024e-28
  6.99211514e-28 1.32403511e-25 9.39794532e-20 1.71187516e-16
  1.28883216e-26 3.70838539e-22 1.12953078e-24 5.25292115e-27
  1.67117674e-20 1.55718114e-14 6.66405080e-24 4.44756466e-27
  9.62257097e-28 3.07735847e-25 2.44672432e-27 3.71406692e-27
  2.50924525e-24 2.94174891e-26 2.61758642e-24 6.27359495e-22
  2.95634800e-13 2.06708406e-30 4.57673950e-33 3.21911240e-22
  2.79821782e-25 1.09973306e-27 3.74176443e-22 6.07767825e-22
  5.33535901e-34 3.33642123e-30 2.99253849e-31 2.40404471e-20
  9.34164315e-29 1.11472791e-33 3.47086722e-21 5.09946407e-29
  1.20043607e-24 1.13977050e-26 1.65659810e-19 3.47400736e-28
  2.98896962e-41 8.01642030e-25 4.71479447e-29 2.90017278e-21
  4.42262247e-29 8.46106463e-15 1.02536397e-29 7.81759106e-24
  1.68829877e-19 5.26683417e-24 2.15594748e-30 6.96389342e-22
  1.48101136e-18 3.02444132e-17 8.90187685e-26 4.87017772e-21
  6.59334028e-27 3.58425027e-24 5.18525435e-33 1.44643379e-25
  5.38737222e-25 6.79855557e-29 1.09668848e-25 8.61002718e-07
  2.24307437e-27 1.27158764e-27 2.19774356e-27 1.74430749e-31
  3.56551498e-23 4.22793875e-16 1.50744929e-23 1.18342122e-33
  1.16670555e-19 2.49842860e-20 9.35688408e-19 2.14593927e-28
  5.27945057e-30 8.73676148e-26 2.35267978e-25 2.66761951e-21
  3.56137074e-22 6.21003261e-26 3.07534862e-27 1.45475212e-23
  5.34930808e-23 1.04986658e-24 2.35787010e-20 1.47792770e-21
  3.44951271e-27 3.23507157e-34 4.11002277e-16 1.20001435e-25
  5.80605324e-27 3.90123271e-32 1.41572339e-23 1.32343486e-24
  9.99584138e-01 2.20683592e-25 5.96457114e-26 7.59165104e-24
  1.70242414e-18 2.53053328e-27 9.59245370e-26 1.16798391e-26
  5.99146174e-17 6.72288010e-24 1.43137617e-27 6.35185500e-23
  2.70024548e-16 1.96860660e-30 1.01181122e-29 2.59471064e-24
  2.06648263e-32 1.16133782e-33 1.07739831e-10 1.20376223e-14
  3.95771316e-32 3.74286757e-35 2.40037676e-22 6.52850746e-23
  1.05706699e-26 2.04480484e-28 4.60194364e-28 3.14645523e-26
  3.59681051e-24 2.65971144e-23 1.01968308e-17 6.39035028e-31
  9.16564182e-22 6.15673536e-30 9.86575293e-31 2.21226664e-30
  1.07748905e-32 1.38380919e-12 1.12142643e-29 1.71301008e-33
  1.21855813e-33 1.58812417e-25 2.58986796e-22 1.60575491e-31
  2.24652379e-21 1.05754209e-20 4.49743900e-25 1.79504707e-19
  1.61627772e-29 2.37670396e-23 1.76278846e-25 5.25653064e-34
  1.21327290e-29 2.03833832e-23 1.36724311e-17 9.04882376e-26
  4.62788626e-23 2.64958102e-26 4.64105707e-38 7.38338210e-25
  6.00494363e-23 3.01372695e-27 3.72948291e-36 4.27052721e-28
  6.18470046e-30 2.19488990e-24 5.12932602e-32 6.17480537e-28
  6.80215865e-15 8.24838756e-26 1.01330117e-31 7.75102808e-21
  7.65042236e-27 2.68425574e-23 1.19955073e-27 6.50316901e-26
  2.25455485e-25 4.43428197e-20 6.92282716e-18 2.77474283e-21
  2.35577039e-24 3.99370854e-29 1.22039847e-21 2.25609053e-42
  1.91325961e-28 4.60682866e-27 2.52075526e-26 1.19118274e-28
  3.90264204e-27 1.00642877e-12 3.97668291e-21 1.88967877e-28
  9.02527573e-34 4.34645004e-24 4.56102762e-29 2.87898346e-16
  3.15315575e-21 4.96006169e-36 2.30858887e-28 3.75664143e-28
  1.04941634e-20 2.48868697e-21 3.83974785e-20 7.20871903e-24
  6.67735603e-25 7.43204249e-25 1.08536337e-26 8.53840857e-27
  7.14417048e-23 8.71733659e-22 3.51967910e-29 4.30108271e-21
  6.17039272e-31 5.24922708e-15 2.55815779e-18 6.70045882e-22
  1.47780025e-18 2.13165868e-23 2.07517387e-23 6.85071896e-24
  6.11123849e-18 1.70891858e-17 2.42491200e-08 3.14114792e-30
  2.37639022e-20 4.80690736e-28 2.83867206e-27 6.78281123e-20
  2.19286745e-29 9.79394243e-24 1.29773898e-23 3.17698445e-24
  1.12325376e-23 2.08328204e-21 2.01065603e-27 2.26021350e-24
  2.41449845e-13 9.34780357e-23 1.42608836e-21 1.79005082e-24
  5.43337432e-18 1.13357600e-23 2.22103884e-27 6.32897895e-29
  3.93564111e-25 1.10435064e-24 3.58950645e-23 1.57311576e-26
  1.83276106e-37 2.88893899e-24 8.00547796e-18 5.74800214e-21
  3.81666175e-19 2.92192079e-14 3.54066536e-26 2.41722107e-21
  9.36508036e-30 7.25413648e-21 2.20420339e-24 1.87145829e-27
  3.34006319e-30 1.56305961e-27 4.68212403e-20 5.08683049e-31
  9.41729742e-18 9.55424223e-21 1.41330305e-27 7.30918680e-22
  1.57053345e-31 1.56667830e-33 4.41333959e-35 5.39685433e-25
  2.24590102e-26 2.59873435e-15 1.14633723e-26 3.47125743e-25
  3.73277714e-25 5.57857585e-29 6.71519029e-24 4.07983724e-04
  4.67081769e-26 7.96692787e-21 3.71012854e-28 4.85525664e-24
  2.43803953e-28 1.67665766e-25 4.50700400e-18 1.15906045e-22
  3.88151078e-29 5.53055031e-33 3.17930913e-26 2.16206387e-25
  3.32457737e-24 1.14632128e-31 1.07285171e-22 7.08757050e-24
  3.56398209e-06 1.53911619e-32 2.01570559e-23 5.47019860e-28
  1.69949478e-41 4.34204291e-31 1.83723783e-18 3.66316395e-32
  1.53607168e-25 1.80661355e-21 1.92899132e-23 4.67678807e-27
  2.26817227e-28 6.99522063e-25 6.56599224e-24 2.15080521e-30
  1.22660406e-24 6.31842271e-26 6.48783121e-26 6.54911900e-28
  9.63913294e-07 1.13671367e-26 7.24959910e-21 2.41516126e-17
  1.89025977e-16 2.13763892e-35 4.15485623e-23 5.07423515e-15
  2.68414642e-28 3.23701951e-28 2.37370904e-27 8.18225017e-29
  2.64623316e-29 3.73082890e-24 4.70288884e-20 4.21544930e-35
  2.80426432e-26 1.46538055e-21 1.07837204e-19 7.73018135e-21
  1.15226977e-25 1.28904618e-22 2.99847383e-28 2.70595468e-26
  1.43030126e-19 4.33634882e-31 3.05914392e-34 2.58403889e-27
  3.03142276e-29 1.30039222e-20 6.69015973e-24 2.82792792e-20
  1.83464412e-28 2.02032346e-24 4.60147505e-18 4.78102207e-24
  2.86505781e-24 7.06942138e-25 1.61382944e-32 6.46326272e-38
  3.62200169e-27 4.22846355e-27 3.96374660e-22 6.76591207e-27
  1.38746181e-23 1.39790858e-20 2.15493000e-28 4.67836896e-30
  2.53307190e-28 1.24017375e-23 3.16213447e-18 5.45823286e-21
  7.53968640e-28 1.35119126e-31 6.68398246e-25 3.00815805e-26
  3.66262384e-07 5.03282857e-22 1.13883282e-13 2.27463460e-22
  3.18960541e-29 4.00449484e-21 7.94985385e-24 7.98404860e-23
  1.40272416e-24 6.18789080e-36 4.73228939e-37 1.00091328e-35
  1.12151968e-23 1.23523670e-28 8.27643957e-26 2.30775127e-29
  4.56121419e-16 2.04100447e-18 4.72095658e-26 2.88085228e-30
  9.26571432e-32 1.35588086e-29 2.03498347e-29 6.54331840e-30
  1.30055557e-28 9.96559846e-26 4.46937886e-30 3.15193288e-26
  1.12562089e-24 4.33173400e-32 1.48147737e-22 2.39613250e-29
  2.92455518e-31 1.08983946e-27 2.04359714e-27 3.41856177e-25
  7.19582586e-26 2.43321874e-27 5.50389852e-28 2.35331172e-37
  5.86753589e-32 4.88007025e-29 7.18972067e-30 6.42605662e-26
  3.60121561e-25 2.56230088e-24 1.01580100e-24 2.30844268e-25
  3.25919244e-24 4.70211756e-20 6.35355073e-23 6.46393271e-14
  2.41058182e-21 8.78997219e-28 5.34746247e-34 1.41749226e-28
  1.85143840e-30 9.50411219e-29 1.07297469e-29 2.24711790e-26
  4.07511495e-31 1.94643332e-33 1.02385784e-24 1.04096408e-27
  7.35541481e-24 1.10321998e-28 2.81254773e-32 4.09822151e-22
  1.10764794e-35 1.89936135e-22 3.07554179e-31 3.12573635e-25
  1.07808027e-26 1.56110903e-22 2.98900506e-29 1.97846239e-29
  9.50756656e-27 8.87434499e-26 1.77860189e-24 3.12951540e-27
  6.61553024e-29 3.81120598e-19 6.35585711e-37 2.28193665e-28
  7.89169318e-23 7.54681103e-31 3.64091086e-27 4.44023101e-24
  1.56539791e-28 1.06301318e-30 2.16894520e-25 1.97984005e-18
  1.53720449e-23 1.29568182e-22 2.37944233e-24 1.26710221e-22]]
[[1.52953842e-03 1.03893930e-04 3.21686530e-04 4.61673975e-04
  3.22015653e-03 1.69560616e-03 1.63337274e-03 3.30949406e-04
  4.32345754e-04 1.21156278e-04 3.23652232e-04 2.80423602e-03
  1.39548152e-03 9.14226053e-04 8.98631988e-04 1.51272211e-03
  1.45565471e-04 5.15360967e-04 6.29548216e-03 3.08430230e-04
  7.07045838e-05 8.42930021e-05 8.50674580e-04 8.56652216e-04
  4.71904350e-04 4.10265311e-05 2.10057115e-04 8.78997598e-05
  8.40575885e-05 5.41113477e-05 1.76030109e-04 2.69795739e-04
  1.08484383e-04 2.87581846e-04 1.13520992e-03 2.24421572e-04
  2.21822134e-04 1.87060621e-04 1.47404222e-04 1.34627626e-03
  7.21168180e-04 2.82424036e-04 3.36137076e-04 9.89685141e-05
  7.37145601e-04 2.58156681e-04 3.94119369e-03 8.10174533e-05
  1.70076266e-03 3.40853949e-05 6.60065096e-04 1.23172813e-05
  4.67589329e-04 4.60396259e-04 4.69094113e-04 3.66072840e-04
  6.93481823e-04 1.58285300e-04 3.42212006e-04 2.09452017e-04
  8.47645453e-04 3.36371624e-04 2.19297421e-04 2.81536602e-04
  3.71378352e-04 3.34426272e-03 4.50709049e-04 8.19899316e-04
  4.24786820e-04 8.02514318e-04 6.42133295e-04 1.50033040e-03
  6.72371592e-04 2.40533822e-03 1.63683470e-03 2.00230815e-03
  1.97969959e-03 2.25134986e-03 1.93305733e-03 1.75580115e-03
  8.86823545e-05 1.95930377e-04 1.15395535e-03 3.61152961e-05
  1.76188746e-03 1.02509419e-03 3.34325654e-04 3.08244402e-04
  7.53356609e-04 1.32914295e-03 3.42633924e-04 2.23302253e-04
  3.97549098e-04 5.23121911e-04 6.20449136e-04 7.05275033e-06
  1.43953366e-03 1.14566334e-04 3.91406211e-05 7.30605272e-04
  4.52730572e-04 6.47906054e-05 4.48786013e-04 3.10096482e-04
  6.93478447e-04 8.25868221e-04 9.03041800e-04 6.88411342e-03
  7.77587353e-04 1.92118017e-03 4.27589606e-04 8.16899538e-03
  2.95290462e-04 7.10891676e-04 7.21718592e-04 9.92868590e-05
  8.76576669e-05 2.09272112e-04 5.81061759e-04 2.00926122e-04
  6.89151857e-05 2.33454441e-04 7.83293726e-05 2.56132073e-04
  3.75567237e-04 1.73886001e-04 8.48824508e-04 2.34480511e-04
  5.91372489e-04 1.74226530e-04 1.38552059e-04 1.10087520e-03
  7.50120147e-04 4.66634927e-04 5.55812498e-04 3.45097040e-04
  2.28635225e-04 3.28827562e-04 8.15638923e-04 8.99995648e-05
  8.68313000e-05 1.29214750e-04 1.28681655e-04 2.08377620e-04
  7.74238579e-05 4.82742180e-04 1.70614891e-04 6.23489323e-05
  5.03247429e-04 2.01820672e-04 7.68742539e-05 2.00376031e-03
  9.35429780e-05 1.28277682e-03 3.28540744e-04 1.15779182e-03
  1.49044179e-04 1.27937121e-04 4.72828688e-04 6.48804125e-04
  1.17066251e-04 1.32624316e-03 4.37298510e-03 3.74570722e-04
  4.95949003e-04 1.50155640e-04 1.97818517e-04 1.89055190e-05
  1.05310697e-03 2.01923278e-04 1.59818883e-04 3.14390613e-03
  9.17854122e-05 6.52733899e-04 1.20601733e-04 3.01882392e-05
  1.01170415e-04 1.65448437e-04 3.93552799e-03 7.69277176e-05
  2.61151319e-04 2.19421141e-04 5.61139022e-04 2.35433952e-04
  4.08201537e-04 4.26334504e-04 1.96256980e-04 2.26657651e-03
  6.35444667e-05 9.47535154e-04 2.27562559e-04 4.66903584e-04
  5.41191141e-04 3.05414695e-04 1.41334065e-04 1.79740542e-03
  6.72698719e-04 3.41369072e-04 8.30255376e-05 1.40137086e-03
  2.04718919e-04 9.71451344e-04 1.35416281e-03 1.79239549e-03
  1.71741867e-03 2.78834894e-04 1.99785412e-04 4.45097801e-04
  5.20560949e-04 4.54398018e-04 8.83575995e-05 9.19162354e-04
  1.26995216e-03 7.97461835e-04 4.66999307e-04 7.01391371e-04
  2.38841196e-04 7.70365237e-04 2.25708776e-04 2.69666989e-03
  8.33722734e-05 2.28348013e-04 1.12540343e-04 7.33734865e-04
  7.18753028e-04 2.89868331e-04 9.53897179e-05 4.13293223e-04
  9.68387249e-05 2.48504221e-04 6.63999119e-04 3.25799425e-04
  1.42979072e-04 2.76786777e-05 2.45544070e-04 1.85046927e-04
  6.59014739e-04 1.12185732e-03 1.06175139e-03 2.88800453e-04
  2.68419099e-04 1.30505578e-04 8.46497132e-04 1.33240916e-04
  6.13709417e-05 1.55645213e-03 4.60907177e-04 9.38890807e-05
  3.73399438e-04 6.27543996e-05 5.03549178e-04 6.76422496e-04
  1.87365658e-04 8.86389054e-04 1.75371859e-03 2.23197385e-05
  5.27108437e-04 3.09207157e-04 6.27894187e-05 3.80815094e-04
  3.33538890e-04 1.13425223e-04 1.10977664e-04 1.37850526e-03
  3.52514442e-04 8.38827342e-04 6.25858258e-04 1.43901139e-04
  4.04969251e-05 1.60403011e-04 3.36978643e-04 2.88599462e-04
  6.95221359e-04 3.29736213e-04 7.82698189e-05 1.23968930e-04
  3.40675557e-04 2.46966520e-04 1.16644376e-04 2.50522891e-04
  4.45395039e-04 1.18474150e-03 9.52509989e-04 1.35890744e-03
  1.61065056e-03 9.62143298e-04 5.68768883e-04 1.20409866e-04
  1.14367605e-04 1.56353213e-04 4.96537759e-05 3.46148503e-04
  2.50664249e-04 1.97564557e-04 3.81561636e-04 9.94744245e-04
  4.25141421e-04 2.23270850e-04 6.17244164e-04 6.02117972e-04
  1.92254200e-04 1.63694727e-03 7.81216804e-05 7.97647052e-04
  1.97089670e-04 4.84395947e-04 5.30467252e-04 9.21535306e-04
  5.82451816e-04 2.01698494e-04 2.06437148e-03 4.22623183e-04
  4.31386346e-04 7.13465782e-03 1.61338563e-03 2.65426189e-03
  1.68694323e-03 2.34808750e-03 3.43549275e-03 2.04275246e-04
  3.56556353e-04 6.48629939e-05 3.60224913e-05 1.12703216e-04
  1.55188172e-04 1.41120530e-04 3.11403674e-05 5.29170502e-04
  3.87055421e-04 9.70347610e-05 6.03112741e-04 3.19942134e-04
  2.37211585e-04 8.33482700e-05 2.00564973e-04 6.95490919e-04
  1.69873238e-04 4.09200497e-04 1.06055720e-03 8.46460316e-05
  7.54644498e-05 2.28738485e-04 4.13217786e-04 5.75593265e-04
  5.45445873e-05 1.64395111e-04 1.25347491e-04 1.96177833e-04
  9.51828988e-05 2.79831089e-04 1.36077171e-04 6.36976692e-05
  1.01465354e-04 9.38606099e-05 1.95984947e-04 1.65515507e-04
  2.67477852e-04 1.26560510e-04 2.93495745e-04 3.18438979e-04
  1.44263482e-04 1.64982886e-03 1.45844286e-04 1.11866905e-03
  1.50194450e-04 1.32997672e-03 1.15022028e-03 4.62190044e-04
  3.88949033e-04 6.19219267e-04 1.04445935e-04 8.87171627e-05
  1.32415909e-03 3.61114391e-04 1.36229617e-04 2.45868374e-04
  7.84708318e-05 4.64860896e-05 5.19839377e-05 5.48830176e-05
  3.94468152e-05 2.66270450e-04 1.15252944e-04 8.25729279e-04
  1.10667670e-05 1.14960079e-04 1.94082662e-04 9.06440837e-05
  1.23349790e-04 1.92894324e-04 5.16745145e-04 1.57367764e-03
  2.11323798e-03 2.81743822e-04 2.57165433e-04 9.78702214e-04
  2.47257005e-04 7.01959187e-04 8.08681580e-05 2.30751373e-02
  5.51313860e-04 3.89306981e-04 2.53431383e-04 2.47986463e-04
  5.74115529e-06 1.96783731e-04 2.57159190e-05 4.81458526e-04
  9.64176870e-05 6.40944811e-04 1.64226280e-04 1.34783201e-02
  1.46577868e-03 1.43232383e-03 3.01851542e-03 1.44754440e-05
  5.52646234e-04 9.35053395e-04 3.97920748e-03 1.01812661e-03
  1.01977494e-03 6.15202822e-04 6.75524818e-04 6.19862694e-05
  7.78475078e-05 2.41258778e-04 9.84849830e-05 2.35678570e-04
  3.32127645e-04 1.00749324e-03 6.46504632e-04 2.92804907e-04
  2.07688217e-03 3.06468690e-04 1.10703858e-03 1.55873218e-04
  1.86229809e-04 4.36430593e-04 2.76993145e-04 1.59722127e-04
  1.35219679e-03 6.57003198e-04 1.75549474e-04 6.41786773e-03
  2.68054835e-04 1.03856080e-04 6.13962393e-03 2.73386860e-04
  1.19765391e-04 2.02623523e-05 3.98138727e-05 8.75462603e-04
  2.26743286e-03 5.55606857e-05 7.67757447e-05 2.45379983e-04
  2.39636895e-04 1.74406357e-03 4.84137703e-03 4.25691978e-04
  5.48296375e-04 4.94175940e-04 1.92641187e-03 1.33322191e-03
  1.04052830e-03 1.13742508e-03 9.70657129e-06 1.37450234e-05
  6.84085302e-04 4.05175961e-04 1.51868837e-04 1.11698202e-04
  4.21312725e-04 1.06983134e-04 1.68630872e-02 2.17026402e-03
  7.57483485e-06 6.46886649e-04 2.71953642e-04 1.10812107e-04
  4.97923640e-04 1.28464575e-03 1.43634598e-03 3.71575676e-04
  6.27603556e-04 7.73156527e-04 1.52841315e-03 4.48646315e-04
  1.15438853e-03 5.46816224e-03 3.15774754e-02 2.39185974e-04
  6.99269236e-04 2.00601498e-05 8.26263276e-05 5.90922027e-05
  2.94850499e-04 1.30161527e-04 8.88686220e-04 3.41826322e-04
  1.18932287e-04 7.45462812e-03 8.10118101e-04 3.68696725e-04
  4.19814867e-04 4.51929722e-04 5.34424384e-04 2.21790295e-04
  6.40917046e-04 2.26881282e-04 9.94290385e-05 8.95796766e-05
  7.72724219e-04 2.99938838e-03 5.30822261e-04 6.44440297e-04
  2.30277496e-04 7.35304260e-04 5.35983418e-04 1.42428646e-04
  5.90305572e-05 6.82274404e-05 6.84773026e-04 1.61576224e-03
  4.29270149e-04 1.52981593e-05 8.89992734e-05 5.46315045e-04
  5.64136368e-04 7.37351598e-04 1.35700963e-03 7.58672832e-05
  1.25546154e-04 6.03623921e-03 5.99305386e-05 2.60081215e-05
  2.74846061e-05 8.56234401e-04 2.61111592e-04 9.41960793e-03
  1.04565377e-04 2.86134920e-04 1.41104462e-03 1.59197208e-03
  3.91929527e-04 5.92352124e-04 4.25073551e-04 2.97010456e-06
  6.15582248e-05 1.09722186e-03 6.06642243e-05 2.57165084e-04
  8.88398150e-04 1.03374710e-03 6.07958937e-05 1.94367909e-04
  4.64805169e-03 2.02853800e-04 1.53955759e-03 8.87976668e-04
  2.60080735e-04 2.48873024e-04 5.41210501e-03 3.10768746e-03
  4.57033457e-05 2.28728546e-04 2.67366268e-04 1.35747681e-03
  4.14371258e-04 1.33966396e-05 3.26812500e-04 1.52846478e-04
  1.16835663e-03 4.42585879e-04 3.81742325e-03 9.88361280e-05
  5.79083189e-06 7.81089417e-04 3.87249835e-04 8.57893261e-04
  3.10963573e-04 4.03491838e-04 6.30583163e-05 3.13827972e-04
  6.21169049e-04 5.37330692e-04 5.25736141e-05 7.85639102e-04
  1.68047857e-03 9.79416538e-04 5.75157865e-05 3.74279870e-03
  2.02113821e-04 3.52919626e-04 9.46599394e-05 1.67617385e-04
  9.92385787e-04 2.60592252e-03 3.56876670e-04 7.57072296e-04
  1.84440159e-03 6.35018514e-04 2.65552895e-04 3.71526257e-05
  1.09977809e-04 6.71566057e-04 3.51881637e-04 2.89788004e-04
  5.18017709e-02 4.28830281e-05 1.07454825e-02 1.70605257e-03
  3.76369135e-05 1.11843937e-03 4.69361228e-04 1.44252519e-03
  2.88582291e-03 6.55640542e-05 2.83936359e-04 1.38373987e-03
  7.24011799e-04 3.48992005e-04 1.00488178e-02 4.67368227e-04
  4.65536141e-05 3.67340908e-05 1.94038951e-03 8.59289285e-05
  6.09966111e-04 9.11463751e-04 6.35046232e-03 1.57006420e-04
  2.45254133e-02 8.80473352e-04 1.93226824e-05 2.22085640e-04
  1.72480009e-03 1.55682836e-04 4.07697720e-04 1.90467210e-04
  5.80542069e-03 2.49434263e-04 5.06766606e-04 7.21944030e-04
  8.28357693e-03 8.76769700e-05 6.80675323e-04 9.07406502e-04
  8.52298253e-05 5.22519578e-04 2.12425319e-03 2.22557705e-04
  1.99131924e-03 8.87042261e-05 1.72320098e-04 2.79577938e-03
  3.01045628e-04 6.53486291e-04 2.80088652e-03 7.63292774e-05
  4.90951388e-05 1.12935188e-04 7.77167734e-04 3.62634251e-04
  9.81125864e-04 2.13341744e-04 3.72508657e-04 1.36531389e-03
  5.23623021e-04 3.56627535e-03 6.34169119e-05 1.25849605e-04
  7.69669059e-05 5.06922719e-04 1.51041814e-03 2.61978112e-05
  3.30019175e-05 1.42385333e-03 2.82381196e-04 6.54000649e-03
  2.56823550e-04 3.39313556e-04 8.62822868e-04 1.48571597e-03
  1.56467329e-04 2.25720089e-03 1.25983803e-04 7.36326983e-05
  1.61678487e-04 1.33298163e-03 2.65089748e-05 1.04593411e-04
  7.93354993e-04 1.19629774e-04 7.48789607e-05 2.68438976e-04
  7.47573096e-04 5.96997095e-04 4.86588018e-04 9.04088782e-04
  2.81988573e-03 8.77936181e-05 1.08407025e-04 1.42921961e-03
  6.09870651e-04 4.45628721e-05 3.24772591e-05 3.64074571e-04
  4.37800598e-04 8.69954180e-04 1.55153190e-04 8.14863713e-04
  1.02042069e-03 3.20525760e-05 1.57347124e-03 1.26264777e-05
  7.31976412e-04 6.03743756e-05 2.01249655e-04 2.75714265e-04
  6.57542900e-04 2.49255309e-03 1.97706814e-03 5.86561160e-04
  2.02196155e-04 3.70929542e-04 2.85406022e-05 4.52920125e-04
  1.03132520e-03 2.74850263e-05 2.43814880e-04 1.11444888e-03
  1.12431888e-04 1.13135262e-03 1.15916140e-04 2.87329089e-02
  1.76778150e-04 3.93280963e-04 1.43338321e-03 2.82934470e-05
  9.44832631e-04 1.63906754e-03 2.45698204e-04 2.23137380e-04
  2.63558090e-04 1.25750928e-04 6.22056541e-04 4.78920381e-04
  1.32605876e-03 3.01359245e-03 6.53096300e-04 3.99024400e-04
  3.26102978e-04 9.00627521e-04 8.06057244e-04 2.02243871e-04
  4.00890334e-04 1.81124953e-04 1.78352158e-04 8.45586183e-04
  3.25956353e-04 1.52549916e-03 4.97987457e-05 2.87762564e-03
  1.73742976e-03 3.76526383e-04 2.13080548e-05 3.53692914e-04
  9.96394432e-04 1.44601776e-03 8.42912297e-04 6.62962673e-04
  1.49790570e-03 9.62663180e-05 1.95810082e-03 1.20371010e-03
  6.36365556e-04 1.06470461e-03 1.21606179e-04 8.24449817e-05
  2.47987191e-04 1.46598832e-04 1.18561485e-03 2.80536828e-03
  2.60173576e-03 2.08780332e-03 1.44621430e-04 8.48826458e-05
  5.70356031e-04 3.53807089e-04 2.67802010e-04 8.17486434e-04
  1.31762656e-03 1.60666578e-03 6.16540527e-03 2.35343599e-04
  5.01670223e-03 9.16053134e-04 7.95085652e-05 1.88316350e-04
  7.60215844e-05 6.06506874e-05 1.39524127e-04 6.35850229e-05
  5.90283656e-04 8.60870408e-04 6.71791704e-03 1.11208043e-04
  4.18868265e-04 5.91199132e-05 2.60028264e-05 1.65655813e-03
  2.80908745e-04 5.28658740e-04 1.85542071e-04 5.24901203e-04
  9.20246879e-04 4.43668796e-05 1.43834983e-03 3.71529080e-04
  8.87177230e-05 2.78014286e-05 1.97662550e-04 7.04515784e-04
  7.65811792e-03 1.87781377e-04 3.28295835e-04 5.67307347e-04
  8.57837673e-04 3.97407712e-05 2.19867521e-04 2.22542963e-04
  3.11426120e-05 8.35283834e-04 7.62182986e-03 4.55592177e-04
  1.24145462e-03 3.65052931e-03 1.84365228e-04 1.36128700e-04
  5.73686732e-04 6.43402804e-03 6.90698449e-04 1.81793192e-04
  2.47405958e-03 6.00388041e-03 2.37462024e-04 9.94627499e-06
  1.69678917e-03 2.53389589e-04 4.16327006e-04 2.32874649e-03
  2.92362389e-03 3.08481860e-04 2.73682672e-04 3.93924012e-04
  1.47822329e-05 7.93044819e-05 2.54629762e-04 5.08833502e-04
  3.15321486e-05 2.47933029e-04 1.00449746e-04 2.22523988e-04
  5.04579584e-05 4.92191357e-05 3.43584310e-04 5.34074607e-05
  5.51626028e-04 2.20674975e-03 9.77355638e-04 5.61044610e-04
  1.04495604e-03 4.63530392e-04 6.78419383e-06 1.25133514e-03
  1.02616279e-04 1.44526639e-04 3.93154405e-05 2.65327422e-03
  2.11634487e-03 1.10849411e-04 7.43030920e-04 4.32052882e-04
  2.35537707e-04 1.56608820e-02 2.62868183e-04 4.32919660e-05
  7.23024714e-05 9.16487421e-04 1.12014277e-04 6.83243634e-05
  4.93017258e-04 3.80433444e-03 1.00709335e-03 6.97260766e-05
  1.36364985e-03 2.95375939e-04 2.55971705e-03 1.33765076e-04
  1.89929575e-04 8.09506455e-05 5.31338097e-04 2.14456348e-03
  1.61709003e-02 3.78746656e-04 1.46292383e-03 3.25094094e-04
  3.34115583e-04 2.88022216e-04 7.13959569e-04 2.28761826e-02
  2.91129545e-04 1.47725295e-04 2.03237196e-05 3.87523942e-05
  5.05941804e-04 3.10883392e-04 5.13278123e-04 3.18380538e-04
  4.33231296e-04 1.06873515e-04 1.21428915e-04 2.42148199e-05
  2.18318946e-05 7.48166131e-05 1.01339592e-05 3.28994101e-05
  2.66467687e-05 1.00452628e-03 3.18647690e-05 3.77978977e-05
  2.75443017e-04 8.91608124e-06 4.36693917e-05 7.44910722e-05
  4.41412340e-05 1.33021711e-03 9.73020069e-05 6.00977801e-05
  3.54578915e-05 1.37240258e-05 8.20743389e-06 9.18467049e-06
  1.12891094e-05 8.08276818e-05 2.89083036e-05 1.53120767e-04
  9.67594751e-05 2.99885694e-04 3.55594821e-04 8.03969044e-04
  1.19955192e-04 1.54474474e-04 3.73775663e-04 1.99282099e-03
  2.44747771e-05 1.84494158e-04 1.36016973e-03 2.49043980e-04
  8.51248296e-06 4.19001917e-05 1.67168710e-05 3.57429642e-04
  6.45207410e-06 6.00043950e-06 5.70542936e-04 2.07756355e-04
  6.03174907e-04 4.65207704e-05 1.40467237e-04 2.25819601e-03
  2.09763748e-04 9.32108611e-04 9.88614163e-04 3.93133028e-04
  7.12807465e-04 7.84759596e-03 9.39904072e-04 1.51060798e-04
  1.19100837e-03 4.11277782e-04 3.11490119e-04 7.71383406e-04
  2.73074664e-04 7.78528163e-04 1.33158464e-04 9.72103793e-04
  5.01735776e-04 2.45849718e-04 2.68233853e-04 8.52815749e-04
  4.43323624e-05 4.93565785e-05 7.29458907e-06 3.31578558e-05
  5.01199174e-05 1.93775559e-04 5.53262944e-04 5.70519245e-04]]
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/testing/test_meta_schedule.py", line 111, in <module>
    assert np.allclose(actual_output, expected_output, rtol=1e-4, atol=2e-4)
AssertionError
