nohup: ignoring input
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_transpose"
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_transpose: T.Buffer[(1, 3, 224, 224), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax2, ax3, ax1])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
    

[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_transpose: T.Buffer[(1, 3, 224, 224), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
                with T.block("T_transpose"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax2, ax3, ax1])
                    T.writes(T_transpose[ax0, ax1, ax2, ax3])
                    T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_clip"
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 225, 225], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 225, 225):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(50175):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, ax0_ax1_ax2_ax3_fused % 50175 // 223)
                                    v3 = T.axis.spatial(225, i6_0 + ax0_ax1_ax2_ax3_fused % 223)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(24):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 28, 1, 3, 1, 1, 4, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 2 * 56 + i3_3 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 56):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 8 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                                v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 2 * 56 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 56, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 28, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_conv2d_add_clip_1"
[14:38:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 114, 114], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 114, 114):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 112, 112, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(211584):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused % 211584 // 6612)
                                    v2 = T.axis.spatial(114, ax0_ax1_ax2_ax3_fused % 6612 // 58)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused * 56 + ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 28, 1, 1, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 112 * 8 + i1_3 * 4 + i1_4)
                                    i = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 28 + i2_3)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 28, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 112 * 8 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 28 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 28, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 56, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add"
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 112, 112, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(200704):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 6272)
                                    v2 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 6272 // 56)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax0_ax1_ax2_ax3_fused % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(256):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 112, 1, 8, 1, 1, 1, 1, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                    yy = T.axis.spatial(112, i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4)
                                    rc = T.axis.reduce(32, i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 112, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(112, ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[3, 1, 2, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 112, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 8, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 4, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_clip_2"
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 144, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 144, 112, 112, 24, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 144, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(75264):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 6 + ax0_ax1_ax2_ax3_fused // 12544)
                                    v2 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 12544 // 112)
                                    v3 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(432):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 72 + ax0_ax1_ax2_ax3_fused // 6)
                                    v1 = T.axis.spatial(24, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 7, 1, 6, 1, 1, 1, 9, 2, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 72 + i0_2_i1_2_i2_2_i3_2_fused // 64 * 36 + i1_3 * 9 + i1_4)
                                    yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 64 // 8 * 14 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_4)
                                    rc = T.axis.reduce(24, i4_0 * 6 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 36, 14, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 72 + i0_2_i1_2_i2_2_i3_2_fused // 64 * 36 + ax1)
                                v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 64 // 8 * 14 + ax2)
                                v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 4, 9])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 7, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_clip_3"
[14:38:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 113, 113], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 113, 113):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 144, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 112, 112], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 144, 113, 113], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(244080):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused % 244080 // 1695)
                                    v2 = T.axis.spatial(113, ax0_ax1_ax2_ax3_fused % 1695 // 15)
                                    v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1296):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 1, 3, 1, 1, 18, 7, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 14 * 18 + i1_4)
                                    i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + i2_3 * 7 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 112, 112], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 18, 14, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 14 * 18 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 18])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_conv2d_add_1"
[14:38:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 144, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(36, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6272):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 1568)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax0_ax1_ax2_ax3_fused % 1568 // 56)
                                    v3 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(144, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 28, 1, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(56, i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(144, i4_0 * 4 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 56):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(56, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 16, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 28, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[36, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_conv2d_add_clip_4"
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 192, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 192, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(34560):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused % 34560 // 180)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ax0_ax1_ax2_ax3_fused % 180 // 30)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1728):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 24, 1, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 24, 1, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 1, 1, 24])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_conv2d_add_add"
[14:38:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 56, 56), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 56, 56), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1176):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 3 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(24):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(192, i4_0 * 3 + ax0_ax1_ax2_ax3_fused % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i2_3)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i3_4)
                                    rc = T.axis.reduce(192, i4_0 * 3 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_clip_5"
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 192, 56, 56, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 56, 56], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1568, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 4 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 14)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4)
                                    rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 56, 56], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 4 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 14 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 16, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_conv2d_add_clip_6"
[14:38:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 192, 59, 59], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 59, 59):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 192, 28, 28, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 5, 5], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 192, 59, 59], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(42, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(5, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12320):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ax0_ax1_ax2_ax3_fused % 12320 // 385)
                                    v2 = T.axis.spatial(59, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i4_0 + ax0_ax1_ax2_ax3_fused % 385 // 55)
                                    v3 = T.axis.spatial(59, i5_0 + ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(32):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 8, 2, 7, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 5, 5], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused * 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 4, 1, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_2"
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(56, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1), "float32"], T_add: T.Buffer[(1, 56, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 56, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 56, 28, 28, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [56, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 56, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(56, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1), "float32"], T_add: T.Buffer[(1, 56, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 56, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([56, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1176):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 6 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + ax0_ax1_ax2_ax3_fused % 196 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(336):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused // 6)
                                    v1 = T.axis.spatial(192, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 14, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 14 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_4)
                                    rc = T.axis.reduce(192, i4_0 * 6 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [56, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 14, 1, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 14 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 14])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 3, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_conv2d_add_clip_7"
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 336, 32, 32], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 336, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 336, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 336, 32, 32):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 30 and 2 <= i3_1 and i3_1 < 30, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 336, 28, 28, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [336, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 336, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 336, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 336, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 336, 32, 32], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([336, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(5, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(10752):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ax0_ax1_ax2_ax3_fused % 10752 // 224)
                                    v2 = T.axis.spatial(32, i4_0 + ax0_ax1_ax2_ax3_fused % 224 // 8)
                                    v3 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 30 and 2 <= v3 and v3 < 30, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(240):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ax0_ax1_ax2_ax3_fused // 5)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(5, i4_0)
                                    v3 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 5, 1, 12, 1, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + i1_4)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [336, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 12])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_add_1"
[14:38:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(56, 336, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 56, 28, 28), "float32"], T_add: T.Buffer[(1, 56, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 336, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 56, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 56, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 336, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 56, 28, 28, 336, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [56, 336, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 56, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 56, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(56, 336, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 56, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 56, 28, 28), "float32"], T_add: T.Buffer[(1, 56, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 56, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 336, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([56, 336, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(28, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4704):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(336, i4_0 * 12 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax0_ax1_ax2_ax3_fused % 392 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(672):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused // 12)
                                    v1 = T.axis.spatial(336, i4_0 * 12 + ax0_ax1_ax2_ax3_fused % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 2, 7, 2, 2, 1, 1, 1, 4, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 7 + i2_3)
                                    xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                    rc = T.axis.reduce(336, i4_0 * 12 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [56, 336, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[28, 6, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_conv2d_add_clip_8"
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 56, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 56, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 336, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 336, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 56, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 336, 28, 28, 56, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 56, 28, 28], "float32"], ["TENSOR", [336, 56, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 336, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 336, 28, 28):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 56, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 56, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 336, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 56, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([336, 56, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(56, i4_0 * 7 + ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(147):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 21 + ax0_ax1_ax2_ax3_fused // 7)
                                    v1 = T.axis.spatial(56, i4_0 * 7 + ax0_ax1_ax2_ax3_fused % 7)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(7, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 21 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(56, i4_0 * 7 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 56, 28, 28], "float32"], ["TENSOR", [336, 56, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused // 7 * 21 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 21, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 7, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_conv2d_add_clip_9"
[14:38:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 336, 29, 29], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 336, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 336, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 336, 29, 29):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 28 and 0 <= i3_1 and i3_1 < 28, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 336, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [336, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 336, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 336, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 28, 28), "float32"], placeholder_1: T.Buffer[(336, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 336, 1, 1), "float32"], compute: T.Buffer[(1, 336, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 336, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 336, 29, 29], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([336, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(141288):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused * 168 + ax0_ax1_ax2_ax3_fused // 841)
                                    v2 = T.axis.spatial(29, ax0_ax1_ax2_ax3_fused % 841 // 29)
                                    v3 = T.axis.spatial(29, ax0_ax1_ax2_ax3_fused % 29)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 28 and 0 <= v3 and v3 < 28, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused * 168 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 7, 3, 1, 1, 1, 7, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused * 168 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3)
                                    i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_4)
                                    j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 336, 28, 28], "float32"], ["TENSOR", [336, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(336, i0_0_i1_0_i2_0_i3_0_fused * 168 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 3, 14, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_conv2d_add_3"
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 14, 14), "float32"], placeholder_1: T.Buffer[(112, 336, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 336, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 112, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 336, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 112, 14, 14, 336, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 336, 14, 14], "float32"], ["TENSOR", [112, 336, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 112, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 336, 14, 14), "float32"], placeholder_1: T.Buffer[(112, 336, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], T_add: T.Buffer[(1, 112, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 112, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 336, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([112, 336, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(32928):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(336, i4_0 * 168 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(9408):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + ax0_ax1_ax2_ax3_fused // 168)
                                    v1 = T.axis.spatial(336, i4_0 * 168 + ax0_ax1_ax2_ax3_fused % 168)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(21, 1, 1, 1, 7, 2, 1, 8, 1, 1, 1, 8, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i1_3 * 8 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 2 + i2_3)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4)
                                    rc = T.axis.reduce(336, i4_0 * 168 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 336, 14, 14], "float32"], ["TENSOR", [112, 336, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 56, 2, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 21, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_clip_10"
[14:38:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 672, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 672, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [672, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 672, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([672, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(28224):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 2 * 224 + ax0_ax1_ax2_ax3_fused % 28224 // 126)
                                    v2 = T.axis.spatial(16, i4_0 + ax0_ax1_ax2_ax3_fused % 126 // 9)
                                    v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(672):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 2 * 224 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 7, 1, 1, 1, 1, 2, 1, 7):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 2 * 224 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_3)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [672, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 2 * 224 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 56, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_conv2d_add_add_2"
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(112, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 14, 14), "float32"], T_add: T.Buffer[(1, 112, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 112, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 112, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 112, 14, 14, 672, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [112, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 112, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 112, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(112, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 112, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 112, 14, 14), "float32"], T_add: T.Buffer[(1, 112, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 112, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([112, 672, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, i4_0 * 224 + ax0_ax1_ax2_ax3_fused // 14)
                                    v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + ax0_ax1_ax2_ax3_fused // 224)
                                    v1 = T.axis.spatial(672, i4_0 * 224 + ax0_ax1_ax2_ax3_fused % 224)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(14, 1, 1, 1, 8, 1, 1, 16, 1, 1, 1, 1, 1, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3)
                                    yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                    xx = T.axis.spatial(14, i3_4)
                                    rc = T.axis.reduce(672, i4_0 * 224 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [112, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                                v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax2)
                                v3 = T.axis.spatial(14, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 7, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 14, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_conv2d_add_clip_11"
[14:38:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 112, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 112, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 112, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 672, 14, 14, 112, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 112, 14, 14], "float32"], ["TENSOR", [672, 112, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 112, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 112, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 112, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([672, 112, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(336, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(7, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(448):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(112, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(224):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 7 * 14 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(112, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 7, 1, 1, 8, 1, 1, 1, 2, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 7 * 14 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    rc = T.axis.reduce(112, i4_0 * 16 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 112, 14, 14], "float32"], ["TENSOR", [672, 112, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 14, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(672, i0_0_i1_0_i2_0_i3_0_fused // 7 * 14 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[48, 1, 1, 7, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[7, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_conv2d_add_clip_12"
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 672, 18, 18], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 18, 18):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 16 and 2 <= i3_1 and i3_1 < 16, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 672, 14, 14, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [672, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(672, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 672, 1, 1), "float32"], compute: T.Buffer[(1, 672, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 672, 18, 18], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([672, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(5, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9408):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, ax0_ax1_ax2_ax3_fused % 9408 // 14)
                                    v2 = T.axis.spatial(18, i4_0 + ax0_ax1_ax2_ax3_fused % 14)
                                    v3 = T.axis.spatial(18, i0_0_i1_0_i2_0_i3_0_fused + i5_0 + 0)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 16 and 2 <= v3 and v3 < 16, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(672):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(672, ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 7, 7, 1, 1, 1, 1, 48, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(672, i0_1_i1_1_i2_1_i3_1_fused // 2 * 336 + i1_3 * 48 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    j, di, dj = T.axis.remap("SRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_0, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [672, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 336, 7, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(672, i0_1_i1_1_i2_1_i3_1_fused // 2 * 336 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 48])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_4"
[14:38:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(160, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_add: T.Buffer[(1, 160, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 672, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 160, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 672, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 160, 14, 14, 672, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [160, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 160, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 672, 14, 14), "float32"], placeholder_1: T.Buffer[(160, 672, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_add: T.Buffer[(1, 160, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 160, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 672, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([160, 672, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(5, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(32928):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(672, i4_0 * 168 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(5376):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 168)
                                    v1 = T.axis.spatial(672, i4_0 * 168 + ax0_ax1_ax2_ax3_fused % 168)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(56, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 16, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(672, i4_0 * 168 + i4_1 * 3 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 672, 14, 14], "float32"], ["TENSOR", [160, 672, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[5, 1, 2, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 56, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_conv2d_add_clip_13"
[14:38:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 960, 18, 18], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 18, 18):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 16 and 2 <= i3_1 and i3_1 < 16, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 960, 14, 14, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [960, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 960, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 960, 18, 18], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([960, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(15120):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 60 + ax0_ax1_ax2_ax3_fused % 15120 // 252)
                                    v2 = T.axis.spatial(18, ax0_ax1_ax2_ax3_fused % 252 // 14)
                                    v3 = T.axis.spatial(18, i5_0 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 16 and 2 <= v3 and v3 < 16, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(300):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 60 + ax0_ax1_ax2_ax3_fused // 5)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    v3 = T.axis.spatial(5, i5_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 10, 2, 2, 5, 1, 1, 6, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 60 + i1_3 * 6 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [960, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 60, 2, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 60 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 1, 10, 6])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_conv2d_add_add_3"
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 14, 14), "float32"], T_add: T.Buffer[(1, 160, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 160, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 160, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 160, 14, 14, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 160, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 14, 14), "float32"], T_add: T.Buffer[(1, 160, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 160, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([160, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(5, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(37632):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 192 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(30720):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(160, ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(960, i4_0 * 192 + ax0_ax1_ax2_ax3_fused % 192)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 16, 1, 1, 32, 1, 1, 1, 10, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(160, i1_3 * 10 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    rc = T.axis.reduce(960, i4_0 * 192 + i4_1 * 32 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 160, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 16, 10])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 6, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_conv2d_add_clip_14"
[14:38:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 160, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 960, 14, 14, 160, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 14, 14], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 14, 14):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 960, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 160, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1120):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 80 + ax0_ax1_ax2_ax3_fused // 14)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(76800):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, ax0_ax1_ax2_ax3_fused // 80)
                                    v1 = T.axis.spatial(160, i4_0 * 80 + ax0_ax1_ax2_ax3_fused % 80)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 3, 1, 1, 20, 1, 1, 1, 10, 2, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(960, i0_2_i1_2_i2_2_i3_2_fused * 30 + i1_3 * 10 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused)
                                    rc = T.axis.reduce(160, i4_0 * 80 + i4_1 * 20 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 14, 14], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 30, 2, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(960, i0_2_i1_2_i2_2_i3_2_fused * 30 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 3, 10])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 20])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:38:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_conv2d_add_clip_15"
[14:38:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 960, 17, 17], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 17, 17):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 960, 7, 7, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [960, 1, 5, 5], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 14, 14), "float32"], placeholder_1: T.Buffer[(960, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], compute: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 960, 17, 17], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([960, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(280, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(46240):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + ax0_ax1_ax2_ax3_fused // 289)
                                    v2 = T.axis.spatial(17, ax0_ax1_ax2_ax3_fused % 289 // 17)
                                    v3 = T.axis.spatial(17, ax0_ax1_ax2_ax3_fused % 17)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(4000):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + ax0_ax1_ax2_ax3_fused // 25)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 25 // 5)
                                    v3 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(5, 1, 1, 1, 1, 7, 1, 5, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i1_4)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_3, i4_1, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 14, 14], "float32"], ["TENSOR", [960, 1, 5, 5], "float32"], [2, 2], [1, 1, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 40, 1, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_conv2d_add_5"
[14:38:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 272, 7, 7, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [272, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(34, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(23520):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 480 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(32640):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + ax0_ax1_ax2_ax3_fused // 480)
                                    v1 = T.axis.spatial(960, i4_0 * 480 + ax0_ax1_ax2_ax3_fused % 480)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(48, 1, 1, 1, 2, 7, 7, 10, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                    yy, xx = T.axis.remap("SS", [i2_3, i3_3])
                                    rc = T.axis.reduce(960, i4_0 * 480 + i4_1 * 10 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [272, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_0_i1_0_i2_0_i3_0_fused * 68 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 34, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 48, 10])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_conv2d_add_clip_16"
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 1632, 11, 11], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1632, 11, 11):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 9 and 2 <= i3_1 and i3_1 < 9, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1632, 7, 7, 5, 5):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [1632, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 1, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 1632, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 1632, 11, 11], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1632, 1, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(238, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(5, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1680):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ax0_ax1_ax2_ax3_fused % 1680 // 35)
                                    v2 = T.axis.spatial(11, i4_0 + ax0_ax1_ax2_ax3_fused % 35 // 5)
                                    v3 = T.axis.spatial(11, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 9 and 2 <= v3 and v3 < 9, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(240):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ax0_ax1_ax2_ax3_fused // 5)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(5, i4_0)
                                    v3 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 5, 1, 8, 7, 1, 1, 1, 1, 3, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused * 24 + i1_3 * 3 + i1_4)
                                    i = T.axis.spatial(7, i2_3)
                                    j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [1632, 1, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 24, 7, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused * 24 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[34, 1, 2, 8, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 5, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_conv2d_add_add_4"
[14:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 1632, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 7, 7), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 272, 7, 7, 1632, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [272, 1632, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[14:39:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(272, 1632, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 272, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 272, 7, 7), "float32"], T_add: T.Buffer[(1, 272, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1632, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([272, 1632, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3332):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1632, i4_0 * 68 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(18496):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 68)
                                    v1 = T.axis.spatial(1632, i4_0 * 68 + ax0_ax1_ax2_ax3_fused % 68)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 17, 1, 7, 34, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 34 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_3)
                                    rc = T.axis.reduce(1632, i4_0 * 68 + i4_1 * 34 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [272, 1632, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 34, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(272, i0_1_i1_1_i2_1_i3_1_fused // 7 * 34 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 1, 17, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 2, 34])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_conv2d_add_clip_17"
[14:39:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 272, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 272, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 272, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1632, 7, 7, 272, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [1632, 272, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:39:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 272, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 272, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 1632, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 272, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1632, 272, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(136, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(13328):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(443904):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1632, ax0_ax1_ax2_ax3_fused // 272)
                                    v1 = T.axis.spatial(272, ax0_ax1_ax2_ax3_fused % 272)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(136, 1, 1, 1, 6, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1632, i0_1_i1_1_i2_1_i3_1_fused // 49 * 816 + i0_2_i1_2_i2_2_i3_2_fused * 6 + i1_3)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                    xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    rc = T.axis.reduce(272, i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 272, 7, 7], "float32"], ["TENSOR", [1632, 272, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1632, i0_1_i1_1_i2_1_i3_1_fused // 49 * 816 + i0_2_i1_2_i2_2_i3_2_fused * 6 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 136, 6, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 136, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_nn_conv2d_add_clip_18"
[14:39:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 1632, 9, 9], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1632, 9, 9):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1632, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [1632, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(1632, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 1632, 1, 1), "float32"], compute: T.Buffer[(1, 1632, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 1632, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 1632, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1632, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(17, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(21, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6048):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused * 96 + ax0_ax1_ax2_ax3_fused % 6048 // 63)
                                    v2 = T.axis.spatial(9, i4_0 + ax0_ax1_ax2_ax3_fused % 63 // 9)
                                    v3 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused * 96 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 8, 1, 7, 1, 3, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_3, i4_0, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [1632, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1632, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[17, 3, 2, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_conv2d_add_6"
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(448, 1632, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], T_add: T.Buffer[(1, 448, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1632, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 448, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1632, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 448, 7, 7, 1632, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [448, 1632, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 448, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1632, 7, 7), "float32"], placeholder_1: T.Buffer[(448, 1632, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 448, 1, 1), "float32"], T_add: T.Buffer[(1, 448, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 448, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1632, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([448, 1632, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(5712):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1632, i4_0 * 816 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(91392):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(448, i0_0_i1_0_i2_0_i3_0_fused // 7 * 112 + ax0_ax1_ax2_ax3_fused // 816)
                                    v1 = T.axis.spatial(1632, i4_0 * 816 + ax0_ax1_ax2_ax3_fused % 816)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(24, 1, 1, 1, 2, 1, 1, 34, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(448, i0_0_i1_0_i2_0_i3_0_fused // 7 * 112 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(1632, i4_0 * 816 + i4_1 * 34 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1632, 7, 7], "float32"], ["TENSOR", [448, 1632, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(448, i0_0_i1_0_i2_0_i3_0_fused // 7 * 112 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 28, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 24, 34])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_conv2d_add_clip_19"
[14:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 7, 7), "float32"], placeholder_1: T.Buffer[(1280, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1280, 1, 1), "float32"], compute: T.Buffer[(1, 1280, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 448, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1280, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1280, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 448, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1280, 7, 7, 448, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 7, 7], "float32"], ["TENSOR", [1280, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1280, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1280, 7, 7):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 448, 7, 7), "float32"], placeholder_1: T.Buffer[(1280, 448, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1280, 1, 1), "float32"], compute: T.Buffer[(1, 1280, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 1280, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 448, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1280, 448, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(28, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(448, i4_0 * 16 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(20480):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1280, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(448, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 16, 1, 1, 4, 1, 1, 1, 10, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1280, i0_1_i1_1_i2_1_i3_1_fused * 320 + i0_2_i1_2_i2_2_i3_2_fused * 160 + i1_3 * 10 + i1_4)
                                    yy = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(448, i4_0 * 16 + i4_1 * 4 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 448, 7, 7], "float32"], ["TENSOR", [1280, 448, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 160, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1280, i0_1_i1_1_i2_1_i3_1_fused * 320 + i0_2_i1_2_i2_2_i3_2_fused * 160 + ax1)
                                v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused // 7 + ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(compute[v0, v1, v2, v3])
                                compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 2, 16, 10])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[28, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_nn_avg_pool2d"
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 1280, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1280, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 1280, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 2 design space(s) generated
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            tensor_shared = T.alloc_buffer([1, 1280, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 1280, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 13):
                    for ax4_ax5_fused_1 in T.thread_binding(4, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(1280, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(7, (ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1) // 7)
                            rv1 = T.axis.reduce(7, (ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1) % 7)
                            T.where(ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1 < 49)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 + rv0, ax3_1 + rv1]
                for i3_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1280, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            tensor_1 = T.alloc_buffer([1, 1280, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 1280, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 + rv0, ax3 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 1280, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] / T.cast(T.max((T.min(ax2 + 6, 6) + 1 - T.max(0, ax2)) * (T.min(ax3 + 6, 6) + 1 - T.max(0, ax3)), 1), "float32")
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #34: "fused_squeeze"
[14:39:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 1, 1), "float32"], T_squeeze: T.Buffer[(1, 1280), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 1280):
            with T.block("T_squeeze"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[ax0, ax1, 0, 0])
                T.writes(T_squeeze[ax0, ax1])
                T_squeeze[ax0, ax1] = placeholder[ax0, ax1, 0, 0]
    

[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 1, 1), "float32"], T_squeeze: T.Buffer[(1, 1280), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1 in T.grid(1, 1280):
                with T.block("T_squeeze"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[ax0, ax1, 0, 0])
                    T.writes(T_squeeze[ax0, ax1])
                    T_squeeze[ax0, ax1] = placeholder[ax0, ax1, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #35: "fused_nn_dense_add"
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 1000], dtype="float32")
        for i0, i1, i2 in T.grid(1, 1000, 1280):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"workload":["dense_small_batch.gpu", ["TENSOR", [1, 1280], "float32"], ["TENSOR", [1000, 1280], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280), "float32"], placeholder_1: T.Buffer[(1000, 1280), "float32"], placeholder_2: T.Buffer[(1, 1000), "float32"], T_add: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True, "layout_free_placeholders": [1]})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_matmul_NT_local = T.alloc_buffer([1, 1000], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 1280], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([1000, 1280], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(25, thread="threadIdx.x"):
                        for i2_0 in T.serial(8):
                            for ax0_ax1_fused in T.serial(160):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1280, i2_0 * 160 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(160000):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1000, ax0_ax1_fused // 160)
                                    v1 = T.axis.spatial(1280, i2_0 * 160 + ax0_ax1_fused % 160)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(40, 1, 5, 4, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(1000, i0_1_i1_1_fused * 125 + i0_2_i1_2_fused * 5 + i1_3)
                                    k = T.axis.reduce(1280, i2_0 * 160 + i2_1 * 4 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 1280], "float32"], ["TENSOR", [1000, 1280], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 5):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1000, i0_1_i1_1_fused * 125 + i0_2_i1_2_fused * 5 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 8, 25, 5, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[8, 40, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #36: "fused_nn_softmax"
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 1000], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_maxelem"):
                i0_1, k = T.axis.remap("SR", [i0, i1])
                T.reads(placeholder[i0_1, k])
                T.writes(T_softmax_maxelem[i0_1])
                with T.init():
                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_softmax_exp"):
                i0_2, i1_1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[i0_2, i1_1], T_softmax_maxelem[i0_2])
                T.writes(T_softmax_exp[i0_2, i1_1])
                T_softmax_exp[i0_2, i1_1] = T.exp(placeholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype="float32")
        for i0_3, i1 in T.grid(1, 1000):
            with T.block("T_softmax_expsum"):
                i0_4, k = T.axis.remap("SR", [i0_3, i1])
                T.reads(T_softmax_exp[i0_4, k])
                T.writes(T_softmax_expsum[i0_4])
                with T.init():
                    T_softmax_expsum[i0_4] = T.float32(0)
                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]
        for i0_5, i1 in T.grid(1, 1000):
            with T.block("T_softmax_norm"):
                i0_6, i1_2 = T.axis.remap("SS", [i0_5, i1])
                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])
                T.writes(T_softmax_norm[i0_6, i1_2])
                T.block_attr({"axis":1})
                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]
    

[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 4 design space(s) generated
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_softmax_maxelem_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 8):
                    for ax1_1 in T.thread_binding(128, thread="threadIdx.x"):
                        with T.block("T_softmax_maxelem"):
                            i0_1 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 128 + ax1_1)
                            T.where(ax1_0 * 128 + ax1_1 < 1000)
                            T.reads(placeholder[i0_1, k])
                            T.writes(T_softmax_maxelem_shared[i0_1])
                            with T.init():
                                T_softmax_maxelem_shared[i0_1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[i0_1] = T.max(T_softmax_maxelem_shared[i0_1], placeholder[i0_1, k])
                for ax0, ax1_0 in T.grid(1, 8):
                    for ax1_1 in T.thread_binding(128, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 128 + ax1_1)
                            T.where(ax1_0 * 128 + ax1_1 < 1000)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem_shared[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem_shared[i0_2], dtype="float32")
                for i1_0 in T.serial(8):
                    for i1_1 in T.thread_binding(128, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1000, i1_0 * 128 + i1_1)
                            T.where(i1_0 * 128 + i1_1 < 1000)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem_shared[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem_shared[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
b4, = sch.get_consumers(block=b2)
l5, l6 = sch.get_loops(block=b4)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l5, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l10, l11, l12 = sch.get_loops(block=b2)
l13, l14 = sch.split(loop=l12, factors=[None, v7])
sch.bind(loop=l14, thread_axis="threadIdx.x")
b15, b16 = sch.get_consumers(block=b0)
l17, l18, l19, l20 = sch.get_loops(block=b15)
sch.compute_at(block=b0, loop=l17, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l21, l22, l23 = sch.get_loops(block=b0)
l24, l25 = sch.split(loop=l23, factors=[None, v7])
sch.bind(loop=l25, thread_axis="threadIdx.x")
v26 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v26)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum_shared = T.alloc_buffer([1], dtype="float32", scope="shared")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0 in T.serial(1):
                for ax0, ax1_0 in T.grid(1, 8):
                    for ax1_1 in T.thread_binding(128, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            k = T.axis.reduce(1000, ax1_0 * 128 + ax1_1)
                            T.where(ax1_0 * 128 + ax1_1 < 1000)
                            T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                            T.writes(T_softmax_expsum_shared[i0_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2] = T_softmax_expsum_shared[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
                for i1_0 in T.serial(8):
                    for i1_1 in T.thread_binding(128, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1 = T.axis.spatial(1000, i1_0 * 128 + i1_1)
                            T.where(i1_0 * 128 + i1_1 < 1000)
                            T.reads(placeholder[i0_3, i1], T_softmax_maxelem[i0_3], T_softmax_expsum_shared[i0_3])
                            T.writes(T_softmax_norm[i0_3, i1])
                            T.block_attr({"axis":1})
                            T_softmax_norm[i0_3, i1] = T.exp(placeholder[i0_3, i1] - T_softmax_maxelem[i0_3], dtype="float32") / T_softmax_expsum_shared[i0_3]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="T_softmax_expsum", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
b3, = sch.get_consumers(block=b1)
l4, l5 = sch.get_loops(block=b3)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=5)
l7, l8 = sch.split(loop=l5, factors=[None, v6])
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l4, preserve_unit_loops=True)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l9, l10, l11 = sch.get_loops(block=b1)
l12, l13 = sch.split(loop=l11, factors=[None, v6])
sch.bind(loop=l13, thread_axis="threadIdx.x")
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1_0 in T.grid(1, 125):
                for i1_1 in T.thread_binding(8, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0_1 = T.axis.spatial(1, 0)
                        k = T.axis.reduce(1000, i1_0 * 8 + i1_1)
                        T.reads(placeholder[i0_1, k])
                        T.writes(T_softmax_maxelem[i0_1])
                        with T.init():
                            T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_4, i1_2 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_2], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_2])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_2] = T.exp(placeholder[i0_4, i1_2] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=1)
l4, l5 = sch.get_loops(block=b0)
l6, l7 = sch.split(loop=l5, factors=[None, v3])
sch.bind(loop=l7, thread_axis="threadIdx.x")
v8 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v8)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000), "float32"], T_softmax_norm: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_softmax_maxelem = T.alloc_buffer([1], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1], dtype="float32")
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_maxelem"):
                    i0_1, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_1, k])
                    T.writes(T_softmax_maxelem[i0_1])
                    with T.init():
                        T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], placeholder[i0_1, k])
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_softmax_expsum"):
                    i0_2, k = T.axis.remap("SR", [i0, i1])
                    T.reads(placeholder[i0_2, k], T_softmax_maxelem[i0_2])
                    T.writes(T_softmax_expsum[i0_2])
                    with T.init():
                        T_softmax_expsum[i0_2] = T.float32(0)
                    T_softmax_expsum[i0_2] = T_softmax_expsum[i0_2] + T.exp(placeholder[i0_2, k] - T_softmax_maxelem[i0_2], dtype="float32")
            for i0_3, i1 in T.grid(1, 1000):
                with T.block("T_softmax_norm"):
                    i0_4, i1_1 = T.axis.remap("SS", [i0_3, i1])
                    T.reads(placeholder[i0_4, i1_1], T_softmax_maxelem[i0_4], T_softmax_expsum[i0_4])
                    T.writes(T_softmax_norm[i0_4, i1_1])
                    T.block_attr({"axis":1})
                    T_softmax_norm[i0_4, i1_1] = T.exp(placeholder[i0_4, i1_1] - T_softmax_maxelem[i0_4], dtype="float32") / T_softmax_expsum[i0_4]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |         fused_nn_conv2d_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[14:39:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_transpose"
[14:39:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:39:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:39:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03913f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0372b208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037c6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03643f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03817cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03656f38)]: 0 failure(s)
[14:39:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[14:39:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03913f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0372b208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037c6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03643f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03817cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03656f38)]: 0 failure(s)
[14:39:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03913f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0372b208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037c6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03643f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03817cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03656f38)]: 0 failure(s)
[14:39:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03913f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0372b208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037c6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03643f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03817cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03656f38)]: 0 failure(s)
[14:39:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03913f78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0372b208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037c6ab8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03643f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03817cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03656f38)]: 0 failure(s)
[14:39:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.9346  0.7473  0.5748  0.4214  0.2000
[14:39:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[14:39:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[14:39:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[14:39:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[14:39:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_clip"
[14:39:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:39:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:39:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e036f0d48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037be3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0109c578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03826e78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0396fb18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e01082a58)]: 1899 failure(s)
[14:39:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 149 candidate(s)
[14:40:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e036f0d48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037be3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0109c578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03826e78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0396fb18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e01082a58)]: 303 failure(s)
[14:41:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e036f0d48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037be3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0109c578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03826e78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0396fb18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e01082a58)]: 280 failure(s)
[14:42:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e036f0d48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037be3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0109c578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03826e78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0396fb18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e01082a58)]: 268 failure(s)
[14:43:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e036f0d48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037be3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0109c578)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03826e78)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0396fb18)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e01082a58)]: 239 failure(s)
[14:43:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9995  0.9994  0.9993  0.9990  0.9986  0.9981  0.9980  0.9980  0.9978  0.9977  0.9976  0.9975  0.9972  0.9966
[17 : 32]:	0.9965  0.9963  0.9962  0.9962  0.9961  0.9961  0.9958  0.9958  0.9957  0.9956  0.9953  0.9953  0.9951  0.9950  0.9949  0.9949
[14:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:43:28] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:43:28] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:44:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:44:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_clip_1"
[14:44:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:44:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:44:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0388a078)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03915368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037b6018)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03916a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e036fe528)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03950888)]: 1975 failure(s)
[14:44:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 73 candidate(s)
[14:45:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0388a078)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03915368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037b6018)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03916a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e036fe528)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03950888)]: 330 failure(s)
[14:46:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0388a078)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03915368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037b6018)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03916a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e036fe528)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03950888)]: 326 failure(s)
[14:47:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0388a078)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03915368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037b6018)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03916a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e036fe528)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03950888)]: 328 failure(s)
[14:49:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0388a078)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03915368)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037b6018)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03916a88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e036fe528)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03950888)]: 304 failure(s)
[14:49:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9992  0.9991  0.9989  0.9989  0.9988  0.9987  0.9987  0.9987  0.9986  0.9986  0.9985  0.9985  0.9984  0.9982
[17 : 32]:	0.9981  0.9980  0.9980  0.9979  0.9979  0.9976  0.9974  0.9965  0.9964  0.9961  0.9959  0.9959  0.9958  0.9958  0.9957  0.9954
[14:49:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:49:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:49:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:49:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:50:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add"
[14:50:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:50:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:50:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0370d448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0370f468)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e03736488)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03926858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aabb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037b1408)]: 1921 failure(s)
[14:50:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 127 candidate(s)
[14:51:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0370d448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0370f468)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e03736488)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03926858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aabb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037b1408)]: 484 failure(s)
[14:51:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0370d448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0370f468)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e03736488)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03926858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aabb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037b1408)]: 428 failure(s)
[14:52:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0370d448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0370f468)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e03736488)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03926858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aabb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037b1408)]: 447 failure(s)
[14:53:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0370d448)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0370f468)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e03736488)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e03926858)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aabb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037b1408)]: 413 failure(s)
[14:53:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9997  0.9995  0.9995  0.9994  0.9993  0.9993  0.9991  0.9991  0.9989  0.9989  0.9989  0.9988  0.9987  0.9985  0.9981
[17 : 32]:	0.9981  0.9979  0.9979  0.9976  0.9973  0.9971  0.9970  0.9970  0.9969  0.9968  0.9967  0.9964  0.9963  0.9962  0.9959  0.9958
[14:53:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:53:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:53:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:54:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:54:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_clip_2"
[14:54:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:54:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:54:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03703fa8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037c7bc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0375a8d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0376e728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aad38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e0380a4e8)]: 1836 failure(s)
[14:54:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 212 candidate(s)
[14:54:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03703fa8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037c7bc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0375a8d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0376e728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aad38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e0380a4e8)]: 373 failure(s)
[14:55:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03703fa8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037c7bc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0375a8d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0376e728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aad38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e0380a4e8)]: 341 failure(s)
[14:56:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03703fa8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037c7bc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0375a8d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0376e728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aad38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e0380a4e8)]: 357 failure(s)
[14:57:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03703fa8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037c7bc8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0375a8d8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0376e728)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e010aad38)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e0380a4e8)]: 334 failure(s)
[14:57:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9995  0.9994  0.9994  0.9992  0.9988  0.9985  0.9985  0.9984  0.9983  0.9981  0.9979  0.9976  0.9974  0.9969  0.9967
[17 : 32]:	0.9965  0.9963  0.9961  0.9960  0.9956  0.9955  0.9949  0.9949  0.9949  0.9949  0.9948  0.9946  0.9946  0.9946  0.9944  0.9944
[14:57:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[14:57:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[14:57:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[14:58:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[14:58:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_clip_3"
[14:58:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[14:58:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[14:58:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 2033 failure(s)
[14:59:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 4064 failure(s)
[14:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 6096 failure(s)
[15:00:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 8130 failure(s)
[15:00:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 62 candidate(s)
[15:01:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 479 failure(s)
[15:02:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 486 failure(s)
[15:02:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 501 failure(s)
[15:03:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e03918608)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e037222e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0361ace8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e0396d058)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e03910b58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037f5698)]: 447 failure(s)
[15:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9993  0.9992  0.9991  0.9991  0.9990  0.9990  0.9986  0.9986  0.9985  0.9982  0.9973  0.9973  0.9972  0.9972  0.9970
[17 : 32]:	0.9966  0.9966  0.9965  0.9964  0.9961  0.9961  0.9955  0.9955  0.9950  0.9944  0.9940  0.9940  0.9937  0.9937  0.9936  0.9931
[15:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:03:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:03:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:03:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:04:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_1"
[15:04:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:04:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:04:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e038b1498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03967008)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cc548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e02037e28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0380c1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e036c9418)]: 1924 failure(s)
[15:04:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 124 candidate(s)
[15:05:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e038b1498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03967008)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cc548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e02037e28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0380c1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e036c9418)]: 430 failure(s)
[15:05:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e038b1498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03967008)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cc548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e02037e28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0380c1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e036c9418)]: 388 failure(s)
[15:06:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e038b1498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03967008)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cc548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e02037e28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0380c1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e036c9418)]: 439 failure(s)
[15:06:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e038b1498)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e03967008)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cc548)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e02037e28)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0380c1b8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e036c9418)]: 369 failure(s)
[15:06:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9998  0.9991  0.9986  0.9986  0.9986  0.9984  0.9984  0.9982  0.9982  0.9981  0.9979  0.9978  0.9974  0.9974
[17 : 32]:	0.9974  0.9971  0.9968  0.9967  0.9967  0.9964  0.9964  0.9961  0.9960  0.9958  0.9958  0.9955  0.9949  0.9948  0.9943  0.9943
[15:06:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:06:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:06:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:07:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:07:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_clip_4"
[15:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:07:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:08:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0377d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e038ac3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e038ac328)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0397fd08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03829848)]: 1989 failure(s)
[15:08:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 59 candidate(s)
[15:09:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0377d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e038ac3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e038ac328)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0397fd08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03829848)]: 426 failure(s)
[15:10:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0377d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e038ac3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e038ac328)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0397fd08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03829848)]: 338 failure(s)
[15:11:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0377d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e038ac3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e038ac328)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0397fd08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03829848)]: 346 failure(s)
[15:12:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0377d048)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e038ac3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e037cdf98)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e038ac328)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0397fd08)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e03829848)]: 394 failure(s)
[15:12:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9995  0.9995  0.9995  0.9995  0.9994  0.9994  0.9992  0.9990  0.9982  0.9980  0.9980  0.9979  0.9979  0.9978  0.9977
[17 : 32]:	0.9973  0.9972  0.9972  0.9971  0.9967  0.9967  0.9961  0.9960  0.9960  0.9949  0.9949  0.9948  0.9947  0.9946  0.9944  0.9942
[15:12:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:12:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:12:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:13:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:13:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_add"
[15:13:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[15:13:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[15:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0382e768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0391a7d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0379cae8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e055e4f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0392a308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037175b8)]: 1916 failure(s)
[15:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 132 candidate(s)
[15:14:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0382e768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0391a7d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0379cae8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e055e4f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0392a308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037175b8)]: 426 failure(s)
[15:15:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0382e768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0391a7d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0379cae8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e055e4f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0392a308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037175b8)]: 392 failure(s)
[15:16:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0382e768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0391a7d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0379cae8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e055e4f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0392a308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037175b8)]: 398 failure(s)
[15:16:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x555e0382e768)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x555e0391a7d8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x555e0379cae8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x555e055e4f58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x555e0392a308)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x555e037175b8)]: 359 failure(s)
[15:17:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9996  0.9992  0.9991  0.9988  0.9986  0.9985  0.9969  0.9967  0.9967  0.9967  0.9967  0.9964  0.9962  0.9962
[17 : 32]:	0.9961  0.9960  0.9960  0.9960  0.9957  0.9954  0.9954  0.9952  0.9951  0.9948  0.9948  0.9946  0.9945  0.9945  0.9942  0.9942
[15:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[15:17:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[15:17:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[15:17:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[15:17:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose"] Trial #0: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[15:17:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose"] Trial #1: GFLOPs: 0.0000. Time: 0.0038 ms. Best GFLOPs: 0.0000
[15:17:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose"] Trial #2: GFLOPs: 0.0000. Time: 0.0065 ms. Best GFLOPs: 0.0000
[15:17:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose"] Trial #3: GFLOPs: 0.0000. Time: 0.0055 ms. Best GFLOPs: 0.0000
[15:17:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_transpose"] Trial #4: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[15:17:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_transpose"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |            
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |         fused_nn_conv2d_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 5
Total latency (us): 3.7841

[15:17:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #0 has finished. Remaining task(s): 36
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(59):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5643 // 1881)
                                        v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1881 // 33)
                                        v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 33)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5643)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i2_3_init * 2 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #1: GFLOPs: 1030.7767. Time: 0.0222 ms. Best GFLOPs: 1030.7767
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #2: GFLOPs: 875.2996. Time: 0.0261 ms. Best GFLOPs: 1030.7767
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init in T.grid(4, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(230):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 7359 // 223)
                                    v3 = T.axis.spatial(225, i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 223)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 7359)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 2, 4, 1, 3, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 28, 1, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #4: GFLOPs: 1811.1159. Time: 0.0126 ms. Best GFLOPs: 1811.1159
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #5: GFLOPs: 97.2999. Time: 0.2352 ms. Best GFLOPs: 1811.1159
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #6: GFLOPs: 1855.0734. Time: 0.0123 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #7: GFLOPs: 1753.8133. Time: 0.0130 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #8: GFLOPs: 496.3686. Time: 0.0461 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #9: GFLOPs: 891.0881. Time: 0.0257 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1089 // 33)
                                        v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 33)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1089)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 3, 3, 1, 4, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_2])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 1, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #11: GFLOPs: 493.0698. Time: 0.0464 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #12: GFLOPs: 281.0279. Time: 0.0814 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 7 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1881 // 33)
                                        v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 33)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1881)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #14: GFLOPs: 57.4245. Time: 0.3984 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #15: GFLOPs: 569.3546. Time: 0.0402 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_clip"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i2_4_init, i3_4_init in T.grid(4, 4, 7):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_4_init)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i3_3_init * 7 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(53):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10035 // 3345)
                                            v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3345 // 223)
                                            v3 = T.axis.spatial(225, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 223)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 10035)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 4, 3, 1, 1, 1, 1, 4, 7):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_4)
                                    xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i3_3 * 7 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 1, 4, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l176)
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #17: GFLOPs: 1060.8033. Time: 0.0216 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #18: GFLOPs: 279.8624. Time: 0.0818 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #19: GFLOPs: 213.5195. Time: 0.1072 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #20: GFLOPs: 96.3005. Time: 0.2376 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #21: GFLOPs: 1725.6993. Time: 0.0133 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #22: GFLOPs: 136.5067. Time: 0.1676 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #23: GFLOPs: 638.7497. Time: 0.0358 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #24: GFLOPs: 114.5660. Time: 0.1997 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #25: GFLOPs: 342.4340. Time: 0.0668 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #26: GFLOPs: 485.2985. Time: 0.0471 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #27: GFLOPs: 882.1910. Time: 0.0259 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #28: GFLOPs: 1052.0926. Time: 0.0217 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #29: GFLOPs: 611.9272. Time: 0.0374 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #30: GFLOPs: 492.5380. Time: 0.0465 ms. Best GFLOPs: 1855.0734
[15:17:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_clip"] Trial #31: GFLOPs: 191.8692. Time: 0.1192 ms. Best GFLOPs: 1855.0734
[15:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_clip"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |            
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |         fused_nn_conv2d_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 37
Total latency (us): 16.118

[15:17:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #1 has finished. Remaining task(s): 35
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #0: GFLOPs: 275.0603. Time: 0.0306 ms. Best GFLOPs: 275.0603
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #1: GFLOPs: 15.5608. Time: 0.5417 ms. Best GFLOPs: 275.0603
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #2: GFLOPs: 166.4582. Time: 0.0506 ms. Best GFLOPs: 275.0603
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #3: GFLOPs: 118.2624. Time: 0.0713 ms. Best GFLOPs: 275.0603
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #4: GFLOPs: 443.4939. Time: 0.0190 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #5: GFLOPs: 205.9031. Time: 0.0409 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #6: GFLOPs: 219.6759. Time: 0.0384 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #7: GFLOPs: 148.6477. Time: 0.0567 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #8: GFLOPs: 62.9571. Time: 0.1339 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #9: GFLOPs: 59.4181. Time: 0.1419 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #10: GFLOPs: 43.3896. Time: 0.1943 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #11: GFLOPs: 398.0204. Time: 0.0212 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #12: GFLOPs: 439.5021. Time: 0.0192 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #13: GFLOPs: 159.1970. Time: 0.0530 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #14: GFLOPs: 120.9497. Time: 0.0697 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #15: GFLOPs: 165.4227. Time: 0.0510 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #16: GFLOPs: 120.4503. Time: 0.0700 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #17: GFLOPs: 215.6669. Time: 0.0391 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #18: GFLOPs: 112.7096. Time: 0.0748 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #19: GFLOPs: 134.5516. Time: 0.0626 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #20: GFLOPs: 83.1070. Time: 0.1014 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #21: GFLOPs: 127.8680. Time: 0.0659 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #22: GFLOPs: 326.1541. Time: 0.0258 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 8, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i2_3_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7680 // 240)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 240 // 30)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 7680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 8, 1, 1, 3, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 8, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 8, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 14, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #24: GFLOPs: 62.1112. Time: 0.1357 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #25: GFLOPs: 135.6270. Time: 0.0622 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #26: GFLOPs: 177.9111. Time: 0.0474 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #27: GFLOPs: 192.3798. Time: 0.0438 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #28: GFLOPs: 55.7396. Time: 0.1512 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(180):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 5760 // 180)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 28 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 180 // 6)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 6)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i1_4_init in T.grid(4, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3_init * 2 + i1_4_init)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 4, 1, 1, 3, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 14, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 32])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l158)
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #30: GFLOPs: 110.8859. Time: 0.0760 ms. Best GFLOPs: 443.4939
[15:17:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_clip_1"] Trial #31: GFLOPs: 479.7716. Time: 0.0176 ms. Best GFLOPs: 479.7716
[15:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_clip_1"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |            
  3 |         fused_nn_conv2d_add | 19568640 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 69
Total latency (us): 33.6879

[15:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #2 has finished. Remaining task(s): 34
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 822.0220. Time: 0.0238 ms. Best GFLOPs: 822.0220
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 82.5242. Time: 0.2371 ms. Best GFLOPs: 822.0220
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 528.0077. Time: 0.0371 ms. Best GFLOPs: 822.0220
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 899.2254. Time: 0.0218 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 361.8460. Time: 0.0541 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 578.5768. Time: 0.0338 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #6: GFLOPs: 520.8819. Time: 0.0376 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #7: GFLOPs: 15.3042. Time: 1.2786 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 95.1427. Time: 0.2057 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #9: GFLOPs: 200.1560. Time: 0.0978 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #10: GFLOPs: 530.7746. Time: 0.0369 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 786.6161. Time: 0.0249 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 519.4102. Time: 0.0377 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #13: GFLOPs: 582.5426. Time: 0.0336 ms. Best GFLOPs: 899.2254
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 1146.6495. Time: 0.0171 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 65.7505. Time: 0.2976 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(84, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 1792)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 1792 // 16)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 48)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 28 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 3, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 28, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 128])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 128])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 117.5568. Time: 0.1665 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(3, 2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 16 * 6 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 896)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 896 // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 2, 2, 8, 1, 1, 1, 2, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 16 * 6 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 16 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 4, 3, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 1, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 176.4021. Time: 0.1109 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 970.3004. Time: 0.0202 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #21: GFLOPs: 134.9871. Time: 0.1450 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 63.6756. Time: 0.3073 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(4, 7, 6, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i1_3_init * 6 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 7, 4, 1, 1, 1, 6, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i1_3 * 6 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 6])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 4, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 393.4802. Time: 0.0497 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 790.4833. Time: 0.0248 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 115.6544. Time: 0.1692 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i3_4_init in T.grid(3, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 1792)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 1792 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 8, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 4, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 2, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 64, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #28: GFLOPs: 483.2698. Time: 0.0405 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 647.7186. Time: 0.0302 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 229.8978. Time: 0.0851 ms. Best GFLOPs: 1146.6495
[15:18:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #31: GFLOPs: 3.9121. Time: 5.0021 ms. Best GFLOPs: 1146.6495
[15:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |            
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 101
Total latency (us): 50.7539

[15:18:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #3 has finished. Remaining task(s): 33
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #0: GFLOPs: 273.8067. Time: 0.3365 ms. Best GFLOPs: 273.8067
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #1: GFLOPs: 410.1530. Time: 0.2246 ms. Best GFLOPs: 410.1530
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #2: GFLOPs: 135.9273. Time: 0.6777 ms. Best GFLOPs: 410.1530
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(24, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 256)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 256 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i1_4_init in T.grid(3, 8):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + i1_3_init * 8 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 3, 1, 1, 4, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(24, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 6, 1, 3, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 6, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #4: GFLOPs: 967.1060. Time: 0.0953 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(252, thread="threadIdx.x"):
                    for i3_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 1008 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 896)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 1008 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 896 // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 1008 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 6, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(24, i4_0 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 9, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 1, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 252, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 252, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #6: GFLOPs: 682.8203. Time: 0.1349 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #7: GFLOPs: 133.8286. Time: 0.6884 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #8: GFLOPs: 183.6999. Time: 0.5015 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #9: GFLOPs: 107.1218. Time: 0.8600 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #10: GFLOPs: 64.7039. Time: 1.4238 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(96, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 32 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1728)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 32 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 3 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(24, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 32 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 3 + i1_3)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(24, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 32 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 32 // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 12, 2, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 24])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 98])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 98, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #12: GFLOPs: 14.2353. Time: 6.4715 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #13: GFLOPs: 51.6583. Time: 1.7833 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #14: GFLOPs: 590.4430. Time: 0.1560 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #15: GFLOPs: 762.6183. Time: 0.1208 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(896, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(36, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 224 * 36 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 224 // 56 * 7 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 1792 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 1792 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(24, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 144)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 36, 1, 1, 1, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 224 * 36 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 224 // 56 * 7 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56)
                                rc = T.axis.reduce(24, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 36, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 224 * 36 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 224 // 56 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 36, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 56, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 896, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 896])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #17: GFLOPs: 389.9481. Time: 0.2362 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #18: GFLOPs: 396.7576. Time: 0.2322 ms. Best GFLOPs: 967.1060
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #19: GFLOPs: 1680.5225. Time: 0.0548 ms. Best GFLOPs: 1680.5225
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 3, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_3_init * 3 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 28)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 36 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_3 * 3 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(24, i4_0 * 6 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 6, 2, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 3, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 84])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 84, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #21: GFLOPs: 109.0202. Time: 0.8450 ms. Best GFLOPs: 1680.5225
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #22: GFLOPs: 2173.9453. Time: 0.0424 ms. Best GFLOPs: 2173.9453
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 256)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 256 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 16 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 6, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 16, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 12, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 48, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 28 + i2_3_init * 14 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3584)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 28 + i2_3 * 14 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 28 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 3, 24, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 192, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 192, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #25: GFLOPs: 16.2667. Time: 5.6633 ms. Best GFLOPs: 2173.9453
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #26: GFLOPs: 258.7586. Time: 0.3560 ms. Best GFLOPs: 2173.9453
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #27: GFLOPs: 407.4493. Time: 0.2261 ms. Best GFLOPs: 2173.9453
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #28: GFLOPs: 2822.4394. Time: 0.0326 ms. Best GFLOPs: 2822.4394
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i3_4_init in T.grid(8, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 36 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(24, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 108)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 8, 2, 1, 1, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 6, 6, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 8, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 84, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 84, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #30: GFLOPs: 881.8833. Time: 0.1045 ms. Best GFLOPs: 2822.4394
[15:18:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_clip_2"] Trial #31: GFLOPs: 1592.9488. Time: 0.0578 ms. Best GFLOPs: 2822.4394
[15:18:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_clip_2"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |          Y 
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |      2822.4394 |      32.6395 |               32.6395 |     32 |            
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 133
Total latency (us): 83.3934

[15:18:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #4 has finished. Remaining task(s): 32
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #0: GFLOPs: 207.6791. Time: 0.0457 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #1: GFLOPs: 77.2456. Time: 0.1228 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #2: GFLOPs: 185.1808. Time: 0.0512 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #3: GFLOPs: 10.7744. Time: 0.8802 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #4: GFLOPs: 15.2209. Time: 0.6230 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #5: GFLOPs: 13.0360. Time: 0.7275 ms. Best GFLOPs: 207.6791
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #6: GFLOPs: 238.3269. Time: 0.0398 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #7: GFLOPs: 169.7039. Time: 0.0559 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #8: GFLOPs: 28.3982. Time: 0.3339 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #9: GFLOPs: 87.1241. Time: 0.1088 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #10: GFLOPs: 209.6970. Time: 0.0452 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #11: GFLOPs: 219.4835. Time: 0.0432 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #12: GFLOPs: 174.7178. Time: 0.0543 ms. Best GFLOPs: 238.3269
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #13: GFLOPs: 272.2055. Time: 0.0348 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #14: GFLOPs: 69.1648. Time: 0.1371 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #15: GFLOPs: 72.4967. Time: 0.1308 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #16: GFLOPs: 84.3461. Time: 0.1124 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #17: GFLOPs: 33.3405. Time: 0.2844 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #18: GFLOPs: 65.4176. Time: 0.1450 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #19: GFLOPs: 95.2772. Time: 0.0995 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #20: GFLOPs: 74.8722. Time: 0.1267 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #21: GFLOPs: 67.7378. Time: 0.1400 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #22: GFLOPs: 122.4644. Time: 0.0774 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #23: GFLOPs: 18.4680. Time: 0.5135 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #24: GFLOPs: 109.0826. Time: 0.0869 ms. Best GFLOPs: 272.2055
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #25: GFLOPs: 286.6406. Time: 0.0331 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #26: GFLOPs: 179.0250. Time: 0.0530 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #27: GFLOPs: 124.8545. Time: 0.0760 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #28: GFLOPs: 12.3298. Time: 0.7691 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #29: GFLOPs: 54.6116. Time: 0.1736 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #30: GFLOPs: 115.0343. Time: 0.0824 ms. Best GFLOPs: 286.6406
[15:18:06] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_clip_3"] Trial #31: GFLOPs: 122.1406. Time: 0.0776 ms. Best GFLOPs: 286.6406
[15:18:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_clip_3"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |          Y 
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |      2822.4394 |      32.6395 |               32.6395 |     32 |          Y 
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |       286.6406 |      33.0842 |               33.0842 |     32 |            
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 165
Total latency (us): 116.478

[15:18:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #5 has finished. Remaining task(s): 31
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #0: GFLOPs: 1017.1605. Time: 0.0285 ms. Best GFLOPs: 1017.1605
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #1: GFLOPs: 1344.1435. Time: 0.0216 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #2: GFLOPs: 527.0207. Time: 0.0550 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #3: GFLOPs: 39.7611. Time: 0.7294 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #4: GFLOPs: 197.3716. Time: 0.1469 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #5: GFLOPs: 464.3441. Time: 0.0625 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 28, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(84):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 3136)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3136 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused_1 // 3)
                                    v1 = T.axis.spatial(144, i4_0 * 3 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 96)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 2, 28, 3, 1, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(56, i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 56):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 14 * 4 + ax2)
                            v3 = T.axis.spatial(56, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 28, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[48, 1, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 112])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 112])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #7: GFLOPs: 1281.0661. Time: 0.0226 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #8: GFLOPs: 392.3537. Time: 0.0739 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #9: GFLOPs: 263.3678. Time: 0.1101 ms. Best GFLOPs: 1344.1435
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #10: GFLOPs: 1424.8970. Time: 0.0204 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #11: GFLOPs: 34.1353. Time: 0.8496 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #12: GFLOPs: 393.8617. Time: 0.0736 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #13: GFLOPs: 692.4862. Time: 0.0419 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #14: GFLOPs: 972.4583. Time: 0.0298 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #15: GFLOPs: 45.9610. Time: 0.6310 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #16: GFLOPs: 766.8643. Time: 0.0378 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #17: GFLOPs: 383.7864. Time: 0.0756 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #18: GFLOPs: 467.2051. Time: 0.0621 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #19: GFLOPs: 1259.2715. Time: 0.0230 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #20: GFLOPs: 983.7343. Time: 0.0295 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #21: GFLOPs: 139.0312. Time: 0.2086 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #22: GFLOPs: 429.6377. Time: 0.0675 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #23: GFLOPs: 426.1075. Time: 0.0681 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #24: GFLOPs: 327.1233. Time: 0.0887 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #25: GFLOPs: 369.3784. Time: 0.0785 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #26: GFLOPs: 60.2422. Time: 0.4814 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #27: GFLOPs: 52.1793. Time: 0.5558 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #28: GFLOPs: 377.8479. Time: 0.0768 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #29: GFLOPs: 327.6391. Time: 0.0885 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #30: GFLOPs: 1091.9067. Time: 0.0266 ms. Best GFLOPs: 1424.8970
[15:18:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #31: GFLOPs: 215.3400. Time: 0.1347 ms. Best GFLOPs: 1424.8970
[15:18:10] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_1"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |          Y 
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |      2822.4394 |      32.6395 |               32.6395 |     32 |          Y 
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |       286.6406 |      33.0842 |               33.0842 |     32 |          Y 
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |      1424.8970 |      20.3536 |               20.3536 |     32 |            
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |            N/A |          N/A |                   N/A |      0 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 197
Total latency (us): 136.831

[15:18:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #6 has finished. Remaining task(s): 30
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #0: GFLOPs: 99.0231. Time: 0.1277 ms. Best GFLOPs: 99.0231
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #1: GFLOPs: 175.2761. Time: 0.0721 ms. Best GFLOPs: 175.2761
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #2: GFLOPs: 643.3220. Time: 0.0197 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #3: GFLOPs: 343.4171. Time: 0.0368 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #4: GFLOPs: 457.9469. Time: 0.0276 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #5: GFLOPs: 527.6130. Time: 0.0240 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #6: GFLOPs: 476.5925. Time: 0.0265 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #7: GFLOPs: 160.9980. Time: 0.0785 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #8: GFLOPs: 255.2328. Time: 0.0495 ms. Best GFLOPs: 643.3220
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(100):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11136 // 348)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 348 // 6)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 11136)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_4_init in T.grid(4, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i1_3_init)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i1_3)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 8, 1, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #10: GFLOPs: 643.4411. Time: 0.0197 ms. Best GFLOPs: 643.4411
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #11: GFLOPs: 32.7403. Time: 0.3862 ms. Best GFLOPs: 643.4411
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #12: GFLOPs: 354.3699. Time: 0.0357 ms. Best GFLOPs: 643.4411
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #13: GFLOPs: 254.6280. Time: 0.0497 ms. Best GFLOPs: 643.4411
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #14: GFLOPs: 713.7829. Time: 0.0177 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #15: GFLOPs: 240.7120. Time: 0.0525 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #16: GFLOPs: 268.5659. Time: 0.0471 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #17: GFLOPs: 339.0713. Time: 0.0373 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #18: GFLOPs: 135.5204. Time: 0.0933 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #19: GFLOPs: 197.7675. Time: 0.0639 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #20: GFLOPs: 132.3521. Time: 0.0955 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #21: GFLOPs: 274.9035. Time: 0.0460 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(100):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 96 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9600 // 100)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 100 // 10)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init, i2_4_init in T.grid(3, 4, 8):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 3 + i1_3_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 3, 1, 4, 1, 3, 1, 1, 8, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 3 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 8, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 16, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #23: GFLOPs: 62.8291. Time: 0.2012 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(73):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9280 // 580)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 580 // 58)
                                        v3 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9280)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 144)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i3_3_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                    j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                    j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 1, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #25: GFLOPs: 211.1952. Time: 0.0599 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #26: GFLOPs: 37.8143. Time: 0.3344 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #27: GFLOPs: 166.7365. Time: 0.0758 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #28: GFLOPs: 481.4186. Time: 0.0263 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(96, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(63):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6960 // 1740)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1740 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6960)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28)
                            i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[48, 4, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 28, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #30: GFLOPs: 477.8744. Time: 0.0265 ms. Best GFLOPs: 713.7829
[15:18:11] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_clip_4"] Trial #31: GFLOPs: 710.5479. Time: 0.0178 ms. Best GFLOPs: 713.7829
[15:18:13] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_clip_4"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |          Y 
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |      2822.4394 |      32.6395 |               32.6395 |     32 |          Y 
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |       286.6406 |      33.0842 |               33.0842 |     32 |          Y 
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |      1424.8970 |      20.3536 |               20.3536 |     32 |          Y 
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |       713.7829 |      17.7146 |               53.1437 |     32 |            
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 229
Total latency (us): 189.975

[15:18:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #7 has finished. Remaining task(s): 29
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #0: GFLOPs: 591.8963. Time: 0.0654 ms. Best GFLOPs: 591.8963
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #1: GFLOPs: 303.5121. Time: 0.1276 ms. Best GFLOPs: 591.8963
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #2: GFLOPs: 269.4822. Time: 0.1437 ms. Best GFLOPs: 591.8963
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #3: GFLOPs: 590.7782. Time: 0.0656 ms. Best GFLOPs: 591.8963
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #4: GFLOPs: 1901.3072. Time: 0.0204 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #5: GFLOPs: 506.6278. Time: 0.0765 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #6: GFLOPs: 1022.5579. Time: 0.0379 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #7: GFLOPs: 337.7456. Time: 0.1147 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #8: GFLOPs: 1023.1982. Time: 0.0379 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #9: GFLOPs: 1086.6385. Time: 0.0356 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #10: GFLOPs: 409.6486. Time: 0.0946 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #11: GFLOPs: 1513.0851. Time: 0.0256 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #12: GFLOPs: 1210.0575. Time: 0.0320 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #13: GFLOPs: 194.4911. Time: 0.1992 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #14: GFLOPs: 661.0342. Time: 0.0586 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #15: GFLOPs: 853.6272. Time: 0.0454 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #16: GFLOPs: 1072.0674. Time: 0.0361 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #17: GFLOPs: 1186.9687. Time: 0.0326 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #18: GFLOPs: 1495.7302. Time: 0.0259 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #19: GFLOPs: 1784.5958. Time: 0.0217 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #20: GFLOPs: 952.1766. Time: 0.0407 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #21: GFLOPs: 533.8248. Time: 0.0726 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #22: GFLOPs: 42.9945. Time: 0.9009 ms. Best GFLOPs: 1901.3072
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #23: GFLOPs: 2988.5187. Time: 0.0130 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #24: GFLOPs: 32.3391. Time: 1.1978 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #25: GFLOPs: 1822.3262. Time: 0.0213 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #26: GFLOPs: 314.4796. Time: 0.1232 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #27: GFLOPs: 103.1354. Time: 0.3756 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #28: GFLOPs: 808.4524. Time: 0.0479 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #29: GFLOPs: 689.1182. Time: 0.0562 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #30: GFLOPs: 875.9097. Time: 0.0442 ms. Best GFLOPs: 2988.5187
[15:18:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #31: GFLOPs: 736.7587. Time: 0.0526 ms. Best GFLOPs: 2988.5187
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_add"
 ID |                        Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
------------------------------------------------------------------------------------------------------------------------------------
  0 |             fused_transpose |        1 |      1 |         0.0003 |       3.7841 |                3.7841 |      5 |          Y 
  1 |    fused_nn_conv2d_add_clip | 22880256 |      1 |      1855.0734 |      12.3339 |               12.3339 |     32 |          Y 
  2 |  fused_nn_conv2d_add_clip_1 |  8429568 |      1 |       479.7716 |      17.5700 |               17.5700 |     32 |          Y 
  3 |         fused_nn_conv2d_add | 19568640 |      1 |      1146.6495 |      17.0659 |               17.0659 |     32 |          Y 
  4 |  fused_nn_conv2d_add_clip_2 | 92123136 |      1 |      2822.4394 |      32.6395 |               32.6395 |     32 |          Y 
  5 |  fused_nn_conv2d_add_clip_3 |  9483264 |      1 |       286.6406 |      33.0842 |               33.0842 |     32 |          Y 
  6 |       fused_nn_conv2d_add_1 | 29001728 |      1 |      1424.8970 |      20.3536 |               20.3536 |     32 |          Y 
  7 |  fused_nn_conv2d_add_clip_4 | 12644352 |      3 |       713.7829 |      17.7146 |               53.1437 |     32 |          Y 
  8 |     fused_nn_conv2d_add_add | 38735872 |      3 |      2988.5187 |      12.9616 |               38.8847 |     32 |            
  9 |  fused_nn_conv2d_add_clip_5 | 40341504 |      4 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_clip_6 |  7977984 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |       fused_nn_conv2d_add_2 | 16903040 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_conv2d_add_clip_7 | 13961472 |      3 |            N/A |          N/A |                   N/A |      0 |            
 13 |   fused_nn_conv2d_add_add_1 | 29591296 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_clip_8 | 30293760 |      4 |            N/A |          N/A |                   N/A |      0 |            
 15 |  fused_nn_conv2d_add_clip_9 |  1382976 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 |       fused_nn_conv2d_add_3 | 14773696 |      1 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_nn_conv2d_add_clip_10 |  2765952 |      5 |            N/A |          N/A |                   N/A |      0 |            
 18 |   fused_nn_conv2d_add_add_2 | 29547392 |      5 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_clip_11 | 29898624 |      6 |            N/A |          N/A |                   N/A |      0 |            
 20 | fused_nn_conv2d_add_clip_12 |  6980736 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 |       fused_nn_conv2d_add_4 | 42179200 |      1 |            N/A |          N/A |                   N/A |      0 |            
 22 | fused_nn_conv2d_add_clip_13 |  9972480 |      5 |            N/A |          N/A |                   N/A |      0 |            
 23 |   fused_nn_conv2d_add_add_3 | 60273920 |      5 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_clip_14 | 60775680 |      6 |            N/A |          N/A |                   N/A |      0 |            
 25 | fused_nn_conv2d_add_clip_15 |  2493120 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |       fused_nn_conv2d_add_5 | 25603088 |      1 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_clip_16 |  4238304 |      7 |            N/A |          N/A |                   N/A |      0 |            
 28 |   fused_nn_conv2d_add_add_4 | 43529248 |      7 |            N/A |          N/A |                   N/A |      0 |            
 29 | fused_nn_conv2d_add_clip_17 | 43742496 |      8 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_clip_18 |  1679328 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |       fused_nn_conv2d_add_6 | 71673280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 | fused_nn_conv2d_add_clip_19 | 56385280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |         fused_nn_avg_pool2d |    79360 |      1 |            N/A |          N/A |                   N/A |      0 |            
 34 |               fused_squeeze |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 35 |          fused_nn_dense_add |  2561000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 36 |            fused_nn_softmax |     4000 |      1 |            N/A |          N/A |                   N/A |      0 |            
------------------------------------------------------------------------------------------------------------------------------------
Total trials: 261
Total latency (us): 228.86

[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #8 has finished. Remaining task(s): 28
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #9 has finished. Remaining task(s): 27
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #10 has finished. Remaining task(s): 26
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #11 has finished. Remaining task(s): 25
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #12 has finished. Remaining task(s): 24
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #13 has finished. Remaining task(s): 23
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #14 has finished. Remaining task(s): 22
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #15 has finished. Remaining task(s): 21
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #16 has finished. Remaining task(s): 20
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #17 has finished. Remaining task(s): 19
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #18 has finished. Remaining task(s): 18
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #19 has finished. Remaining task(s): 17
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #20 has finished. Remaining task(s): 16
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #21 has finished. Remaining task(s): 15
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #22 has finished. Remaining task(s): 14
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #23 has finished. Remaining task(s): 13
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #24 has finished. Remaining task(s): 12
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #25 has finished. Remaining task(s): 11
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #26 has finished. Remaining task(s): 10
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #27 has finished. Remaining task(s): 9
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #28 has finished. Remaining task(s): 8
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #29 has finished. Remaining task(s): 7
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #30 has finished. Remaining task(s): 6
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #31 has finished. Remaining task(s): 5
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #32 has finished. Remaining task(s): 4
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #33 has finished. Remaining task(s): 3
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #34 has finished. Remaining task(s): 2
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #35 has finished. Remaining task(s): 1
[15:18:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:150: Task #36 has finished. Remaining task(s): 0
[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_transpose: T.Buffer[(1, 3, 224, 224), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax2, ax3, ax1])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
    

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_transpose(placeholder: T.Buffer[(1, 224, 224, 3), "float32"], T_transpose: T.Buffer[(1, 3, 224, 224), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0_i1_i2_i3_fused_0 in T.thread_binding(147, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_i1_i2_i3_fused_1 in T.thread_binding(1024, thread="threadIdx.x"):
                with T.block("T_transpose"):
                    ax0 = T.axis.spatial(1, 0)
                    ax1 = T.axis.spatial(3, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) // 50176)
                    ax2 = T.axis.spatial(224, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 50176 // 224)
                    ax3 = T.axis.spatial(224, (i0_i1_i2_i3_fused_0 * 1024 + i0_i1_i2_i3_fused_1) % 224)
                    T.reads(placeholder[ax0, ax2, ax3, ax1])
                    T.writes(T_transpose[ax0, ax1, ax2, ax3])
                    T_transpose[ax0, ax1, ax2, ax3] = placeholder[ax0, ax2, ax3, ax1]
    

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"root\", func_name=\"main\")", "v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)", "sch.annotate(block_or_loop=b0, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v1)", "sch.enter_postproc()", "b2 = sch.get_block(name=\"T_transpose\", func_name=\"main\")", "l3, l4, l5, l6 = sch.get_loops(block=b2)", "l7 = sch.fuse(l3, l4, l5, l6)", "l8, l9 = sch.split(loop=l7, factors=[None, 1024])", "sch.bind(loop=l8, thread_axis=\"blockIdx.x\")", "sch.bind(loop=l9, thread_axis=\"threadIdx.x\")", "b10 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b10, ann_key=\"meta_schedule.unroll_explicit\")", "b11, = sch.get_child_blocks(b10)", "l12, l13 = sch.get_loops(block=b11)", "sch.annotate(block_or_loop=l12, ann_key=\"pragma_auto_unroll_max_step\", ann_val=16)", "sch.annotate(block_or_loop=l12, ann_key=\"pragma_unroll_explicit\", ann_val=1)"]
[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 225, 225], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 225, 225):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 224 and 0 <= i3_1 and i3_1 < 224, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_clip(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 225, 225], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 1485 // 495)
                                    v2 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused // 16 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 495 // 15)
                                    v3 = T.axis.spatial(225, i0_0_i1_0_i2_0_i3_0_fused % 16 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 1485)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 224 and 0 <= v3 and v3 < 224, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i2_4_init in T.grid(2, 8):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + i2_3_init * 8 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 8, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + i2_3 * 8 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 16, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 16 * 16 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:18:58] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"compute\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 8])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[16, 1, 7, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 224])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 3])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l176)"]
[15:19:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 114, 114], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 114, 114):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 112, 112, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[15:19:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_clip_1(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], compute: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6728 // 3364)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3364 // 58)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 6728)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 2 + ax0_ax1_ax2_ax3_fused_1 // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 18)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1):
                            for i2_4_init, i3_4_init in T.grid(2, 7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4_init)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 2, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 4 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

[15:19:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:02] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"compute\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 1, 1, 1])", "l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 2])", "l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 7])", "l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)", "l63 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l63, thread_axis=\"blockIdx.x\")", "l64 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l64, thread_axis=\"vthread.x\")", "l65 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l65, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)", "b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)", "l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)", "l77 = sch.fuse(l73, l74, l75, l76)", "v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v78)", "b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)", "l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)", "l89 = sch.fuse(l85, l86, l87, l88)", "v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v90)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v91)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\")", "l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)", "l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 2])", "sch.vectorize(loop=l100)", "sch.bind(loop=l99, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)", "l107, l108 = sch.split(loop=l106, factors=[None, 224])", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "b109 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b109, ann_key=\"meta_schedule.unroll_explicit\")", "b110, b111, b112, b113 = sch.get_child_blocks(b109)", "l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)", "l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)", "l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)", "l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)", "b153 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)", "b171 = sch.decompose_reduction(block=b153, loop=l165)"]
[15:19:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 112, 112, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(24, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 512)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [24, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:03] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 3, 2, 2])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[56, 1, 1, 2, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 16, 1])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108, l109 = sch.split(loop=l106, factors=[None, 48, 4])", "sch.vectorize(loop=l109)", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 48, 3])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l134, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l143, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l143, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l163, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l163, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b170 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)", "b191 = sch.decompose_reduction(block=b170, loop=l174)"]
[15:19:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 144, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 144, 112, 112, 24, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 144, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 112, 112):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[15:19:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_clip_2(placeholder: T.Buffer[(1, 24, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(12, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 4 + i2_3_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 112)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 896)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 896 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 12, 4, 1, 2, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 112)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 112, 112], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 112 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

[15:19:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:05] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"compute\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 1, 12, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 1])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 112, 1, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 1, 2])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 224])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 4])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l125, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l133, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l142, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l162, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b169 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)", "b190 = sch.decompose_reduction(block=b169, loop=l173)"]
[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 113, 113], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 113, 113):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(0 <= i2_1 and i2_1 < 112 and 0 <= i3_1 and i3_1 < 112, placeholder[i0_1, i1_1, i2_1, i3_1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 144, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 112, 112], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_clip_3(placeholder: T.Buffer[(1, 144, 112, 112), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], compute: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 113, 113], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(448, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(252, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 112, 112], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(17):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 4131 // 459)
                                        v2 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 459 // 27)
                                        v3 = T.axis.spatial(113, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 < 4131)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(0 <= v2 and v2 < 112 and 0 <= v3 and v3 < 112, placeholder[v0, v1, v2, v3], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 9 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 27)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 112, 112], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [0, 0, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:06] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"compute\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 9, 1, 1])", "l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])", "l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 2, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])", "l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])", "l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)", "l63 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l63, thread_axis=\"blockIdx.x\")", "l64 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l64, thread_axis=\"vthread.x\")", "l65 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l65, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)", "b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)", "l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)", "l77 = sch.fuse(l73, l74, l75, l76)", "v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)", "sch.annotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v78)", "b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)", "l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)", "l89 = sch.fuse(l85, l86, l87, l88)", "v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v90)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v91)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\")", "l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)", "l98, l99 = sch.split(loop=l97, factors=[None, 252])", "sch.bind(loop=l99, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)", "l106, l107 = sch.split(loop=l105, factors=[None, 252])", "sch.bind(loop=l107, thread_axis=\"threadIdx.x\")", "b108 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b108, ann_key=\"meta_schedule.unroll_explicit\")", "b109, b110, b111, b112 = sch.get_child_blocks(b108)", "l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)", "sch.annotate(block_or_loop=l113, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l113, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)", "sch.annotate(block_or_loop=l120, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l120, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)", "sch.annotate(block_or_loop=l127, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l127, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l144, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b151 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)", "b169 = sch.decompose_reduction(block=b151, loop=l156)"]
[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 144, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_1(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                        yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                        xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 28)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28 // 7)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 504)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(144, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(144, i4_0 * 18 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 16, 2, 1, 1])", "l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])", "l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 3, 6])", "l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])", "v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)", "l69 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l69, thread_axis=\"blockIdx.x\")", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"vthread.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)", "b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)", "l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)", "l84 = sch.fuse(l80, l81, l82, l83)", "v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v85)", "b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)", "l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)", "l97 = sch.fuse(l93, l94, l95, l96)", "v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v98)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)", "sch.annotate(block_or_loop=b3, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v99)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b73, ann_key=\"meta_schedule.cooperative_fetch\")", "l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)", "l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])", "sch.vectorize(loop=l109)", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b86, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)", "l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3])", "sch.vectorize(loop=l119)", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b120 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b120, ann_key=\"meta_schedule.unroll_explicit\")", "b121, b122, b123, b124 = sch.get_child_blocks(b120)", "l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)", "l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)", "l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)", "l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)", "b170 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)", "b191 = sch.decompose_reduction(block=b170, loop=l174)"]
[15:19:08] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_4
[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 192, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 192, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("compute"):
                i0_2, i1_2, i2_2, i3_2 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[i0_2, i1_2, i2_2, i3_2])
                T.writes(compute[i0_2, i1_2, i2_2, i3_2])
                compute[i0_2, i1_2, i2_2, i3_2] = T.max(T.min(T_add[i0_2, i1_2, i2_2, i3_2], T.float32(6)), T.float32(0))
    

[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_clip_5(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], compute: T.Buffer[(1, 192, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init in T.grid(6, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 4 + i2_4_init)
                                j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6720 // 560)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 560 // 56)
                                            v3 = T.axis.spatial(58, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6720)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 6, 4, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 4 + i2_4)
                                    j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(compute[v0, v1, v2, v3])
                            compute[v0, v1, v2, v3] = T.max(T.min(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(6)), T.float32(0))
    

[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:09] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"PaddedInput\", func_name=\"main\")", "b1 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"compute\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)", "v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])", "v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 1, 6])", "l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])", "v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 4])", "l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])", "v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 56, 1, 1])", "l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])", "v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])", "l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])", "v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])", "l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])", "sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)", "l63 = sch.fuse(l16, l26, l36, l46)", "sch.bind(loop=l63, thread_axis=\"blockIdx.x\")", "l64 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l64, thread_axis=\"vthread.x\")", "l65 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l65, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)", "b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)", "l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)", "l77 = sch.fuse(l73, l74, l75, l76)", "v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)", "sch.annotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v78)", "b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)", "l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)", "l89 = sch.fuse(l85, l86, l87, l88)", "v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v90)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v91)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b67, ann_key=\"meta_schedule.cooperative_fetch\")", "l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)", "l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 4])", "sch.vectorize(loop=l100)", "sch.bind(loop=l99, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b79, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)", "l107, l108 = sch.split(loop=l106, factors=[None, 224])", "sch.bind(loop=l108, thread_axis=\"threadIdx.x\")", "b109 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b109, ann_key=\"meta_schedule.unroll_explicit\")", "b110, b111, b112, b113 = sch.get_child_blocks(b109)", "l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)", "sch.annotate(block_or_loop=l114, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l114, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)", "sch.annotate(block_or_loop=l122, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l122, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)", "sch.annotate(block_or_loop=l129, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l129, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)", "sch.annotate(block_or_loop=l146, ann_key=\"pragma_auto_unroll_max_step\", ann_val=1024)", "sch.annotate(block_or_loop=l146, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b153 = sch.get_block(name=\"DepthwiseConv2d\", func_name=\"main\")", "l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)", "b171 = sch.decompose_reduction(block=b153, loop=l158)"]
[15:19:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:122: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 56, 56), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 56, 56, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[15:19:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:123: best replacement TIR found for the above:

[15:19:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:124: # from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def tvmgen_default_fused_nn_conv2d_add_add(placeholder: T.Buffer[(1, 192, 56, 56), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 56, 56), "float32"], T_add: T.Buffer[(1, 32, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(192, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                rc = T.axis.reduce(192, i4_0 * 32 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 56, 56], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

[15:19:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:125: Printing out trace for AHB:
[15:19:12] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:126: ["b0 = sch.get_block(name=\"pad_temp\", func_name=\"main\")", "b1 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "b2 = sch.get_block(name=\"T_add\", func_name=\"main\")", "b3 = sch.get_block(name=\"T_add_1\", func_name=\"main\")", "b4 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.tiling_structure\", ann_val=\"SSSRRSRS\")", "l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)", "v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])", "l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])", "v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 2])", "l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])", "v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])", "l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])", "v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 1])", "l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])", "v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 32, 1])", "l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])", "v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])", "v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])", "l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])", "sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)", "l70 = sch.fuse(l17, l27, l37, l47)", "sch.bind(loop=l70, thread_axis=\"blockIdx.x\")", "l71 = sch.fuse(l18, l28, l38, l48)", "sch.bind(loop=l71, thread_axis=\"vthread.x\")", "l72 = sch.fuse(l19, l29, l39, l49)", "sch.bind(loop=l72, thread_axis=\"threadIdx.x\")", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_low_inclusive\", ann_val=32)", "sch.annotate(block_or_loop=b1, ann_key=\"meta_schedule.thread_extent_high_inclusive\", ann_val=1024)", "b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")", "sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)", "b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")", "sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)", "l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)", "l85 = sch.fuse(l81, l82, l83, l84)", "v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)", "sch.annotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v86)", "b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"shared\")", "sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)", "l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)", "l98 = sch.fuse(l94, l95, l96, l97)", "v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)", "sch.annotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v99)", "sch.reverse_compute_inline(block=b3)", "sch.reverse_compute_inline(block=b2)", "sch.compute_inline(block=b0)", "v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)", "sch.annotate(block_or_loop=b4, ann_key=\"meta_schedule.unroll_explicit\", ann_val=v100)", "sch.enter_postproc()", "sch.unannotate(block_or_loop=b74, ann_key=\"meta_schedule.cooperative_fetch\")", "l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)", "l108, l109 = sch.split(loop=l107, factors=[None, 32])", "sch.bind(loop=l109, thread_axis=\"threadIdx.x\")", "sch.unannotate(block_or_loop=b87, ann_key=\"meta_schedule.cooperative_fetch\")", "l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)", "l117, l118 = sch.split(loop=l116, factors=[None, 32])", "sch.bind(loop=l118, thread_axis=\"threadIdx.x\")", "b119 = sch.get_block(name=\"root\", func_name=\"main\")", "sch.unannotate(block_or_loop=b119, ann_key=\"meta_schedule.unroll_explicit\")", "b120, b121, b122, b123 = sch.get_child_blocks(b119)", "l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l124, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l132, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l140, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_auto_unroll_max_step\", ann_val=512)", "sch.annotate(block_or_loop=l160, ann_key=\"pragma_unroll_explicit\", ann_val=1)", "b167 = sch.get_block(name=\"conv2d_nchw\", func_name=\"main\")", "l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)", "b188 = sch.decompose_reduction(block=b167, loop=l171)"]
[15:19:13] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_6
[15:19:14] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_2
[15:19:14] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_7
[15:19:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_8
[15:19:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_1
[15:19:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_9
[15:19:15] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_3
[15:19:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_10
[15:19:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_11
[15:19:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_2
[15:19:16] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_12
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_4
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_13
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_14
[15:19:17] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_3
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_15
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_5
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_16
[15:19:18] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_17
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_add_4
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_18
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_6
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_conv2d_add_clip_19
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_avg_pool2d
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_squeeze
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_dense_add
[15:19:19] /home/yj/tvm/src/meta_schedule/apply_history_best.cc:130: Warning: Cannot find workload: tvmgen_default_fused_nn_softmax
https://storage.cloud.google.com/octoml-aquarium-models/onnx_model_zoo/vision_classification_efficientnet-lite4-11.onnx
Downloading object from gcp. Bucket name: octoml-aquarium-models, file path: onnx_model_zoo/vision_classification_efficientnet-lite4-11.onnx
/home/yj/models/efficientnet-lite4.onnx
Starting to build with relay.
/home/yj/anaconda3/lib/python3.8/site-packages/google/auth/_default.py:79: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a "quota exceeded" or "API not enabled" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/
  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)
/home/yj/anaconda3/lib/python3.8/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
