nohup: ignoring input
[23:26:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #0: "fused_nn_conv2d_add_nn_relu"
[23:26:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 226, 226], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 226, 226):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 225 and 1 <= i3_1 and i3_1 < 225, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 112, 112, 3, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:26:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:26:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(76275):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 76275 // 25425)
                                    v2 = T.axis.spatial(226, ax0_ax1_ax2_ax3_fused % 25425 // 113)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + ax0_ax1_ax2_ax3_fused % 113)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(27):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 4, 1, 3, 1, 3, 1, 1, 1, 14):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:26:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #1: "fused_nn_conv2d_add_nn_relu_1"
[23:26:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 112, 112, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1792):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 896)
                                    v2 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 896 // 8)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(8):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 4, 2, 2, 1, 1, 1, 1, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i1_3)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + ax1)
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 2, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #2: "fused_nn_conv2d_add_nn_relu_2"
[23:26:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 32, 114, 114], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 114, 114):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 32, 112, 112, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:26:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:26:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(107648):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused % 107648 // 3364)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + ax0_ax1_ax2_ax3_fused % 3364 // 58)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax0_ax1_ax2_ax3_fused % 58)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 3, 1, 32, 8, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 8, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 32])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:26:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #3: "fused_nn_conv2d_add"
[23:26:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 16, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 16, 112, 112, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 16, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:26:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:26:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 1568)
                                    v2 = T.axis.spatial(112, ax0_ax1_ax2_ax3_fused % 1568 // 14)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 16, 4, 1, 2, 1, 1, 1, 1, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(16, i1_3)
                                    yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 7 * 28 + i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 28, 1):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused // 7 * 28 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 16, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 4, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 7, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 4, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:26:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #4: "fused_nn_conv2d_add_nn_relu_3"
[23:26:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 16, 112, 112], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 96, 112, 112], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 112, 112], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 112, 112):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 112, 112, 16, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 96, 112, 112):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 112, 112):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:26:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:26:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(784, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused // 32)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 4 + ax0_ax1_ax2_ax3_fused % 32 // 8)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 16, 1, 1, 1, 3, 2, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 12 + i1_3 * 3 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                    rc = T.axis.reduce(16, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 12, 2, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 12 + ax1)
                                v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                                v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 4, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 4, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:26:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #5: "fused_nn_conv2d_add_nn_relu_4"
[23:26:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 96, 114, 114], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 114, 114):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 113 and 1 <= i3_1 and i3_1 < 113, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 96, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(206112):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax0_ax1_ax2_ax3_fused % 206112 // 6441)
                                    v2 = T.axis.spatial(114, ax0_ax1_ax2_ax3_fused % 6441 // 57)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ax0_ax1_ax2_ax3_fused % 57)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 28, 3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3)
                                    i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 28):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 28, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 28, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #6: "fused_nn_conv2d_add_1"
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 24, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 56, 56, 96, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(7, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4704):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 6 + ax0_ax1_ax2_ax3_fused // 784)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + ax0_ax1_ax2_ax3_fused % 784 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(72):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 12 + ax0_ax1_ax2_ax3_fused // 6)
                                    v1 = T.axis.spatial(96, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 1, 14, 4):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                    yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i2_4)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i3_4)
                                    rc = T.axis.reduce(96, i4_0 * 6 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 6, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #7: "fused_nn_conv2d_add_nn_relu_5"
[23:27:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 144, 56, 56, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(21, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(28, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(250560):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused % 250560 // 1740)
                                    v2 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 1740 // 30)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused * 28 + ax0_ax1_ax2_ax3_fused % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1296):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 4, 4, 2, 1, 1, 1, 3, 2, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_3 * 3 + i1_4)
                                    i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 12, 8, 4):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 4, 4, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 4, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:27:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #8: "fused_nn_conv2d_add_add"
[23:27:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 24, 56, 56], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 24, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 24, 56, 56, 144, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[23:27:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(112896):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 36 + ax0_ax1_ax2_ax3_fused // 3136)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 3136 // 56)
                                    v3 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused // 36)
                                    v1 = T.axis.spatial(144, i4_0 * 36 + ax0_ax1_ax2_ax3_fused % 36)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(36, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 28):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i1_4)
                                    yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i2_3)
                                    xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i3_4)
                                    rc = T.axis.reduce(144, i4_0 * 36 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 28):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + ax1)
                                v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax2)
                                v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 2, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 28])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 36, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:27:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #9: "fused_nn_conv2d_add_nn_relu_6"
[23:27:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 24, 56, 56], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 24, 56, 56):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 144, 56, 56, 24, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 56, 56):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4704):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 3 + ax0_ax1_ax2_ax3_fused // 1568)
                                    v2 = T.axis.spatial(56, ax0_ax1_ax2_ax3_fused % 1568 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(108):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(24, i4_0 * 3 + ax0_ax1_ax2_ax3_fused % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 18, 1, 28, 3, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + i0_1_i1_1_i2_1_i3_1_fused * 18 + i1_3)
                                    yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i3_3)
                                    rc = T.axis.reduce(24, i4_0 * 3 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 18, 1, 28):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + i0_1_i1_1_i2_1_i3_1_fused * 18 + ax1)
                                v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 1, 18, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 56, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 28, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:27:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #10: "fused_nn_conv2d_add_nn_relu_7"
[23:27:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 144, 58, 58], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 144, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 144, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 58, 58):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 57 and 1 <= i3_1 and i3_1 < 57, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 144, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 144, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 144, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4704, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(150480):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 48 + ax0_ax1_ax2_ax3_fused % 150480 // 3135)
                                    v2 = T.axis.spatial(58, ax0_ax1_ax2_ax3_fused % 3135 // 55)
                                    v3 = T.axis.spatial(58, i5_0 + ax0_ax1_ax2_ax3_fused % 55)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 48 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i5_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 392 * 4 + i1_4)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 392 // 14)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 392 * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 392 // 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 12, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:27:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #11: "fused_nn_conv2d_add_2"
[23:27:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 144, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 144, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 144, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:27:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(18, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 8 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ax0_ax1_ax2_ax3_fused // 8)
                                    v1 = T.axis.spatial(144, i4_0 * 8 + ax0_ax1_ax2_ax3_fused % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 28, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(28, i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    rc = T.axis.reduce(144, i4_0 * 8 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                                v2 = T.axis.spatial(28, ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 4, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 28])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[18, 8, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:27:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #12: "fused_nn_conv2d_add_add_1"
[23:27:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 32, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 32, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 32, 28, 28, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 32, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 32, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[23:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(12544):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 32 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax0_ax1_ax2_ax3_fused % 392 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(1024):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused // 32)
                                    v1 = T.axis.spatial(192, i4_0 * 32 + ax0_ax1_ax2_ax3_fused % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 7, 4, 8, 1, 1, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(28, i3_3 * 7 + i3_4)
                                    rc = T.axis.reduce(192, i4_0 * 32 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 28):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(28, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 4, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #13: "fused_nn_conv2d_add_nn_relu_8"
[23:27:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 32, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 32, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 192, 28, 28, 32, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(12, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1792):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 8, 1, 2, 2, 1, 1, 1, 8, 7, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 4 * 64 + i1_3 * 8 + i1_4)
                                    yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_3 * 2 + i3_4)
                                    rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 7, 4):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 4 * 64 + ax1)
                                v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 3, 8, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #14: "fused_nn_conv2d_add_nn_relu_9"
[23:27:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 192, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 192, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(3, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1728):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + ax0_ax1_ax2_ax3_fused % 1728 // 36)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + ax0_ax1_ax2_ax3_fused % 36 // 6)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 6)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(432):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 8, 1, 1, 1, 3, 1, 1, 2, 4):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 24 + i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 24 + i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 3, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:27:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #15: "fused_nn_conv2d_add_3"
[23:27:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 192, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 192, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 28, 28, 192, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:27:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(96, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 2 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ax0_ax1_ax2_ax3_fused // 2)
                                    v1 = T.axis.spatial(192, i4_0 * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 7, 2, 1, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_3)
                                    xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_3)
                                    rc = T.axis.reduce(192, i4_0 * 2 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                                v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[96, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:27:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #16: "fused_nn_conv2d_add_nn_relu_10"
[23:27:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 384, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 384, 28, 28, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:27:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(48, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(6272):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused % 6272 // 392)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i4_0 + ax0_ax1_ax2_ax3_fused % 392 // 28)
                                    v3 = T.axis.spatial(30, i5_0 + ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(16):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                                v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[24, 1, 4, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:27:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #17: "fused_nn_conv2d_add_add_2"
[23:27:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 64, 28, 28], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 64, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 64, 28, 28, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 64, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 64, 28, 28):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[23:27:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:27:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(16, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(18816):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 48 + ax0_ax1_ax2_ax3_fused // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax0_ax1_ax2_ax3_fused % 392 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(384, i4_0 * 48 + ax0_ax1_ax2_ax3_fused % 48)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 2, 16, 1, 1, 1, 1, 7, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                    rc = T.axis.reduce(384, i4_0 * 48 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 14):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 3, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:27:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #18: "fused_nn_conv2d_add_nn_relu_11"
[23:27:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 64, 28, 28], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 64, 28, 28):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 384, 28, 28, 64, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 28, 28):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(256, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(1792):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax0_ax1_ax2_ax3_fused % 112 // 28)
                                    v3 = T.axis.spatial(28, ax0_ax1_ax2_ax3_fused % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 2 + i1_4)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_4)
                                    rc = T.axis.reduce(64, i4_0 * 16 + i4_1)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 16 * 2 + ax1)
                                v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 16 // 4 + ax2)
                                v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 16, 1, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 16, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #19: "fused_nn_conv2d_add_nn_relu_12"
[23:28:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 384, 30, 30], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 30, 30):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 29 and 1 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 384, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(38880):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + ax0_ax1_ax2_ax3_fused % 38880 // 405)
                                    v2 = T.axis.spatial(30, i4_0 + ax0_ax1_ax2_ax3_fused % 405 // 15)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 12, 1, 1, 1, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 12 + i1_3)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 12 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 4, 12, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:28:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #20: "fused_nn_conv2d_add_4"
[23:28:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 96, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 14, 14, 384, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 96, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:28:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([96, 384, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(96, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(384, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 7, 2, 1, 1, 1, 16, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_4)
                                    yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                    rc = T.axis.reduce(384, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                                v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[3, 2, 1, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[96, 2, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:28:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #21: "fused_nn_conv2d_add_nn_relu_13"
[23:28:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 576, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 576, 14, 14, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            DepthwiseConv2d_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(196, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(2, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(64512):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + ax0_ax1_ax2_ax3_fused % 64512 // 224)
                                    v2 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 224 // 14)
                                    v3 = T.axis.spatial(16, i5_0 + ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(864):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    v3 = T.axis.spatial(3, i5_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 8, 2, 1, 3, 1, 1, 9, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 72 + i1_3 * 9 + i1_4)
                                    i = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 2 + i2_3)
                                    j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 72, 2, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + i0_1_i1_1_i2_1_i3_1_fused // 49 * 72 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 * 2 + ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 1, 8, 9])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #22: "fused_nn_conv2d_add_add_3"
[23:28:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 96, 14, 14], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 96, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 14, 14, 576, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 96, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 14, 14):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[23:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([96, 576, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(36, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(3136):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 196)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 196 // 14)
                                    v3 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(576, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 8, 2, 1, 16, 1, 1, 1, 1, 7, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_3)
                                    yy = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                    xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_4)
                                    rc = T.axis.reduce(576, i4_0 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                                v2 = T.axis.spatial(14, ax2)
                                v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 4, 1, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[36, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #23: "fused_nn_conv2d_add_nn_relu_14"
[23:28:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 96, 14, 14], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        T_add = T.alloc_buffer([1, 576, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 14, 14):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 576, 14, 14, 96, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 14, 14):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([576, 96, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(336):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 12 + ax0_ax1_ax2_ax3_fused // 28)
                                    v2 = T.axis.spatial(14, ax0_ax1_ax2_ax3_fused % 28 // 2)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax0_ax1_ax2_ax3_fused % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(6912):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(576, ax0_ax1_ax2_ax3_fused // 12)
                                    v1 = T.axis.spatial(96, i4_0 * 12 + ax0_ax1_ax2_ax3_fused % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 8, 1, 1, 2, 1, 1, 1, 36, 1, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 14 * 288 + i1_3 * 36 + i1_4)
                                    yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                    xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i3_4)
                                    rc = T.axis.reduce(96, i4_0 * 12 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 288, 1, 2):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 14 * 288 + ax1)
                                v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax2)
                                v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 8, 36])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 6, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #24: "fused_nn_conv2d_add_nn_relu_15"
[23:28:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 576, 16, 16], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 16, 16):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 15 and 1 <= i3_1 and i3_1 < 15, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 576, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i * 2 + di, j * 2 + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i * 2 + di, j * 2 + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(129600):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, ax0_ax1_ax2_ax3_fused // 225)
                                    v2 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 225 // 15)
                                    v3 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused % 15)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(5184):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(576, ax0_ax1_ax2_ax3_fused // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 6, 1, 1, 3, 3, 1, 8, 1, 1):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_2_i1_2_i2_2_i3_2_fused // 7 * 48 + i1_3 * 8 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_2, i5_2])
                                    T.reads(PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 48, 1, 1):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(576, i0_2_i1_2_i2_2_i3_2_fused // 7 * 48 + ax1)
                                v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 12, 6, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #25: "fused_nn_conv2d_add_5"
[23:28:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 576, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 160, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 576, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 160, 7, 7, 576, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 7, 7], "float32"], ["TENSOR", [160, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 160, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:28:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([160, 576, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2352):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 48 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3840):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 80 + ax0_ax1_ax2_ax3_fused // 48)
                                    v1 = T.axis.spatial(576, i4_0 * 48 + ax0_ax1_ax2_ax3_fused % 48)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 10, 1, 7):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 80 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 40 + i0_2_i1_2_i2_2_i3_2_fused * 10 + i1_4)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i3_4)
                                    rc = T.axis.reduce(576, i4_0 * 48 + i4_1 * 6 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 7, 7], "float32"], ["TENSOR", [160, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 10, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(160, i0_0_i1_0_i2_0_i3_0_fused * 80 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 40 + i0_2_i1_2_i2_2_i3_2_fused * 10 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 4, 1, 10])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[12, 8, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:28:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #26: "fused_nn_conv2d_add_add_4"
[23:28:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 7, 7), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 160, 7, 7], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 160, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 160, 7, 7, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 160, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add_1[ax0, ax1, ax2, ax3])
                T_add_1[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 160, 7, 7):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add_1[ax0, ax1, ax2, ax3], placeholder_3[ax0, ax1, ax2, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_add_1[ax0, ax1, ax2, ax3] + placeholder_3[ax0, ax1, ax2, ax3]
    

[23:28:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 7, 7), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            conv2d_nchw_local = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([160, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(5, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(5, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(9408):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 192 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(30720):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(160, ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(960, i4_0 * 192 + ax0_ax1_ax2_ax3_fused % 192)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(96, 1, 1, 1, 1, 1, 7, 2, 1, 1, 1, 32, 7, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused * 32 + i1_4)
                                    yy, xx = T.axis.remap("SS", [i2_4, i3_3])
                                    rc = T.axis.reduce(960, i4_0 * 192 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 32, 7, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused * 32 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 5, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[5, 96, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #27: "fused_nn_conv2d_add_nn_relu_16"
[23:28:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 160, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 160, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 960, 7, 7, 160, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(10, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(784):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 16 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(7680):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + ax0_ax1_ax2_ax3_fused // 16)
                                    v1 = T.axis.spatial(160, i4_0 * 16 + ax0_ax1_ax2_ax3_fused % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 7, 8, 1, 1, 1, 3, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_4)
                                    yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    xx = T.axis.spatial(7, i3_3)
                                    rc = T.axis.reduce(160, i4_0 * 16 + i4_1 * 8 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 7):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + ax1)
                                v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 5, 32, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[10, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #28: "fused_nn_conv2d_add_nn_relu_17"
[23:28:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        PaddedInput = T.alloc_buffer([1, 960, 9, 9], dtype="float32")
        DepthwiseConv2d = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 9, 9):
            with T.block("PaddedInput"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(PaddedInput[i0_1, i1_1, i2_1, i3_1])
                PaddedInput[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 8 and 1 <= i3_1 and i3_1 < 8, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 960, 7, 7, 3, 3):
            with T.block("DepthwiseConv2d"):
                b, c, i, j, di, dj = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(PaddedInput[b, c, i + di, j + dj], placeholder_1[c, 0, di, dj])
                T.writes(DepthwiseConv2d[b, c, i, j])
                T.block_attr({"workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    DepthwiseConv2d[b, c, i, j] = T.float32(0)
                DepthwiseConv2d[b, c, i, j] = DepthwiseConv2d[b, c, i, j] + PaddedInput[b, c, i + di, j + dj] * placeholder_1[c, 0, di, dj]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(DepthwiseConv2d[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = DepthwiseConv2d[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            DepthwiseConv2d_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
            PaddedInput_shared = T.alloc_buffer([1, 960, 9, 9], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([960, 1, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(20, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(30240):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + ax0_ax1_ax2_ax3_fused % 30240 // 63)
                                    v2 = T.axis.spatial(9, i4_0 + ax0_ax1_ax2_ax3_fused % 63 // 9)
                                    v3 = T.axis.spatial(9, ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(1440):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + ax0_ax1_ax2_ax3_fused // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 1, 1, 1, 1, 6, 7, 7):
                                with T.block("DepthwiseConv2d"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i1_3 * 6 + i1_4)
                                    i, j, di, dj = T.axis.remap("SSRR", [i2_4, i3_4, i4_0, i5_1])
                                    T.reads(PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 24, 7, 7):
                            with T.block("DepthwiseConv2d_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused * 24 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 20, 1, 4, 6])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
[23:28:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #29: "fused_nn_conv2d_add_6"
[23:28:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(320, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_add: T.Buffer[(1, 320, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 960, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 320, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 960, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 320, 7, 7, 960, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 320, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
    

[23:28:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(320, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_add: T.Buffer[(1, 320, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            conv2d_nchw_local = T.alloc_buffer([1, 320, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([320, 960, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(560):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 80 + ax0_ax1_ax2_ax3_fused // 7)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(12800):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(320, i0_0_i1_0_i2_0_i3_0_fused // 7 * 160 + ax0_ax1_ax2_ax3_fused // 80)
                                    v1 = T.axis.spatial(960, i4_0 * 80 + ax0_ax1_ax2_ax3_fused % 80)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(40, 1, 1, 1, 40, 7, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(320, i0_0_i1_0_i2_0_i3_0_fused // 7 * 160 + i0_2_i1_2_i2_2_i3_2_fused * 40 + i1_3)
                                    yy = T.axis.spatial(7, i2_3)
                                    xx = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    rc = T.axis.reduce(960, i4_0 * 80 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 40, 7, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(320, i0_0_i1_0_i2_0_i3_0_fused // 7 * 160 + i0_2_i1_2_i2_2_i3_2_fused * 40 + ax1)
                                v2 = T.axis.spatial(7, ax2)
                                v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_add[v0, v1, v2, v3])
                                T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 4, 40, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[12, 40, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
[23:28:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #30: "fused_nn_conv2d_add_nn_relu_18"
[23:28:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7), "float32"], placeholder_1: T.Buffer[(1280, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1280, 1, 1), "float32"], T_relu: T.Buffer[(1, 1280, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 320, 7, 7], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 1280, 7, 7], dtype="float32")
        T_add = T.alloc_buffer([1, 1280, 7, 7], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 320, 7, 7):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1280, 7, 7, 320, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 7, 7], "float32"], ["TENSOR", [1280, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 1280, 7, 7):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 1280, 7, 7):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[23:28:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 320, 7, 7), "float32"], placeholder_1: T.Buffer[(1280, 320, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 1280, 1, 1), "float32"], T_relu: T.Buffer[(1, 1280, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            conv2d_nchw_local = T.alloc_buffer([1, 1280, 7, 7], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 320, 7, 7], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1280, 320, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(40, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(14, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(80, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(196):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(320, i4_0 * 4 + ax0_ax1_ax2_ax3_fused // 49)
                                    v2 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 49 // 7)
                                    v3 = T.axis.spatial(7, ax0_ax1_ax2_ax3_fused % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(128):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1280, i0_0_i1_0_i2_0_i3_0_fused * 32 + ax0_ax1_ax2_ax3_fused // 4)
                                    v1 = T.axis.spatial(320, i4_0 * 4 + ax0_ax1_ax2_ax3_fused % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1280, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    yy = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(320, i4_0 * 4 + i4_1 * 2 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 320, 7, 7], "float32"], ["TENSOR", [1280, 320, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1280, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                                v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                                v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[40, 16, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[80, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[23:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #31: "fused_nn_global_avg_pool2d"
[23:28:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        tensor_1 = T.alloc_buffer([1, 1280, 1, 1], dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 1280, 1, 1, 7, 7):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1]
        for i0, i1, i2, i3 in T.grid(1, 1280, 1, 1):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor_1[ax0, ax1, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

[23:28:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 2 design space(s) generated
[23:28:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            tensor_shared = T.alloc_buffer([1, 1280, 1, 1], dtype="float32", scope="shared")
            for i0, i1, i2, i3_0 in T.grid(1, 1280, 1, 1):
                for ax0, ax1, ax2, ax3, ax4_ax5_fused_0 in T.grid(1, 1, 1, 1, 13):
                    for ax4_ax5_fused_1 in T.thread_binding(4, thread="threadIdx.x"):
                        with T.block("tensor"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(1280, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            ax3_1 = T.axis.spatial(1, 0)
                            rv0 = T.axis.reduce(7, (ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1) // 7)
                            rv1 = T.axis.reduce(7, (ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1) % 7)
                            T.where(ax4_ax5_fused_0 * 4 + ax4_ax5_fused_1 < 49)
                            T.reads(placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1])
                            T.writes(tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1])
                            with T.init():
                                tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = T.float32(0)
                            tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] = tensor_shared[ax0_1, ax1_1, ax2_1, ax3_1] + placeholder[ax0_1, ax1_1, ax2_1 * 7 + rv0, ax3_1 * 7 + rv1]
                for i3_1 in T.thread_binding(4, thread="threadIdx.x"):
                    with T.block("tensor_1"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(1280, i1)
                        ax2 = T.axis.spatial(1, 0)
                        ax3 = T.axis.spatial(1, 0)
                        T.where(i3_1 < 1)
                        T.reads(tensor_shared[ax0, ax1, ax2, ax3])
                        T.writes(tensor[ax0, ax1, ax2, ax3])
                        tensor[ax0, ax1, ax2, ax3] = tensor_shared[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="tensor", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5, l6 = sch.get_loops(block=b2)
v7 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=0)
l8, l9 = sch.split(loop=l6, factors=[None, v7])
sch.bind(loop=l9, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l8, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l10, l11, l12, l13, l14, l15, l16, l17, l18, l19 = sch.get_loops(block=b0)
l20 = sch.fuse(l18, l19)
l21, l22 = sch.split(loop=l20, factors=[None, v7])
sch.bind(loop=l22, thread_axis="threadIdx.x")
v23 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v23)
[23:28:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 7, 7), "float32"], tensor: T.Buffer[(1, 1280, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            tensor_1 = T.alloc_buffer([1, 1280, 1, 1], dtype="float32")
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 1280, 1, 1, 7, 7):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1])
                    T.writes(tensor_1[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor_1[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor_1[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] + placeholder[ax0, ax1, ax2 * 7 + rv0, ax3 * 7 + rv1]
            for i0, i1, i2, i3 in T.grid(1, 1280, 1, 1):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(tensor_1[ax0, ax1, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    tensor[ax0, ax1, ax2, ax3] = tensor_1[ax0, ax1, ax2, ax3] * T.float32(0.020408163265306121)
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:28:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #32: "fused_nn_conv2d"
[23:28:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 1, 1), "float32"], placeholder_1: T.Buffer[(1000, 1280, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 1280, 1, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 1280, 1, 1):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 1000, 1, 1, 1280, 1, 1):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1280, 1, 1], "float32"], ["TENSOR", [1000, 1280, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
    

[23:28:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 1, 1), "float32"], placeholder_1: T.Buffer[(1000, 1280, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            conv2d_nchw_local = T.alloc_buffer([1, 1000, 1, 1], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 1280, 1, 1], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([1000, 1280, 1, 1], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(25, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(20, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(64):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1280, i4_0 * 64 + ax0_ax1_ax2_ax3_fused)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(2560):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 40 + ax0_ax1_ax2_ax3_fused // 64)
                                    v1 = T.axis.spatial(1280, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 64)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 8, 1, 1, 16, 1, 1, 1, 5, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 40 + i1_3 * 5 + i1_4)
                                    yy = T.axis.spatial(1, 0)
                                    xx = T.axis.spatial(1, 0)
                                    rc = T.axis.reduce(1280, i4_0 * 64 + i4_1 * 16 + i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1280, 1, 1], "float32"], ["TENSOR", [1000, 1280, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 40, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(1000, i0_0_i1_0_i2_0_i3_0_fused * 40 + ax1)
                                v2, v3 = T.axis.remap("SS", [ax2, ax3])
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                                T.writes(conv2d_nchw[v0, v1, v2, v3])
                                conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[25, 1, 1, 8, 5])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[20, 4, 16])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
[23:28:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:97: Initializing Task #33: "fused_reshape"
[23:28:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:103: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 1000):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 1000, 0, 0])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 1000, 0, 0]
    

[23:28:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:107: Total 1 design space(s) generated
[23:28:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:112: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1000, 1, 1), "float32"], T_reshape: T.Buffer[(1, 1000), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1 in T.grid(1, 1000):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 1000, 0, 0])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 1000, 0, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[23:28:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[23:28:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[23:28:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:28:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:30:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70ea998)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70eae58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d66f90a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70ec478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70ec538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d744c158)]: 1868 failure(s)
[23:30:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 180 candidate(s)
[23:32:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70ea998)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70eae58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d66f90a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70ec478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70ec538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d744c158)]: 303 failure(s)
[23:34:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70ea998)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70eae58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d66f90a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70ec478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70ec538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d744c158)]: 301 failure(s)
[23:37:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70ea998)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70eae58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d66f90a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70ec478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70ec538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d744c158)]: 243 failure(s)
[23:40:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70ea998)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70eae58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d66f90a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70ec478)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70ec538)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d744c158)]: 261 failure(s)
[23:40:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9994  0.9989  0.9984  0.9983  0.9982  0.9980  0.9978  0.9978  0.9977  0.9974  0.9974  0.9972  0.9972  0.9968  0.9968
[17 : 32]:	0.9965  0.9965  0.9964  0.9964  0.9962  0.9961  0.9957  0.9956  0.9954  0.9953  0.9946  0.9936  0.9935  0.9934  0.9934  0.9932
[23:40:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:40:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:40:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:41:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:41:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu_1"
[23:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:41:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7f9f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bef928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e9d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709c838)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7423788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7140c58)]: 1882 failure(s)
[23:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 166 candidate(s)
[23:43:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7f9f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bef928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e9d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709c838)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7423788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7140c58)]: 447 failure(s)
[23:45:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7f9f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bef928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e9d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709c838)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7423788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7140c58)]: 393 failure(s)
[23:48:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7f9f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bef928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e9d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709c838)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7423788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7140c58)]: 402 failure(s)
[23:49:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7f9f948)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bef928)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e9d08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709c838)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7423788)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7140c58)]: 382 failure(s)
[23:50:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9996  0.9996  0.9995  0.9993  0.9989  0.9989  0.9988  0.9987  0.9983  0.9981  0.9976  0.9976  0.9974  0.9972  0.9972
[17 : 32]:	0.9972  0.9971  0.9970  0.9965  0.9962  0.9960  0.9959  0.9959  0.9959  0.9959  0.9957  0.9956  0.9956  0.9953  0.9952  0.9951
[23:50:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[23:50:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[23:51:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[23:51:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[23:51:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_nn_relu_2"
[23:51:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[23:51:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[23:53:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7123578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f83278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70ec8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d77f42b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7260c88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e9568)]: 1966 failure(s)
[23:53:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 82 candidate(s)
[23:56:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7123578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f83278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70ec8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d77f42b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7260c88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e9568)]: 383 failure(s)
[23:59:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7123578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f83278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70ec8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d77f42b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7260c88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e9568)]: 340 failure(s)
[00:02:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7123578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f83278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70ec8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d77f42b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7260c88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e9568)]: 309 failure(s)
[00:05:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7123578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f83278)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70ec8a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d77f42b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7260c88)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e9568)]: 320 failure(s)
[00:06:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9995  0.9992  0.9990  0.9988  0.9987  0.9986  0.9986  0.9984  0.9984  0.9981  0.9980  0.9976  0.9973  0.9973
[17 : 32]:	0.9972  0.9971  0.9971  0.9968  0.9968  0.9966  0.9965  0.9963  0.9963  0.9955  0.9954  0.9953  0.9953  0.9952  0.9949  0.9948
[00:06:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:06:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:06:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:07:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:07:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add"
[00:07:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:07:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:08:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d66ea778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7194b38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d781e208)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc19d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d721ea68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7187228)]: 1936 failure(s)
[00:08:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 112 candidate(s)
[00:09:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d66ea778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7194b38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d781e208)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc19d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d721ea68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7187228)]: 458 failure(s)
[00:11:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d66ea778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7194b38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d781e208)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc19d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d721ea68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7187228)]: 441 failure(s)
[00:13:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d66ea778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7194b38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d781e208)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc19d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d721ea68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7187228)]: 393 failure(s)
[00:15:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d66ea778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7194b38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d781e208)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc19d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d721ea68)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7187228)]: 434 failure(s)
[00:16:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9995  0.9994  0.9994  0.9994  0.9989  0.9989  0.9988  0.9987  0.9985  0.9985  0.9981  0.9981  0.9980  0.9979  0.9979  0.9978
[17 : 32]:	0.9978  0.9975  0.9973  0.9970  0.9963  0.9962  0.9956  0.9956  0.9954  0.9952  0.9951  0.9949  0.9949  0.9947  0.9947  0.9945
[00:16:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:16:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:16:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:16:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:16:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_3"
[00:16:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:16:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:17:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71db068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d709c098)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7f82cc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7166898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76f5ce8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7186ef8)]: 1855 failure(s)
[00:17:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 193 candidate(s)
[00:19:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71db068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d709c098)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7f82cc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7166898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76f5ce8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7186ef8)]: 386 failure(s)
[00:20:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71db068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d709c098)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7f82cc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7166898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76f5ce8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7186ef8)]: 331 failure(s)
[00:22:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71db068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d709c098)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7f82cc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7166898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76f5ce8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7186ef8)]: 328 failure(s)
[00:24:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71db068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d709c098)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7f82cc8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7166898)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76f5ce8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7186ef8)]: 349 failure(s)
[00:24:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9996  0.9992  0.9989  0.9989  0.9986  0.9985  0.9981  0.9980  0.9978  0.9978  0.9972  0.9971  0.9970
[17 : 32]:	0.9970  0.9970  0.9970  0.9969  0.9967  0.9967  0.9964  0.9958  0.9958  0.9958  0.9956  0.9949  0.9949  0.9949  0.9948  0.9946
[00:25:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:25:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:25:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:25:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:25:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_relu_4"
[00:25:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:25:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:27:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 2021 failure(s)
[00:28:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 4057 failure(s)
[00:29:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 6083 failure(s)
[00:29:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 61 candidate(s)
[00:31:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 531 failure(s)
[00:33:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 460 failure(s)
[00:36:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 437 failure(s)
[00:39:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71897c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7117f78)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7286d38)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72606d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d72a7578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76e4768)]: 427 failure(s)
[00:40:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9996  0.9994  0.9993  0.9990  0.9988  0.9986  0.9984  0.9982  0.9979  0.9977  0.9974  0.9971  0.9971  0.9970  0.9965
[17 : 32]:	0.9960  0.9959  0.9959  0.9958  0.9957  0.9957  0.9952  0.9951  0.9951  0.9949  0.9947  0.9946  0.9945  0.9944  0.9943  0.9942
[00:40:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:40:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[00:40:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[00:40:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[00:41:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_1"
[00:41:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:41:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:42:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 1916 failure(s)
[00:42:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 132 candidate(s)
[00:43:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 454 failure(s)
[00:44:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 368 failure(s)
[00:46:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 366 failure(s)
[00:48:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 362 failure(s)
[00:49:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9996  0.9994  0.9994  0.9993  0.9988  0.9986  0.9983  0.9980  0.9979  0.9978  0.9978  0.9976  0.9975  0.9972  0.9971  0.9971
[17 : 32]:	0.9970  0.9969  0.9968  0.9963  0.9962  0.9961  0.9961  0.9960  0.9959  0.9956  0.9955  0.9955  0.9955  0.9953  0.9949  0.9949
[00:49:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[00:49:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 31 candidates(s) for measurement
[00:49:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 31 sample(s) to builder
[00:49:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 31 sample(s) to runner
[00:50:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_5"
[00:50:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[00:50:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[00:51:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d706e778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71bb3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70de6f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72304a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71acba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70dbf68)]: 1989 failure(s)
[00:51:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 59 candidate(s)
[00:54:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d706e778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71bb3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70de6f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72304a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71acba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70dbf68)]: 453 failure(s)
[00:57:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d706e778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71bb3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70de6f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72304a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71acba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70dbf68)]: 381 failure(s)
[01:00:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d706e778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71bb3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70de6f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72304a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71acba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70dbf68)]: 410 failure(s)
[01:04:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d706e778)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71bb3c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70de6f8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72304a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71acba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70dbf68)]: 386 failure(s)
[01:05:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9998  0.9997  0.9995  0.9994  0.9992  0.9991  0.9988  0.9986  0.9984  0.9984  0.9980  0.9980  0.9979  0.9973  0.9971
[17 : 32]:	0.9971  0.9969  0.9963  0.9962  0.9959  0.9957  0.9954  0.9953  0.9952  0.9952  0.9952  0.9948  0.9948  0.9941  0.9941  0.9941
[01:05:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:05:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:05:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:05:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_add"
[01:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:06:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d728a0c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d706ead8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7048aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7edeba8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f730e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d706af08)]: 1924 failure(s)
[01:06:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 124 candidate(s)
[01:08:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d728a0c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d706ead8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7048aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7edeba8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f730e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d706af08)]: 465 failure(s)
[01:10:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d728a0c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d706ead8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7048aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7edeba8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f730e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d706af08)]: 401 failure(s)
[01:12:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d728a0c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d706ead8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7048aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7edeba8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f730e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d706af08)]: 415 failure(s)
[01:14:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d728a0c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d706ead8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7048aa8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7edeba8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f730e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d706af08)]: 363 failure(s)
[01:14:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9987  0.9984  0.9982  0.9980  0.9980  0.9980  0.9980  0.9976  0.9974  0.9971  0.9970  0.9966  0.9965  0.9965
[17 : 32]:	0.9964  0.9963  0.9954  0.9951  0.9948  0.9948  0.9947  0.9944  0.9942  0.9935  0.9935  0.9929  0.9928  0.9927  0.9927  0.9927
[01:14:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:14:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:14:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:15:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:15:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_nn_relu_6"
[01:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:15:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:16:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7094408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76e6b58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d767aeb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c166e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7710908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7726ba8)]: 1741 failure(s)
[01:16:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 307 candidate(s)
[01:17:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7094408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76e6b58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d767aeb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c166e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7710908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7726ba8)]: 337 failure(s)
[01:19:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7094408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76e6b58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d767aeb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c166e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7710908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7726ba8)]: 322 failure(s)
[01:21:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7094408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76e6b58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d767aeb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c166e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7710908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7726ba8)]: 288 failure(s)
[01:22:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7094408)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76e6b58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d767aeb8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c166e8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7710908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7726ba8)]: 302 failure(s)
[01:23:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9997  0.9995  0.9994  0.9992  0.9991  0.9991  0.9989  0.9987  0.9984  0.9983  0.9983  0.9982  0.9979
[17 : 32]:	0.9978  0.9973  0.9973  0.9971  0.9967  0.9966  0.9965  0.9965  0.9960  0.9960  0.9959  0.9958  0.9956  0.9956  0.9952  0.9952
[01:23:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:23:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:23:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:24:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:24:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_conv2d_add_nn_relu_7"
[01:24:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:24:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:25:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d716f7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f5f668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71b1de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d673ed48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d711f868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d712d878)]: 1994 failure(s)
[01:25:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 54 candidate(s)
[01:28:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d716f7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f5f668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71b1de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d673ed48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d711f868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d712d878)]: 549 failure(s)
[01:31:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d716f7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f5f668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71b1de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d673ed48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d711f868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d712d878)]: 477 failure(s)
[01:34:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d716f7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f5f668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71b1de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d673ed48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d711f868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d712d878)]: 476 failure(s)
[01:36:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d716f7a8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f5f668)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71b1de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d673ed48)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d711f868)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d712d878)]: 454 failure(s)
[01:37:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9996  0.9994  0.9989  0.9989  0.9981  0.9979  0.9976  0.9973  0.9973  0.9972  0.9971  0.9970  0.9967  0.9966
[17 : 32]:	0.9966  0.9963  0.9962  0.9961  0.9960  0.9959  0.9958  0.9955  0.9955  0.9946  0.9946  0.9942  0.9939  0.9938  0.9938  0.9938
[01:37:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:37:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:37:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:37:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:38:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_2"
[01:38:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:38:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cd9a78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d716e778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d718ae78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7077e18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f2b298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70f54d8)]: 1872 failure(s)
[01:38:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 176 candidate(s)
[01:39:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cd9a78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d716e778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d718ae78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7077e18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f2b298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70f54d8)]: 397 failure(s)
[01:41:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cd9a78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d716e778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d718ae78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7077e18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f2b298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70f54d8)]: 362 failure(s)
[01:42:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cd9a78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d716e778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d718ae78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7077e18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f2b298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70f54d8)]: 347 failure(s)
[01:44:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cd9a78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d716e778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d718ae78)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7077e18)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f2b298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70f54d8)]: 318 failure(s)
[01:44:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9996  0.9994  0.9986  0.9985  0.9985  0.9983  0.9983  0.9976  0.9966  0.9963  0.9963  0.9962  0.9962  0.9961
[17 : 32]:	0.9960  0.9960  0.9957  0.9954  0.9954  0.9952  0.9952  0.9949  0.9949  0.9944  0.9944  0.9941  0.9940  0.9940  0.9940  0.9940
[01:45:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:45:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:45:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_add_1"
[01:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:46:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7c055f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d703ab98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d8010928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70b5d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7803d38)]: 1887 failure(s)
[01:46:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 161 candidate(s)
[01:47:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7c055f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d703ab98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d8010928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70b5d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7803d38)]: 402 failure(s)
[01:49:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7c055f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d703ab98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d8010928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70b5d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7803d38)]: 384 failure(s)
[01:51:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7c055f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d703ab98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d8010928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70b5d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7803d38)]: 371 failure(s)
[01:53:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7c055f8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d703ab98)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d8010928)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70b5d98)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7803d38)]: 324 failure(s)
[01:54:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9996  0.9995  0.9990  0.9986  0.9986  0.9985  0.9983  0.9982  0.9981  0.9980  0.9979  0.9973  0.9973  0.9969
[17 : 32]:	0.9966  0.9965  0.9965  0.9964  0.9963  0.9961  0.9961  0.9958  0.9958  0.9956  0.9955  0.9951  0.9947  0.9939  0.9939  0.9937
[01:54:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[01:54:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[01:54:39] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[01:55:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[01:55:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_relu_8"
[01:55:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[01:55:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[01:56:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7187958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76eb208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76d3668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d71119a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70dccd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d713dba8)]: 1706 failure(s)
[01:56:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 342 candidate(s)
[01:58:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7187958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76eb208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76d3668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d71119a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70dccd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d713dba8)]: 311 failure(s)
[01:59:38] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7187958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76eb208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76d3668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d71119a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70dccd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d713dba8)]: 306 failure(s)
[02:01:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7187958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76eb208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76d3668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d71119a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70dccd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d713dba8)]: 261 failure(s)
[02:03:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7187958)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76eb208)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76d3668)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d71119a8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70dccd8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d713dba8)]: 289 failure(s)
[02:04:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9991  0.9980  0.9979  0.9978  0.9975  0.9974  0.9973  0.9973  0.9973  0.9969  0.9969  0.9968  0.9967  0.9967  0.9964
[17 : 32]:	0.9964  0.9964  0.9962  0.9962  0.9961  0.9961  0.9961  0.9956  0.9955  0.9955  0.9952  0.9952  0.9952  0.9952  0.9951  0.9950
[02:04:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:04:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:04:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:05:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:05:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_conv2d_add_nn_relu_9"
[02:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:05:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d77ea478)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70ac9e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d700c238)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700e8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f19cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7230858)]: 1939 failure(s)
[02:06:40] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 109 candidate(s)
[02:09:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d77ea478)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70ac9e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d700c238)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700e8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f19cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7230858)]: 415 failure(s)
[02:11:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d77ea478)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70ac9e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d700c238)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700e8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f19cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7230858)]: 373 failure(s)
[02:14:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d77ea478)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70ac9e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d700c238)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700e8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f19cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7230858)]: 313 failure(s)
[02:17:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d77ea478)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d70ac9e8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d700c238)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700e8c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7f19cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7230858)]: 335 failure(s)
[02:18:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  1.0000  1.0000  0.9997  0.9996  0.9995  0.9993  0.9992  0.9991  0.9989  0.9988  0.9986  0.9982  0.9981  0.9974  0.9973
[17 : 32]:	0.9973  0.9972  0.9970  0.9969  0.9968  0.9967  0.9967  0.9965  0.9963  0.9961  0.9959  0.9957  0.9956  0.9955  0.9949  0.9949
[02:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:18:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:18:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:19:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:19:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_conv2d_add_3"
[02:19:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:19:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:20:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70c8888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77413a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7bdba58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc0d58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739f78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70c1948)]: 1876 failure(s)
[02:20:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 172 candidate(s)
[02:21:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70c8888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77413a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7bdba58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc0d58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739f78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70c1948)]: 393 failure(s)
[02:23:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70c8888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77413a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7bdba58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc0d58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739f78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70c1948)]: 395 failure(s)
[02:25:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70c8888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77413a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7bdba58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc0d58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739f78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70c1948)]: 358 failure(s)
[02:26:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70c8888)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77413a8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7bdba58)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fc0d58)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739f78)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70c1948)]: 367 failure(s)
[02:27:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9996  0.9996  0.9995  0.9995  0.9994  0.9992  0.9991  0.9990  0.9988  0.9988  0.9988  0.9986  0.9985  0.9979  0.9978
[17 : 32]:	0.9978  0.9977  0.9974  0.9973  0.9971  0.9967  0.9965  0.9961  0.9958  0.9956  0.9955  0.9953  0.9952  0.9952  0.9951  0.9951
[02:27:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:27:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:27:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:28:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:28:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_nn_relu_10"
[02:28:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:28:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:29:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d712c818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d771e3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d717ef08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ee1b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70f6688)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70ba898)]: 1983 failure(s)
[02:29:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 65 candidate(s)
[02:32:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d712c818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d771e3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d717ef08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ee1b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70f6688)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70ba898)]: 454 failure(s)
[02:34:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d712c818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d771e3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d717ef08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ee1b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70f6688)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70ba898)]: 358 failure(s)
[02:37:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d712c818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d771e3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d717ef08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ee1b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70f6688)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70ba898)]: 346 failure(s)
[02:39:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d712c818)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d771e3b8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d717ef08)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ee1b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70f6688)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70ba898)]: 315 failure(s)
[02:39:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9998  0.9992  0.9991  0.9991  0.9989  0.9989  0.9988  0.9986  0.9985  0.9984  0.9982  0.9982  0.9979  0.9976  0.9976
[17 : 32]:	0.9973  0.9972  0.9972  0.9958  0.9958  0.9957  0.9952  0.9952  0.9950  0.9949  0.9947  0.9944  0.9943  0.9943  0.9942  0.9935
[02:39:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:39:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:39:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:40:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:40:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_add_2"
[02:40:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:40:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:41:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7827f38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7b70d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76b65b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7151408)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6feb458)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7627f78)]: 1901 failure(s)
[02:41:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 147 candidate(s)
[02:42:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7827f38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7b70d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76b65b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7151408)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6feb458)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7627f78)]: 482 failure(s)
[02:44:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7827f38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7b70d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76b65b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7151408)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6feb458)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7627f78)]: 380 failure(s)
[02:45:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7827f38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7b70d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76b65b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7151408)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6feb458)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7627f78)]: 333 failure(s)
[02:47:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7827f38)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7b70d38)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76b65b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7151408)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6feb458)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7627f78)]: 364 failure(s)
[02:47:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9998  0.9997  0.9997  0.9996  0.9996  0.9996  0.9996  0.9995  0.9989  0.9988  0.9988  0.9986  0.9983  0.9983  0.9981
[17 : 32]:	0.9980  0.9972  0.9971  0.9970  0.9967  0.9964  0.9962  0.9962  0.9961  0.9959  0.9958  0.9954  0.9953  0.9953  0.9951  0.9950
[02:48:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:48:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:48:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:48:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:48:58] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_nn_relu_11"
[02:48:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:48:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:49:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f59db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d721d248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7429be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fbdfa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d66f6cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d766c438)]: 1744 failure(s)
[02:49:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 304 candidate(s)
[02:50:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f59db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d721d248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7429be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fbdfa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d66f6cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d766c438)]: 341 failure(s)
[02:52:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f59db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d721d248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7429be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fbdfa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d66f6cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d766c438)]: 317 failure(s)
[02:53:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f59db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d721d248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7429be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fbdfa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d66f6cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d766c438)]: 350 failure(s)
[02:54:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f59db8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d721d248)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7429be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fbdfa8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d66f6cc8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d766c438)]: 333 failure(s)
[02:55:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9998  0.9995  0.9992  0.9992  0.9991  0.9990  0.9987  0.9987  0.9985  0.9981  0.9981  0.9978  0.9978  0.9977
[17 : 32]:	0.9977  0.9976  0.9974  0.9973  0.9973  0.9973  0.9973  0.9972  0.9971  0.9970  0.9969  0.9964  0.9962  0.9962  0.9959  0.9956
[02:55:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[02:55:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[02:55:29] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[02:56:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[02:56:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_conv2d_add_nn_relu_12"
[02:56:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[02:56:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[02:57:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7165c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7243528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d77fc3b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c0f868)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786f628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7684e48)]: 1975 failure(s)
[02:57:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 73 candidate(s)
[02:59:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7165c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7243528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d77fc3b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c0f868)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786f628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7684e48)]: 614 failure(s)
[03:01:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7165c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7243528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d77fc3b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c0f868)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786f628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7684e48)]: 454 failure(s)
[03:03:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7165c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7243528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d77fc3b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c0f868)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786f628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7684e48)]: 461 failure(s)
[03:05:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7165c98)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7243528)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d77fc3b8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c0f868)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786f628)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7684e48)]: 435 failure(s)
[03:05:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9993  0.9992  0.9991  0.9991  0.9990  0.9988  0.9985  0.9983  0.9982  0.9982  0.9981  0.9981  0.9979  0.9977
[17 : 32]:	0.9977  0.9974  0.9973  0.9970  0.9969  0.9969  0.9969  0.9968  0.9968  0.9968  0.9966  0.9965  0.9965  0.9964  0.9963  0.9961
[03:06:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:06:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:06:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:06:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:06:50] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add_4"
[03:06:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:06:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:07:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d717b9b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7cd9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7428fd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70fdc88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70d88f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76a68d8)]: 1859 failure(s)
[03:07:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 189 candidate(s)
[03:08:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d717b9b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7cd9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7428fd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70fdc88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70d88f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76a68d8)]: 392 failure(s)
[03:09:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d717b9b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7cd9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7428fd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70fdc88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70d88f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76a68d8)]: 303 failure(s)
[03:10:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d717b9b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7cd9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7428fd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70fdc88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70d88f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76a68d8)]: 361 failure(s)
[03:11:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d717b9b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7cd9838)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7428fd8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70fdc88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d70d88f8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d76a68d8)]: 323 failure(s)
[03:12:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9997  0.9994  0.9993  0.9992  0.9989  0.9983  0.9982  0.9982  0.9981  0.9981  0.9981  0.9980  0.9979  0.9975  0.9974
[17 : 32]:	0.9974  0.9973  0.9973  0.9972  0.9972  0.9971  0.9971  0.9970  0.9962  0.9960  0.9959  0.9959  0.9958  0.9957  0.9955  0.9953
[03:12:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:12:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:12:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:13:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:13:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_relu_13"
[03:13:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:13:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70cb0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bb7878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72f61e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70a89d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6f63c28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6689b08)]: 1942 failure(s)
[03:14:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 106 candidate(s)
[03:16:11] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70cb0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bb7878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72f61e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70a89d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6f63c28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6689b08)]: 435 failure(s)
[03:18:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70cb0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bb7878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72f61e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70a89d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6f63c28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6689b08)]: 354 failure(s)
[03:20:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70cb0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bb7878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72f61e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70a89d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6f63c28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6689b08)]: 408 failure(s)
[03:21:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70cb0e8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7bb7878)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72f61e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d70a89d8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6f63c28)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6689b08)]: 342 failure(s)
[03:22:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9996  0.9990  0.9989  0.9988  0.9988  0.9985  0.9984  0.9976  0.9976  0.9971  0.9971  0.9970  0.9969  0.9969  0.9968
[17 : 32]:	0.9964  0.9963  0.9962  0.9960  0.9960  0.9958  0.9957  0.9954  0.9953  0.9945  0.9944  0.9942  0.9940  0.9937  0.9937  0.9932
[03:22:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:22:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:22:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:22:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:23:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_conv2d_add_add_3"
[03:23:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:23:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f6a578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f6a998)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d741c608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ed8308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d712e908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6f8a198)]: 1897 failure(s)
[03:23:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 151 candidate(s)
[03:24:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f6a578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f6a998)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d741c608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ed8308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d712e908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6f8a198)]: 447 failure(s)
[03:25:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f6a578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f6a998)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d741c608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ed8308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d712e908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6f8a198)]: 372 failure(s)
[03:26:58] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f6a578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f6a998)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d741c608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ed8308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d712e908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6f8a198)]: 381 failure(s)
[03:28:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f6a578)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6f6a998)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d741c608)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7ed8308)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d712e908)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d6f8a198)]: 346 failure(s)
[03:28:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9994  0.9989  0.9987  0.9983  0.9983  0.9979  0.9979  0.9979  0.9976  0.9974  0.9974  0.9974  0.9973  0.9970
[17 : 32]:	0.9968  0.9962  0.9961  0.9957  0.9952  0.9951  0.9951  0.9950  0.9948  0.9947  0.9941  0.9934  0.9931  0.9930  0.9929  0.9928
[03:28:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:28:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:28:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:29:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:29:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_conv2d_add_nn_relu_14"
[03:29:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:29:14] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:29:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d704f088)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71c6378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d702d218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f24c08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7283078)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d71602c8)]: 1753 failure(s)
[03:29:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 295 candidate(s)
[03:30:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d704f088)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71c6378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d702d218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f24c08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7283078)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d71602c8)]: 343 failure(s)
[03:31:44] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d704f088)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71c6378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d702d218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f24c08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7283078)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d71602c8)]: 308 failure(s)
[03:32:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d704f088)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71c6378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d702d218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f24c08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7283078)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d71602c8)]: 323 failure(s)
[03:33:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d704f088)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71c6378)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d702d218)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f24c08)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7283078)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d71602c8)]: 326 failure(s)
[03:33:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  1.0000  0.9999  0.9998  0.9997  0.9997  0.9997  0.9995  0.9990  0.9985  0.9985  0.9983  0.9982  0.9978  0.9977  0.9973
[17 : 32]:	0.9972  0.9971  0.9970  0.9967  0.9967  0.9966  0.9963  0.9962  0.9962  0.9962  0.9959  0.9955  0.9955  0.9954  0.9953  0.9953
[03:33:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:33:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:34:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:34:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:34:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_conv2d_add_nn_relu_15"
[03:34:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:34:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:35:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d658a6d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76b7bd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e8388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700c4f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7198df8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d80089a8)]: 1943 failure(s)
[03:35:24] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 105 candidate(s)
[03:36:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d658a6d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76b7bd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e8388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700c4f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7198df8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d80089a8)]: 751 failure(s)
[03:38:18] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d658a6d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76b7bd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e8388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700c4f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7198df8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d80089a8)]: 632 failure(s)
[03:39:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d658a6d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76b7bd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e8388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700c4f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7198df8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d80089a8)]: 596 failure(s)
[03:40:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d658a6d8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76b7bd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e8388)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d700c4f8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7198df8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d80089a8)]: 622 failure(s)
[03:41:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9999  0.9998  0.9997  0.9995  0.9994  0.9994  0.9991  0.9991  0.9990  0.9988  0.9986  0.9986  0.9982  0.9981  0.9979
[17 : 32]:	0.9979  0.9975  0.9974  0.9972  0.9972  0.9971  0.9971  0.9967  0.9967  0.9966  0.9960  0.9956  0.9956  0.9953  0.9952  0.9951
[03:41:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:41:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:41:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:41:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:41:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_conv2d_add_5"
[03:41:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:41:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:42:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fd2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7c22e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e7de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fcb218)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76c8aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d711e718)]: 1862 failure(s)
[03:42:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 186 candidate(s)
[03:42:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fd2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7c22e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e7de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fcb218)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76c8aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d711e718)]: 502 failure(s)
[03:43:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fd2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7c22e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e7de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fcb218)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76c8aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d711e718)]: 436 failure(s)
[03:44:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fd2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7c22e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e7de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fcb218)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76c8aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d711e718)]: 473 failure(s)
[03:45:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fd2f18)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7c22e28)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d70e7de8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7fcb218)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d76c8aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d711e718)]: 429 failure(s)
[03:45:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9996  0.9995  0.9993  0.9993  0.9993  0.9988  0.9988  0.9986  0.9986  0.9983  0.9983  0.9983  0.9983  0.9980
[17 : 32]:	0.9979  0.9979  0.9977  0.9977  0.9976  0.9975  0.9975  0.9973  0.9971  0.9969  0.9967  0.9966  0.9965  0.9963  0.9963  0.9963
[03:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:45:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:45:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:46:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:46:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_conv2d_add_add_4"
[03:46:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:46:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:46:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fec0b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6fb9dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d80481a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f55b88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7252298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e7fc8)]: 1901 failure(s)
[03:46:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 147 candidate(s)
[03:47:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fec0b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6fb9dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d80481a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f55b88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7252298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e7fc8)]: 528 failure(s)
[03:48:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fec0b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6fb9dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d80481a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f55b88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7252298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e7fc8)]: 498 failure(s)
[03:49:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fec0b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6fb9dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d80481a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f55b88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7252298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e7fc8)]: 413 failure(s)
[03:49:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6fec0b8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d6fb9dd8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d80481a8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6f55b88)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7252298)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e7fc8)]: 424 failure(s)
[03:50:13] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9997  0.9995  0.9986  0.9985  0.9982  0.9982  0.9976  0.9974  0.9973  0.9972  0.9971  0.9966  0.9963  0.9962
[17 : 32]:	0.9961  0.9959  0.9959  0.9958  0.9957  0.9953  0.9949  0.9949  0.9944  0.9941  0.9940  0.9936  0.9934  0.9934  0.9930  0.9927
[03:50:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:50:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:50:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:50:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[03:50:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_conv2d_add_nn_relu_16"
[03:50:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:50:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:51:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f45cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76be318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7673798)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d658a128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786fbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7f1cbd8)]: 1810 failure(s)
[03:51:17] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 238 candidate(s)
[03:51:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f45cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76be318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7673798)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d658a128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786fbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7f1cbd8)]: 472 failure(s)
[03:52:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f45cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76be318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7673798)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d658a128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786fbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7f1cbd8)]: 402 failure(s)
[03:53:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f45cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76be318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7673798)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d658a128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786fbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7f1cbd8)]: 386 failure(s)
[03:54:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d6f45cf8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d76be318)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7673798)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d658a128)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d786fbb8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7f1cbd8)]: 378 failure(s)
[03:54:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9997  0.9994  0.9994  0.9994  0.9994  0.9992  0.9990  0.9990  0.9989  0.9988  0.9988  0.9987  0.9986  0.9980  0.9977  0.9975
[17 : 32]:	0.9975  0.9973  0.9969  0.9968  0.9968  0.9968  0.9968  0.9965  0.9959  0.9959  0.9958  0.9957  0.9955  0.9953  0.9952  0.9952
[03:54:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[03:54:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[03:54:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[03:55:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [03:55:19] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x00005576ce6d8554
  69: 0xffffffffffffffff


[03:55:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_conv2d_add_nn_relu_17"
[03:55:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[03:55:19] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[03:55:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70b5b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d722de58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7090ed8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7721558)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d705faf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7090f08)]: 1891 failure(s)
[03:55:54] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 157 candidate(s)
[03:57:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70b5b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d722de58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7090ed8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7721558)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d705faf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7090f08)]: 485 failure(s)
[03:58:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70b5b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d722de58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7090ed8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7721558)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d705faf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7090f08)]: 394 failure(s)
[03:59:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70b5b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d722de58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7090ed8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7721558)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d705faf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7090f08)]: 377 failure(s)
[04:00:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70b5b48)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d722de58)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7090ed8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7721558)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d705faf8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7090f08)]: 366 failure(s)
[04:01:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9998  0.9997  0.9995  0.9993  0.9992  0.9990  0.9990  0.9982  0.9982  0.9982  0.9981  0.9981  0.9980  0.9979  0.9978  0.9978
[17 : 32]:	0.9977  0.9977  0.9976  0.9975  0.9974  0.9973  0.9972  0.9972  0.9972  0.9970  0.9967  0.9965  0.9961  0.9960  0.9959  0.9959
[04:01:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:01:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:01:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:01:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:01:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_conv2d_add_6"
[04:01:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:01:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:02:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d743fa78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d72fe4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72521e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709f788)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7c87f48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7252218)]: 1894 failure(s)
[04:02:10] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 154 candidate(s)
[04:02:37] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d743fa78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d72fe4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72521e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709f788)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7c87f48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7252218)]: 526 failure(s)
[04:03:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d743fa78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d72fe4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72521e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709f788)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7c87f48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7252218)]: 476 failure(s)
[04:03:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d743fa78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d72fe4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72521e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709f788)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7c87f48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7252218)]: 437 failure(s)
[04:04:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d743fa78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d72fe4f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d72521e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d709f788)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7c87f48)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7252218)]: 388 failure(s)
[04:04:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9997  0.9996  0.9996  0.9995  0.9993  0.9993  0.9989  0.9988  0.9985  0.9980  0.9979  0.9977  0.9977  0.9975
[17 : 32]:	0.9975  0.9973  0.9972  0.9971  0.9971  0.9970  0.9968  0.9966  0.9963  0.9962  0.9961  0.9956  0.9955  0.9954  0.9954  0.9952
[04:05:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:05:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:05:03] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:05:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [04:05:24] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x0000558b6de74554
  69: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [04:05:33] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x000055e339959554
  69: 0xffffffffffffffff


[04:05:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #30: "fused_nn_conv2d_add_nn_relu_18"
[04:05:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:05:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:05:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cc9e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71b9778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71c3be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c43b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d72f5ba8)]: 1874 failure(s)
[04:05:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 174 candidate(s)
[04:06:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cc9e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71b9778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71c3be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c43b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d72f5ba8)]: 480 failure(s)
[04:07:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cc9e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71b9778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71c3be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c43b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d72f5ba8)]: 428 failure(s)
[04:07:36] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cc9e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71b9778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71c3be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c43b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d72f5ba8)]: 410 failure(s)
[04:08:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7cc9e78)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71b9778)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d71c3be8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c43b68)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6fb29a8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d72f5ba8)]: 406 failure(s)
[04:08:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9999  0.9999  0.9999  0.9997  0.9994  0.9993  0.9993  0.9992  0.9991  0.9987  0.9983  0.9978  0.9977  0.9976  0.9975  0.9974
[17 : 32]:	0.9972  0.9967  0.9965  0.9964  0.9964  0.9963  0.9960  0.9958  0.9955  0.9955  0.9954  0.9952  0.9952  0.9947  0.9947  0.9945
[04:08:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:08:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:08:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:08:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:08:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #31: "fused_nn_global_avg_pool2d"
[04:08:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:08:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7702338)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7282c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7085df8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72f6398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7102578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e8b68)]: 0 failure(s)
[04:08:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:09:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7702338)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7282c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7085df8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72f6398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7102578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e8b68)]: 0 failure(s)
[04:09:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7702338)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7282c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7085df8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72f6398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7102578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e8b68)]: 0 failure(s)
[04:09:21] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7702338)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7282c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7085df8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72f6398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7102578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e8b68)]: 0 failure(s)
[04:09:25] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d7702338)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d7282c68)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7085df8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d72f6398)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7102578)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d70e8b68)]: 0 failure(s)
[04:09:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	0.9747  0.9578  0.9317  0.9303  0.9011  0.8980  0.8978  0.8170  0.8014  0.7868  0.7303  0.7033  0.6862  0.6711  0.6681  0.6118
[17 : 32]:	0.6055  0.6021  0.5945  0.5915  0.5876  0.5640  0.5548  0.5432  0.5371  0.4921  0.4582  0.4358  0.3926  0.3872  0.3583  0.3178
[04:09:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:09:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:09:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:09:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
[04:09:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #32: "fused_nn_conv2d"
[04:09:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:09:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:09:51] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 2009 failure(s)
[04:10:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 4010 failure(s)
[04:10:01] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 86 candidate(s)
[04:10:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 405 failure(s)
[04:10:30] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 337 failure(s)
[04:10:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 358 failure(s)
[04:11:04] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d71f3788)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d715ac88)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7215d18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d721f4b8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d6739168)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7261a58)]: 306 failure(s)
[04:11:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 32 candidates:
[1 : 16]:	1.0000  0.9997  0.9997  0.9986  0.9983  0.9982  0.9977  0.9977  0.9975  0.9971  0.9970  0.9968  0.9964  0.9955  0.9955  0.9947
[17 : 32]:	0.9943  0.9937  0.9935  0.9933  0.9932  0.9928  0.9927  0.9921  0.9916  0.9909  0.9908  0.9906  0.9904  0.9900  0.9899  0.9896
[04:11:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 32 candidate(s) with evolutionary search
[04:11:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 32 candidates(s) for measurement
[04:11:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[04:11:12] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [04:11:22] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:816
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/callproc.c:1188
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.7.11/Modules/_ctypes/_ctypes.c:4025
  24: _PyObject_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:199
  25: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4619
  26: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3093
  27: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  28: function_code_fastcall
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:283
  29: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:322
  30: _PyObject_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:98
  31: call_unbound_noarg
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1515
  32: slot_tp_finalize
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:6684
  33: PyObject_CallFinalizer
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:286
  34: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/object.c:303
  35: subtype_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/typeobject.c:1207
  36: frame_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Objects/frameobject.c:470
  37: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:168
  38: tb_dealloc
        at /tmp/build/80754af9/python_1627392990942/work/Python/traceback.c:167
  39: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:1842
  40: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  41: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  42: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  43: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  44: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  45: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  46: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  47: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3959
  48: PyEval_EvalCode
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:524
  49: builtin_exec_impl.isra.12
        at /tmp/build/80754af9/python_1627392990942/work/Python/bltinmodule.c:1079
  50: builtin_exec
        at /tmp/build/80754af9/python_1627392990942/work/Python/clinic/bltinmodule.c.h:283
  51: _PyMethodDef_RawFastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:654
  52: _PyCFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:732
  53: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4568
  54: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  55: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  56: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  57: _PyFunction_FastCallKeywords
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:433
  58: call_function
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:4616
  59: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3124
  60: PyEval_EvalFrameEx
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:547
  61: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python_1627392990942/work/Python/ceval.c:3930
  62: _PyFunction_FastCallDict
        at /tmp/build/80754af9/python_1627392990942/work/Objects/call.c:376
  63: pymain_run_module
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:355
  64: pymain_run_python
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:2910
  65: pymain_main
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3460
  66: _Py_UnixMain
        at /tmp/build/80754af9/python_1627392990942/work/Modules/main.c:3495
  67: __libc_start_main
  68: 0x000055b11b308554
  69: 0xffffffffffffffff


[04:11:26] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:11:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:11:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 0 candidate(s) from database
[04:11:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:11:27] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2048 candidate(s)
[04:11:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:11:31] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:11:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:11:34] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:11:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 5 candidates:
[1 : 5]:	0.6106  0.6079  0.2702  0.2648  0.1791
[04:11:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 5 candidate(s) with evolutionary search
[04:11:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 5 candidates(s) for measurement
[04:11:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 5 sample(s) to builder
[04:11:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 5 sample(s) to runner
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(115):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3663 // 111)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 111)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 3663)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 28, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(32, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1089 // 33)
                                        v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 33)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1089)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 32, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #2: GFLOPs: 478.4614. Time: 0.0470 ms. Best GFLOPs: 478.4614
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(350):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 11187 // 3729)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3729 // 113)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 11187)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 4, 14):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i3_3)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 3, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_2, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 1, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l183)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 77.4838. Time: 0.2901 ms. Best GFLOPs: 478.4614
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #5: GFLOPs: 618.8252. Time: 0.0363 ms. Best GFLOPs: 618.8252
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i3_4_init in T.grid(2, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1767 // 31)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 7 * 32 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 31)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1767)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + i3_3 * 4 + i3_4)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #7: GFLOPs: 12.4364. Time: 1.8075 ms. Best GFLOPs: 618.8252
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 758.1845. Time: 0.0296 ms. Best GFLOPs: 758.1845
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 32.3590. Time: 0.6947 ms. Best GFLOPs: 758.1845
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(176):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9831 // 3277)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 8 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3277 // 29)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 8 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 29)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 9831)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_4_init, i3_4_init in T.grid(8, 7):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4_init)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 8, 1, 7):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l177)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i1_3_init, i2_3_init in T.grid(16, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 112 * 16 + i1_3_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 14 * 2 + i2_3_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 2673 // 891)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 8 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 891 // 27)
                                        v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 8 * 28 + i6_0 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 27)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 2673)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i6_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 16, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 112 * 16 + i1_3)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 14 * 2 + i2_3)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 112 * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[8, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l173)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i2_4_init, i3_4_init in T.grid(4, 2, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3_init * 2 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(3, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6021 // 2007)
                                            v2 = T.axis.spatial(226, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2007 // 9)
                                            v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6021)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                            v2 = T.axis.spatial(3, i5_0)
                                            v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 4, 1, 3, 1, 1, 1, 1, 2, 4):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                    yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 4, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l176)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 498.1888. Time: 0.0451 ms. Best GFLOPs: 758.1845
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(4, 4, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i2_4_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 6525 // 225)
                                    v3 = T.axis.spatial(226, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 225)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 6525)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 1, 1, 4, 1, 1, 1, 1, 4, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i2_4)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 28 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 1, 1, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 16 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(68):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6525 // 225)
                                        v3 = T.axis.spatial(226, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 225)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6525)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(3, i4_0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 7, 1, 3, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 16 * 7 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 16 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 2, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 7, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 16, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 681.9485. Time: 0.0330 ms. Best GFLOPs: 758.1845
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3729 // 113)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 3729)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 3, 1, 2, 4, 2, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_1, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 28, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 399.0115. Time: 0.0563 ms. Best GFLOPs: 758.1845
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 1017.5074. Time: 0.0221 ms. Best GFLOPs: 1017.5074
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 1353.9251. Time: 0.0166 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 392.0248. Time: 0.0573 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 426.1947. Time: 0.0527 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 135.7542. Time: 0.1656 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(88):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 11187 // 3729)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 3729 // 113)
                                    v3 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 2 * 112 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 113)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 < 11187)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 27)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 7):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3_init)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4_init)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 3, 1, 3, 1, 1, 2, 7):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3)
                                    yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4)
                                    xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 8, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l177)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #25: GFLOPs: 139.7200. Time: 0.1609 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused % 14 * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3345 // 223)
                                        v3 = T.axis.spatial(226, i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 223)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3345)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax0_ax1_ax2_ax3_fused_1)
                                    v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 8)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 14, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 4, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 4, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 8, 14, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(32, 3, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 226, 226], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 3, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init in T.serial(28):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, i4_0 + 0)
                                    v2 = T.axis.spatial(226, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 6525 // 225)
                                    v3 = T.axis.spatial(226, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 225)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 6525)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 225 and 1 <= v3 and v3 < 225, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(3, i4_0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 1, 1, 28, 1, 3, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_2, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [32, 3, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 2 + ry, xx * 2 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 4 * 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 28, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 224])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 412.0014. Time: 0.0546 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #29: GFLOPs: 247.0234. Time: 0.0910 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 342.2072. Time: 0.0657 ms. Best GFLOPs: 1353.9251
[04:11:37] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 242.6926. Time: 0.0926 ms. Best GFLOPs: 1353.9251
[04:11:38] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add_nn_relu"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 16.6027

[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 313.2247. Time: 0.0846 ms. Best GFLOPs: 313.2247
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) // 128)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 128 // 16)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init, i2_4_init in T.grid(2, 4, 4):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_4_init)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 4, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 8, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 128])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 73.6942. Time: 0.3595 ms. Best GFLOPs: 313.2247
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 753.0285. Time: 0.0352 ms. Best GFLOPs: 753.0285
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 959.1979. Time: 0.0276 ms. Best GFLOPs: 959.1979
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 75.9720. Time: 0.3487 ms. Best GFLOPs: 959.1979
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 97.6560. Time: 0.2713 ms. Best GFLOPs: 959.1979
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 335.0887. Time: 0.0791 ms. Best GFLOPs: 959.1979
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 350.3910. Time: 0.0756 ms. Best GFLOPs: 959.1979
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 1206.9962. Time: 0.0219 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 74.5695. Time: 0.3553 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 1093.7791. Time: 0.0242 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: GFLOPs: 859.5456. Time: 0.0308 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 102.8781. Time: 0.2575 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 1173.4433. Time: 0.0226 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(2, 7, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 28 + i2_3_init * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 7, 1, 2, 1, 1, 1, 8, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 28 + i2_3 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 28, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 28 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(7, 2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + i2_3_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 112)
                                        v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 64)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 2, 2, 1, 1, 1, 8, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + i2_3)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 2, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: GFLOPs: 575.4156. Time: 0.0460 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: GFLOPs: 210.8388. Time: 0.1257 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: GFLOPs: 350.1377. Time: 0.0757 ms. Best GFLOPs: 1206.9962
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 2, 1, 16, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: GFLOPs: 1430.5550. Time: 0.0185 ms. Best GFLOPs: 1430.5550
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 1513.4228. Time: 0.0175 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i2_4_init in T.grid(4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 896)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 896 // 8)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1792)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 2, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 128, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 128, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 467.8956. Time: 0.0566 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 1236.7189. Time: 0.0214 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 1001.6326. Time: 0.0264 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: GFLOPs: 150.3982. Time: 0.1762 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 2, 8, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + i3_3_init * 8 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(32, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 8, 1, 8):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + i3_3 * 8 + i3_4)
                                rc = T.axis.reduce(32, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 16):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 28, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 8])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 1420.6325. Time: 0.0186 ms. Best GFLOPs: 1513.4228
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 256)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 256 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 2, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 434.8699. Time: 0.0609 ms. Best GFLOPs: 1513.4228
[04:11:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_conv2d_add_nn_relu_1"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 34.108

[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: GFLOPs: 71.6987. Time: 0.1120 ms. Best GFLOPs: 71.6987
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 121.6049. Time: 0.0660 ms. Best GFLOPs: 121.6049
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: GFLOPs: 97.2233. Time: 0.0826 ms. Best GFLOPs: 121.6049
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(32, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4608 // 144)
                                            v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 144 // 8)
                                            v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 4):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 193.7598. Time: 0.0414 ms. Best GFLOPs: 193.7598
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 296.3217. Time: 0.0271 ms. Best GFLOPs: 296.3217
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 247.7316. Time: 0.0324 ms. Best GFLOPs: 296.3217
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(87):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11136 // 348)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 28 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 348 // 6)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(14, 2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 28 + i2_3_init * 2 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 14, 2, 1, 3, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 28 + i2_3 * 2 + i2_4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 28 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 28 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 28 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 14, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[28, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: GFLOPs: 144.2766. Time: 0.0556 ms. Best GFLOPs: 296.3217
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: GFLOPs: 327.3676. Time: 0.0245 ms. Best GFLOPs: 327.3676
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 216.7757. Time: 0.0370 ms. Best GFLOPs: 327.3676
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: GFLOPs: 230.8858. Time: 0.0348 ms. Best GFLOPs: 327.3676
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8352 // 1044)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1044 // 58)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 8352)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax0_ax1_ax2_ax3_fused_1 // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 72)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + i1_3)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 2 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + i1_3)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 2 + i2_3)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 28 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 2, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 4, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 224])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: GFLOPs: 381.6180. Time: 0.0210 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 2 + i2_3_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7424 // 928)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 928 // 58)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + ((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 7424)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 2 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 56 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 56, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 448, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 448, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 68.9324. Time: 0.1165 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 150.3009. Time: 0.0534 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 76.1041. Time: 0.1055 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: GFLOPs: 111.6456. Time: 0.0719 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 177.2217. Time: 0.0453 ms. Best GFLOPs: 381.6180
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(300):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9600 // 300)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 300 // 30)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 30)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_4_init, i3_4_init in T.grid(4, 4, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 4 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 4, 1, 1, 1, 3, 1, 1, 4, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i1_3)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 4 + i2_4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 32])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l157)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 533.8093. Time: 0.0150 ms. Best GFLOPs: 533.8093
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(100):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3200 // 100)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 100 // 10)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 4, 2):
                            for i1_4_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i2_3)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i3_3 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: GFLOPs: 741.6027. Time: 0.0108 ms. Best GFLOPs: 741.6027
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 128)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 128 // 8)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 32)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 64])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 128)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 128 // 8)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + i2_3)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 64, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 2 + i2_4_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(288):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 9216 // 288)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 288 // 16)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 2 + i2_4)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 8 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 8, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 359.3130. Time: 0.0223 ms. Best GFLOPs: 741.6027
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 4, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_4_init)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + i2_3_init)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + i3_3_init * 4 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4608 // 288)
                                            v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 288 // 16)
                                            v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 3, 1, 1, 4, 1, 4):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_4)
                                    i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + i3_3 * 4 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 8):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 49 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 8, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: GFLOPs: 266.9732. Time: 0.0301 ms. Best GFLOPs: 741.6027
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(32, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_relu: T.Buffer[(1, 32, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 32, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_4_init)
                            j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(144):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8064 // 1008)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 56 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1008 // 18)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 18)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 2, 1, 3, 1, 2, 4, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + i2_4)
                                j = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [32, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 56 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 56, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: GFLOPs: 221.0503. Time: 0.0363 ms. Best GFLOPs: 741.6027
[04:11:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_conv2d_add_nn_relu_2"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 44.9334

[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #0: GFLOPs: 83.3506. Time: 0.1565 ms. Best GFLOPs: 83.3506
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #1: GFLOPs: 1577.3584. Time: 0.0083 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #2: GFLOPs: 313.4894. Time: 0.0416 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #3: GFLOPs: 1089.5398. Time: 0.0120 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #4: GFLOPs: 190.5946. Time: 0.0684 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #5: GFLOPs: 1403.9338. Time: 0.0093 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3_init * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 16)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 8, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 1, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init in T.grid(4, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 1792)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1792 // 16)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 4, 2, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 8, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 14 * 8 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 4, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 8, 1, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 4, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 56, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #8: GFLOPs: 335.2400. Time: 0.0389 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1792)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1792 // 112)
                                        v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 2, 2, 1, 1, 1, 2, 2, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 16 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused % 8 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 4, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 8, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 7, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 56)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_ax3_fused_1 // 2)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + ax0_ax1_ax2_ax3_fused_1 % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 2, 7, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused // 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 2 * 56 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 8 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 7, 2, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 8, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #11: GFLOPs: 1259.2824. Time: 0.0104 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #12: GFLOPs: 169.1320. Time: 0.0771 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 224 // 8)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 16, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 112])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #14: GFLOPs: 53.5024. Time: 0.2438 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #15: GFLOPs: 569.6721. Time: 0.0229 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #16: GFLOPs: 150.9355. Time: 0.0864 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #17: GFLOPs: 155.3303. Time: 0.0840 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #18: GFLOPs: 47.4946. Time: 0.2747 ms. Best GFLOPs: 1577.3584
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #19: GFLOPs: 1625.6850. Time: 0.0080 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #20: GFLOPs: 192.4388. Time: 0.0678 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 8, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 1, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 28, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 2, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 56, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #22: GFLOPs: 1174.4215. Time: 0.0111 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #23: GFLOPs: 386.1338. Time: 0.0338 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #24: GFLOPs: 552.9627. Time: 0.0236 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #25: GFLOPs: 645.8423. Time: 0.0202 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #26: GFLOPs: 246.3081. Time: 0.0530 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #27: GFLOPs: 91.4260. Time: 0.1427 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init, i3_4_init in T.grid(4, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 448 // 16)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 56, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #29: GFLOPs: 488.4789. Time: 0.0267 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add"] Trial #30: GFLOPs: 558.5600. Time: 0.0234 ms. Best GFLOPs: 1625.6850
[04:11:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #3: "fused_nn_conv2d_add"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 112, 112), "float32"], placeholder_1: T.Buffer[(16, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 16, 1, 1), "float32"], T_add: T.Buffer[(1, 16, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([16, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 112)
                                        v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 32)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 4, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 112, 112], "float32"], ["TENSOR", [16, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(16, i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 52.9582

[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(3, 2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 3 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 3 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 1, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 49, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #1: GFLOPs: 30.4102. Time: 1.3464 ms. Best GFLOPs: 30.4102
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #2: GFLOPs: 87.6014. Time: 0.4674 ms. Best GFLOPs: 87.6014
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 224 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i3_4_init in T.grid(8, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4_init)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4)
                                rc = T.axis.reduce(16, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 3, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[56, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 28, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #4: GFLOPs: 158.2785. Time: 0.2587 ms. Best GFLOPs: 158.2785
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #5: GFLOPs: 685.1907. Time: 0.0598 ms. Best GFLOPs: 685.1907
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #6: GFLOPs: 626.2464. Time: 0.0654 ms. Best GFLOPs: 685.1907
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #7: GFLOPs: 646.7031. Time: 0.0633 ms. Best GFLOPs: 685.1907
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(6, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 448 // 112)
                                        v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 6, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #9: GFLOPs: 94.1330. Time: 0.4350 ms. Best GFLOPs: 685.1907
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #10: GFLOPs: 450.5473. Time: 0.0909 ms. Best GFLOPs: 685.1907
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(168, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(16, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 8):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_4)
                                rc = T.axis.reduce(16, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 28 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 56 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 6, 16, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 8])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 224])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #12: GFLOPs: 1069.4028. Time: 0.0383 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #13: GFLOPs: 69.8430. Time: 0.5862 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(112, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            xx = T.axis.spatial(112, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 1792)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 1792 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 48)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 112, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                xx = T.axis.spatial(112, i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 112):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax2)
                            v3 = T.axis.spatial(112, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 3, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 8, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 112, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 8, 6, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_4_init)
                            yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 896)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 896 // 8)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 8, 2, 1, 1, 1, 6, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_4)
                                yy = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 4, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 8, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(6, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(16, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 96)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(16, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 6 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 8, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 64])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 64])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #17: GFLOPs: 267.6129. Time: 0.1530 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #18: GFLOPs: 48.1082. Time: 0.8511 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(3, 8, 2, 16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 48 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) // 3136)
                                    v2 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 3136 // 28)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(16, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 3, 8, 2, 1, 1, 1, 1, 16, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 48 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 48, 16, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 48 + ax1)
                            v2 = T.axis.spatial(112, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 1, 3, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 8, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 98])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 98, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #20: GFLOPs: 304.6274. Time: 0.1344 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(6, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 448 // 8)
                                    v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 6, 1, 4, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused // 14 * 56 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 6, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #22: GFLOPs: 791.7131. Time: 0.0517 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #23: GFLOPs: 164.4660. Time: 0.2489 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #24: GFLOPs: 84.7106. Time: 0.4833 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #25: GFLOPs: 274.3440. Time: 0.1492 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #26: GFLOPs: 678.9249. Time: 0.0603 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #27: GFLOPs: 984.3627. Time: 0.0416 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 224 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 8, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 56 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused % 2 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 3, 1, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[56, 1, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 28, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(3, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 3 + i1_4_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7 * 4 + i2_4_init)
                            xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(16, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 3 + i1_4)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7 * 4 + i2_4)
                                xx = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(16, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 98 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 56 + i0_2_i1_2_i2_2_i3_2_fused % 98 // 7 * 4 + ax2)
                            v3 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 2, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 2, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 196, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 196, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 16, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 112, 112), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 112, 112], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 16, 112, 112], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 16, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(512, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 16 * 3 + i1_3_init)
                            yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3_init)
                            xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) // 448)
                                    v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) % 448 // 112)
                                    v3 = T.axis.spatial(112, (ax0_ax1_ax2_ax3_fused_0 * 512 + ax0_ax1_ax2_ax3_fused_1) % 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(512, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(16, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 2, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 16 * 3 + i1_3)
                                yy = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + i2_3)
                                xx = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(16, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 16, 112, 112], "float32"], ["TENSOR", [96, 16, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 16 * 3 + ax1)
                            v2 = T.axis.spatial(112, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 * 2 + ax2)
                            v3 = T.axis.spatial(112, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 512])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 512, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_conv2d_add_nn_relu_3"] Trial #31: GFLOPs: 670.0479. Time: 0.0611 ms. Best GFLOPs: 1069.4028
[04:11:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_conv2d_add_nn_relu_3"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 91.2447

[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #0: GFLOPs: 77.6658. Time: 0.0775 ms. Best GFLOPs: 77.6658
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init in T.serial(4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i2_3_init)
                            j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(179):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9990 // 1665)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1665 // 111)
                                    v3 = T.axis.spatial(114, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 111)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 9990)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i2_3)
                                j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 6, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(336, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i1_3_init)
                            i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_3_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3774 // 1887)
                                        v2 = T.axis.spatial(114, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1887 // 17)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3774)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 2, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i1_3)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i2_3)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[48, 1, 1, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(90):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 4320 // 45)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 45 // 9)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_4_init, i3_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 48, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 48])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 48])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l157)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(784, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1680 // 35)
                                            v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 35 // 7)
                                            v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1680)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + i1_4)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 392 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 392 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 3, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #5: GFLOPs: 46.1875. Time: 0.1304 ms. Best GFLOPs: 77.6658
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #6: GFLOPs: 136.2195. Time: 0.0442 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(84, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                        i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                        j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                        T.reads()
                        T.writes(DepthwiseConv2d_local[b, c, i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(105):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9990 // 1665)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1665 // 111)
                                        v3 = T.axis.spatial(114, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 111)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 9990)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8)
                                j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 6 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 16 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 8 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 3, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 8, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #8: GFLOPs: 107.9276. Time: 0.0558 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #9: GFLOPs: 48.3740. Time: 0.1245 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #10: GFLOPs: 31.2663. Time: 0.1926 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #11: GFLOPs: 68.2856. Time: 0.0882 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #12: GFLOPs: 34.9450. Time: 0.1723 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #13: GFLOPs: 107.0527. Time: 0.0562 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #14: GFLOPs: 21.1730. Time: 0.2844 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #15: GFLOPs: 43.9524. Time: 0.1370 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #16: GFLOPs: 23.3171. Time: 0.2582 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(24, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(53):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(96, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3360 // 35)
                                            v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 35 // 7)
                                            v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3360)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 6, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 4, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #18: GFLOPs: 49.3634. Time: 0.1220 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #19: GFLOPs: 26.7750. Time: 0.2249 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #20: GFLOPs: 19.1756. Time: 0.3140 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #21: GFLOPs: 135.3338. Time: 0.0445 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #22: GFLOPs: 30.0936. Time: 0.2001 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #23: GFLOPs: 49.6683. Time: 0.1212 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(256, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(147, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(147, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 6 + ((ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4374 // 729)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 28 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 729 // 27)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 4 * 28 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4374)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(147, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 6 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 6)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 16 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 16 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 3, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 147, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 147])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #25: GFLOPs: 68.9782. Time: 0.0873 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #26: GFLOPs: 71.0103. Time: 0.0848 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #27: GFLOPs: 132.6271. Time: 0.0454 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #28: GFLOPs: 43.8142. Time: 0.1374 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #29: GFLOPs: 110.6801. Time: 0.0544 ms. Best GFLOPs: 136.2195
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(392, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(90):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 4320 // 45)
                                    v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 45 // 9)
                                    v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 14 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_4_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4_init)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 48, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 48])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 48, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[04:11:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #5: "fused_nn_conv2d_add_nn_relu_4"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 112, 112), "float32"], placeholder_1: T.Buffer[(96, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 96, 114, 114], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i3_4_init in T.serial(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10608 // 221)
                                        v2 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 14 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 221 // 17)
                                        v3 = T.axis.spatial(114, i0_0_i1_0_i2_0_i3_0_fused % 7 * 16 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 17)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 10608)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 113 and 1 <= v3 and v3 < 113, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 48 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 96, 112, 112], "float32"], ["TENSOR", [96, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 56 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 3, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_conv2d_add_nn_relu_4"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 135.446

[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #0: GFLOPs: 17.3462. Time: 0.8374 ms. Best GFLOPs: 17.3462
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(2, 8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 224 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 1, 2, 8, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_3)
                                rc = T.axis.reduce(96, i4_0 * 24 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 6, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 8, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 12, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 84])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 84, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(4, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(96, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 32 // 4 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[3, 1, 2, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 3, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 64, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #3: GFLOPs: 400.0958. Time: 0.0363 ms. Best GFLOPs: 400.0958
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #4: GFLOPs: 694.4642. Time: 0.0209 ms. Best GFLOPs: 694.4642
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(96, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 14, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_3)
                                rc = T.axis.reduce(96, i4_0 * 6 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 56 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[6, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 14])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 6, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #6: GFLOPs: 501.7571. Time: 0.0290 ms. Best GFLOPs: 694.4642
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #7: GFLOPs: 164.8284. Time: 0.0881 ms. Best GFLOPs: 694.4642
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #8: GFLOPs: 69.9944. Time: 0.2075 ms. Best GFLOPs: 694.4642
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #9: GFLOPs: 41.7234. Time: 0.3481 ms. Best GFLOPs: 694.4642
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 3, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + i1_3_init * 3 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 112 // 2)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 2, 3, 1, 1, 1, 3, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + i1_3 * 3 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 112 // 2)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(96, i4_0 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 112 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 112 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 2, 2, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 56, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 1, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i3_4_init in T.grid(6, 7, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 7 * 7 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(96, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 6, 7, 2, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 7 * 7 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(96, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 7, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 6, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 3, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 112])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 112, 4])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(96, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(96, i4_0 * 6 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 24, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 14, 2, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 2, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 96, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 96, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #13: GFLOPs: 824.3060. Time: 0.0176 ms. Best GFLOPs: 824.3060
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #14: GFLOPs: 358.2921. Time: 0.0405 ms. Best GFLOPs: 824.3060
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #15: GFLOPs: 824.4166. Time: 0.0176 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 56)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 56 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 < 1792)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(96, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                rc = T.axis.reduce(96, i4_0 * 32 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 3, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[28, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 32, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 84])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 84, 3])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #17: GFLOPs: 149.4557. Time: 0.0972 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_1"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i2_3_init in T.serial(28):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 196 // 7)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 28, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(96, i4_0 * 24 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 56, 56], "float32"], ["TENSOR", [24, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 8 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 6, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 1, 28, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[4, 3, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 42, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 42])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #19: GFLOPs: 375.4246. Time: 0.0387 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #20: GFLOPs: 735.6302. Time: 0.0197 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #21: GFLOPs: 312.9655. Time: 0.0464 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #22: GFLOPs: 193.5881. Time: 0.0750 ms. Best GFLOPs: 824.4166
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #23: GFLOPs: 865.3754. Time: 0.0168 ms. Best GFLOPs: 865.3754
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #24: GFLOPs: 301.1160. Time: 0.0482 ms. Best GFLOPs: 865.3754
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #25: GFLOPs: 197.3940. Time: 0.0736 ms. Best GFLOPs: 865.3754
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #26: GFLOPs: 1041.6910. Time: 0.0139 ms. Best GFLOPs: 1041.6910
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #27: GFLOPs: 66.7314. Time: 0.2177 ms. Best GFLOPs: 1041.6910
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #28: GFLOPs: 228.5078. Time: 0.0636 ms. Best GFLOPs: 1041.6910
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #29: GFLOPs: 298.9981. Time: 0.0486 ms. Best GFLOPs: 1041.6910
[04:11:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #30: GFLOPs: 361.1743. Time: 0.0402 ms. Best GFLOPs: 1041.6910
[04:11:51] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_1"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 223
Total latency (us): 149.391

[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i1_3_init in T.serial(3):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 60)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 60 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 48, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 14, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 192, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 192, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(48, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(168, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(168, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 11520 // 480)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 // 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 480 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 11520)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(168, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_4_init, i3_4_init in T.grid(2, 7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 3, 1, 1, 1, 2, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 24, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 168, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 168, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #2: GFLOPs: 165.1295. Time: 0.0547 ms. Best GFLOPs: 165.1295
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(144, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6912 // 48)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 48 // 6)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 1, 1, 3, 1, 1, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 9, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 144, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 144, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #4: GFLOPs: 578.3544. Time: 0.0156 ms. Best GFLOPs: 578.3544
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i1_4_init in T.serial(3):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 60)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 60 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 48, 1, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[28, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 14, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 192, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 192, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #6: GFLOPs: 189.3251. Time: 0.0477 ms. Best GFLOPs: 578.3544
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(96, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(24, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i3_4_init in T.serial(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 49)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                            j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4872 // 406)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 406 // 58)
                                        v3 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 4872)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 49)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7)
                                j = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 49 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 6, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[8, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 98, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 98, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #8: GFLOPs: 128.5960. Time: 0.0702 ms. Best GFLOPs: 578.3544
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #9: GFLOPs: 126.6810. Time: 0.0713 ms. Best GFLOPs: 578.3544
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(252, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                            i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 12096 // 336)
                                    v2 = T.axis.spatial(58, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 336 // 6)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 6)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 108)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 9, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 252])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 252, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #11: GFLOPs: 188.6111. Time: 0.0479 ms. Best GFLOPs: 578.3544
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #12: GFLOPs: 692.4820. Time: 0.0130 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #13: GFLOPs: 556.6736. Time: 0.0162 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(144, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 18 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_3_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5760 // 40)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 40 // 4)
                                            v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 18 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + i1_3)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused * 18 + i0_2_i1_2_i2_2_i3_2_fused // 16 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 16 // 4 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 9, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 144, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 144, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_3_init, i2_4_init in T.grid(3, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 98 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3_init)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init * 2 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 98 * 72 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2880 // 40)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 40 // 4)
                                            v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 98 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 3, 2, 1, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 98 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 98 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 98 // 14 * 8 + i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 24, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 96, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 96, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1568, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_4_init in T.serial(6):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 784 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 784 // 56 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 784 * 72 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 864 // 12)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 784 // 56 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 56 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 784 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1 < 216)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 784 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 784 // 56 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 784 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 784 // 56 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 56 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 12, 1, 6])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 48, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 48])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) % 8640 // 60)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) % 60 // 10)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) % 10)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 < 1296)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_3)
                                    i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                    j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 4 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 24, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 192])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 192])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l163)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #18: GFLOPs: 209.7342. Time: 0.0431 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #19: GFLOPs: 47.2951. Time: 0.1910 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 18 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6264 // 348)
                                        v2 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 348 // 6)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 18 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 162)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 7, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4_init)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4_init)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 7, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + i1_4)
                                i = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 9, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 72, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 72, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #21: GFLOPs: 134.5834. Time: 0.0671 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(196, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(4, 3):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_4_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i2_3_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 9 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 7560 // 840)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 840 // 30)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 7560)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(196, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 9 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 27)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 4, 1, 1, 1, 1, 3, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_4)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i2_3)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 3, 1, 1, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 196, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 196, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #23: GFLOPs: 321.1979. Time: 0.0281 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(252, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init, i3_4_init in T.grid(2, 4, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + i2_4_init)
                            j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 1856 // 464)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 464 // 58)
                                        v3 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 58)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1856)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 12)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 4, 7):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + i2_4)
                                j = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + i3_3 * 7 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 14):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 4 * 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[36, 1, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #25: GFLOPs: 13.6086. Time: 0.6637 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(63, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 28):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i2_4_init)
                            j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(63, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 63 + ax0_ax1_ax2_ax3_fused_1) % 8064 // 448)
                                    v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 63 + ax0_ax1_ax2_ax3_fused_1) % 448 // 16)
                                    v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 63 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(63, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 18 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 54)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 2, 1, 3, 1, 1, 28, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                i = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + i2_4)
                                j = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 8 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 9, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 28])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 63])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 63, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #27: GFLOPs: 120.6623. Time: 0.0749 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #28: GFLOPs: 210.5165. Time: 0.0429 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #29: GFLOPs: 115.4759. Time: 0.0782 ms. Best GFLOPs: 692.4820
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #30: GFLOPs: 1039.9801. Time: 0.0087 ms. Best GFLOPs: 1039.9801
[04:11:52] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_5"] Trial #31: GFLOPs: 567.3324. Time: 0.0159 ms. Best GFLOPs: 1039.9801
[04:11:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_nn_relu_5"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 255
Total latency (us): 158.075

[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #0: GFLOPs: 2104.8289. Time: 0.0104 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #1: GFLOPs: 1137.0515. Time: 0.0192 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #2: GFLOPs: 182.1014. Time: 0.1199 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 224 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(144, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 48)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 48 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 14 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 16, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 84])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 84, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #4: GFLOPs: 713.8435. Time: 0.0306 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #5: GFLOPs: 699.7258. Time: 0.0312 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(18, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 14)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(144, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 64)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 4 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 8, 1, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[18, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 49, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #7: GFLOPs: 875.2730. Time: 0.0249 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(49, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i3_4_init in T.grid(3, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(144, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 3, 1, 2, 3, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 6 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 8 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 2, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #9: GFLOPs: 741.0974. Time: 0.0295 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(3, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(74):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(144, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 1, 1, 6, 1, 1, 1, 1, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 4 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 14 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 1, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #11: GFLOPs: 1040.3509. Time: 0.0210 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #12: GFLOPs: 771.0593. Time: 0.0283 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #13: GFLOPs: 1136.0060. Time: 0.0192 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i2_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 1792 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + (ax0_ax1_ax2_ax3_fused_0 * 1792 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 1792 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(144, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 < 576)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56)
                                rc = T.axis.reduce(144, i4_0 * 24 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 112 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 8 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 112 // 56 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 56 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 6, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 56, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 8, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 448, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 448])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #15: GFLOPs: 485.0351. Time: 0.0450 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #16: GFLOPs: 602.0761. Time: 0.0363 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(6, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(144, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 6, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 12 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 12, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #18: GFLOPs: 515.3979. Time: 0.0423 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #19: GFLOPs: 1652.9778. Time: 0.0132 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #20: GFLOPs: 40.0359. Time: 0.5452 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #21: GFLOPs: 358.5297. Time: 0.0609 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #22: GFLOPs: 63.5317. Time: 0.3436 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #23: GFLOPs: 694.1576. Time: 0.0314 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #8: "fused_nn_conv2d_add_add"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(24, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 24, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 24, 56, 56), "float32"], T_add: T.Buffer[(1, 24, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([24, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(14, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_3_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(36, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(144, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 14, 1, 2, 1, 1, 1, 4, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_3)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [24, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(24, i0_1_i1_1_i2_1_i3_1_fused * 8 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 28 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 2, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[36, 2, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #25: GFLOPs: 684.0595. Time: 0.0319 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #26: GFLOPs: 155.7058. Time: 0.1402 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #27: GFLOPs: 174.9508. Time: 0.1248 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #28: GFLOPs: 303.1286. Time: 0.0720 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #29: GFLOPs: 376.7090. Time: 0.0579 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #30: GFLOPs: 77.4646. Time: 0.2818 ms. Best GFLOPs: 2104.8289
[04:11:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_add"] Trial #31: GFLOPs: 255.9921. Time: 0.0853 ms. Best GFLOPs: 2104.8289
[04:11:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_add"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |            N/A |          N/A |                   N/A |      0 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 287
Total latency (us): 168.445

[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #0: GFLOPs: 175.1498. Time: 0.1289 ms. Best GFLOPs: 175.1498
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(24, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(7, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 3 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 7, 1, 3, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 3 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8)
                                rc = T.axis.reduce(24, i4_0 * 6 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 32 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 32 // 8 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i0_2_i1_2_i2_2_i3_2_fused % 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 2, 2, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 4, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 8, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 64, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(588, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 24, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + i1_3_init * 24 + i1_4_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 4 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(588, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 1764 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 1764 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 28)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 1764 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 588 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 9408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(588, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 588 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(24, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 588 + ax0_ax1_ax2_ax3_fused_1) % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 588 + ax0_ax1_ax2_ax3_fused_1 < 864)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 24, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + i1_3 * 24 + i1_4)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 4 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(24, i4_0 * 6 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 48, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 3, 2, 24])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 6, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 588, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 588])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 28, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 2, 6, 1, 1, 1, 1, 28, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 36, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 28])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 2, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 72, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 72, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #4: GFLOPs: 30.4724. Time: 0.7410 ms. Best GFLOPs: 175.1498
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #5: GFLOPs: 130.9286. Time: 0.1725 ms. Best GFLOPs: 175.1498
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(18, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 7, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 14 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 9, 4, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 4, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 3, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #7: GFLOPs: 614.7411. Time: 0.0367 ms. Best GFLOPs: 614.7411
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(192, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(6, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 1568)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 1568 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 < 288)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 6, 2, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 28 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 24, 1, 6, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 98, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 98])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #9: GFLOPs: 304.3282. Time: 0.0742 ms. Best GFLOPs: 614.7411
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 112)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(24, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(24, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 18, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 4, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 36])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:11:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(50):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 224 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(24, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init, i3_4_init in T.grid(4, 8, 7):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3_init)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(56, i3_3_init * 7 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 8, 12, 1, 1, 1, 1, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused * 4 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused)
                                xx = T.axis.spatial(56, i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(24, i4_1 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 56):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused * 4 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(56, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 36, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 4, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 8, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 2, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 36, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 36])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(24, 2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 28 * 48 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 < 3136)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(24, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 24, 2, 1, 1, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 28 * 48 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 48, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 28 * 48 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 4 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 3, 24, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 84])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 84, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init, i3_4_init in T.grid(2, 2, 2, 14, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3_init * 14 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3_init * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 448)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 14, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + i2_3 * 14 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i3_3 * 4 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 2 * 28 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 36, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 14])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 72, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 72, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #14: GFLOPs: 54.9791. Time: 0.4107 ms. Best GFLOPs: 614.7411
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(4, 3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 3 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 1568)
                                    v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 1568 // 28)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 < 3136)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 72)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 3, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 3 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 8, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 36 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 9 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 3 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 7 * 8 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 3, 1, 3])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 4, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 84])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 84, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #16: GFLOPs: 662.9548. Time: 0.0341 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #17: GFLOPs: 117.5123. Time: 0.1921 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #18: GFLOPs: 19.8312. Time: 1.1386 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #19: GFLOPs: 582.1039. Time: 0.0388 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #20: GFLOPs: 33.9537. Time: 0.6650 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #21: GFLOPs: 35.6609. Time: 0.6332 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(126, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 378 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 378 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 224 // 4)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 378 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2688)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 378 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 378 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1728)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 18, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 6, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 126, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 126, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #23: GFLOPs: 80.7911. Time: 0.2795 ms. Best GFLOPs: 662.9548
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #24: GFLOPs: 1019.3571. Time: 0.0222 ms. Best GFLOPs: 1019.3571
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(126, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 224 // 4)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2688)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(24, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1728)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 12 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 18, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 6, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 126, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 126, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(126, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 8, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 504 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3136)
                                        v2 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 504 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3136 // 56)
                                        v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 504 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(126, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 8, 2, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + i3_3)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 8):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(56, i0_2_i1_2_i2_2_i3_2_fused % 7 * 8 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 18, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 8, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 126, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 126, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i2_4_init in T.grid(2, 6, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_4_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3_init * 7 + i2_4_init)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 224)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 224 // 8)
                                    v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1 < 1792)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(24, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 1, 8, 1, 1, 1, 6, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_4)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3 * 7 + i2_4)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(24, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_1_i1_1_i2_1_i3_1_fused // 4 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 12, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 2, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 48])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 48, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(72, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i3_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 784 // 56)
                                    v3 = T.axis.spatial(56, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 56)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(24, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 16)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 2, 7, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(24, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 4 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(56, i0_1_i1_1_i2_1_i3_1_fused % 2 * 28 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[18, 2, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #29: GFLOPs: 15.7041. Time: 1.4378 ms. Best GFLOPs: 1019.3571
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 24, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 24, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 56, 56), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 144, 56, 56], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 24, 56, 56], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 24, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init in T.serial(36):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + i1_3_init)
                            yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(24, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 224)
                                        v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 28 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 224 // 8)
                                        v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(24, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 36, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + i1_3)
                                yy = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                xx = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(24, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 24, 56, 56], "float32"], ["TENSOR", [144, 24, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 36, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 72 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 36 + ax1)
                            v2 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 28 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(56, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i0_1_i1_1_i2_1_i3_1_fused % 4 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 36, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 4, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:11:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_conv2d_add_nn_relu_6"] Trial #31: GFLOPs: 1208.7109. Time: 0.0187 ms. Best GFLOPs: 1208.7109
[04:11:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_conv2d_add_nn_relu_6"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 319
Total latency (us): 205.806

[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #0: GFLOPs: 67.1448. Time: 0.0336 ms. Best GFLOPs: 67.1448
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #1: GFLOPs: 54.1850. Time: 0.0417 ms. Best GFLOPs: 67.1448
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(97):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10440 // 145)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 145 // 29)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 10440)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i3_3_init in T.grid(4, 7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_3_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 4, 1, 7, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 72 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 18, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 36, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 36, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #3: GFLOPs: 36.0074. Time: 0.0627 ms. Best GFLOPs: 67.1448
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(147, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3888 // 81)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3888)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_3_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_3_init)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 48, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 96, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 96, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #5: GFLOPs: 62.4546. Time: 0.0362 ms. Best GFLOPs: 67.1448
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9396 // 261)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 261 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9396)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 324)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_4_init in T.serial(4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 4):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 36 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 6, 6, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 84, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(196, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3_init * 4 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1296 // 9)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 14 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2)
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 4 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 18, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 36, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 36, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 9 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4617 // 513)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 513 // 57)
                                        v3 = T.axis.spatial(58, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 57)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4617)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 9 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 81)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(2, 3):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2, 1, 1, 1, 3, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 9 + i0_1_i1_1_i2_1_i3_1_fused * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 3, 1, 1, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(252, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(29):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 7182 // 399)
                                        v2 = T.axis.spatial(58, (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 399 // 7)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 < 7182)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(252, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 18 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 54)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 2, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 7 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 9, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 252])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 252])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l156)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #10: GFLOPs: 75.7938. Time: 0.0298 ms. Best GFLOPs: 75.7938
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #11: GFLOPs: 20.7714. Time: 0.1087 ms. Best GFLOPs: 75.7938
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #12: GFLOPs: 86.6914. Time: 0.0260 ms. Best GFLOPs: 86.6914
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i3_4_init in T.grid(4, 2, 3, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + i1_3_init * 3 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(63):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9072 // 63)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 7 * 8 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 63 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 2, 1, 3, 1, 3, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + i1_3 * 3 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 4 * 12 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 12, 4, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 48, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 48, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #14: GFLOPs: 261.4385. Time: 0.0086 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(504, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 105)
                                            v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 105 // 7)
                                            v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 8 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[18, 2, 4, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #16: GFLOPs: 7.1058. Time: 0.3178 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3_init * 4 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(54):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3888 // 27)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 27 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 1, 1, 1, 1, 1, 4, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + i1_3 * 4 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_2_i1_2_i2_2_i3_2_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 18, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 36, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 36, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #18: GFLOPs: 157.9817. Time: 0.0143 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(147, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3888 // 81)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 81 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3888)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(96, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 49 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 48, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 96, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 96, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(336, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 6 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 1566 // 261)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 261 // 29)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1566)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1 < 54)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 14 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[24, 1, 3, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 42])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #21: GFLOPs: 33.2052. Time: 0.0680 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #22: GFLOPs: 34.9002. Time: 0.0647 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(36, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(7, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                            i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(93):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11880 // 1485)
                                        v2 = T.axis.spatial(58, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 1485 // 27)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 2 * 28 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 11880)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 8)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 7, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[18, 1, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 56, 56), "float32"], placeholder_1: T.Buffer[(144, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 144, 1, 1), "float32"], T_relu: T.Buffer[(1, 144, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 144, 58, 58], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([144, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(20):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6480 // 135)
                                        v2 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 135 // 9)
                                        v3 = T.axis.spatial(58, i0_0_i1_0_i2_0_i3_0_fused % 7 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6480)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 57 and 1 <= v3 and v3 < 57, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 432)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 1):
                            for i1_4_init in T.serial(6):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 12 + i1_3 * 6 + i1_4_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 6, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 12 + i1_3 * 6 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 144, 56, 56], "float32"], ["TENSOR", [144, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(144, i0_0_i1_0_i2_0_i3_0_fused // 28 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 12 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 4, 2, 6])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 112])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #25: GFLOPs: 260.7146. Time: 0.0087 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #26: GFLOPs: 94.1853. Time: 0.0240 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #27: GFLOPs: 160.6703. Time: 0.0141 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #28: GFLOPs: 84.9018. Time: 0.0266 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #29: GFLOPs: 224.8373. Time: 0.0100 ms. Best GFLOPs: 261.4385
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #30: GFLOPs: 286.8284. Time: 0.0079 ms. Best GFLOPs: 286.8284
[04:11:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_nn_conv2d_add_nn_relu_7"] Trial #31: GFLOPs: 134.6937. Time: 0.0168 ms. Best GFLOPs: 286.8284
[04:12:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_nn_conv2d_add_nn_relu_7"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 351
Total latency (us): 213.678

[04:12:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #0: GFLOPs: 266.0650. Time: 0.0273 ms. Best GFLOPs: 266.0650
[04:12:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(16, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(9, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(144, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 256)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 16, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(144, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 16])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[9, 2, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 49])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 49, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #2: GFLOPs: 153.7850. Time: 0.0471 ms. Best GFLOPs: 266.0650
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #3: GFLOPs: 20.5148. Time: 0.3534 ms. Best GFLOPs: 266.0650
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #4: GFLOPs: 162.9273. Time: 0.0445 ms. Best GFLOPs: 266.0650
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #5: GFLOPs: 287.3811. Time: 0.0252 ms. Best GFLOPs: 287.3811
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #6: GFLOPs: 237.1055. Time: 0.0306 ms. Best GFLOPs: 287.3811
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #7: GFLOPs: 160.5992. Time: 0.0451 ms. Best GFLOPs: 287.3811
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #8: GFLOPs: 488.9233. Time: 0.0148 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #9: GFLOPs: 173.4333. Time: 0.0418 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #10: GFLOPs: 193.2886. Time: 0.0375 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                        yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                        xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(144, i4_0 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 98)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 98 // 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 7056)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 72)
                                        v1 = T.axis.spatial(144, i4_0 * 72 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 72)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 1, 1, 12, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(144, i4_0 * 72 + i4_1 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 8 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 8 // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 6, 12])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 224])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 224, 3])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #12: GFLOPs: 280.1644. Time: 0.0259 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #13: GFLOPs: 39.5938. Time: 0.1831 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #14: GFLOPs: 186.3641. Time: 0.0389 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #15: GFLOPs: 158.4692. Time: 0.0458 ms. Best GFLOPs: 488.9233
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #16: GFLOPs: 520.3153. Time: 0.0139 ms. Best GFLOPs: 520.3153
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #17: GFLOPs: 644.6897. Time: 0.0112 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #18: GFLOPs: 105.3203. Time: 0.0688 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #19: GFLOPs: 513.6360. Time: 0.0141 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #20: GFLOPs: 255.2331. Time: 0.0284 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #21: GFLOPs: 198.5240. Time: 0.0365 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(8, 2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 49 * 16 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(144, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 8, 2, 2, 1, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 49 * 16 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(144, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 49 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[48, 3, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 98, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 98, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 56 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(144, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 48)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 7, 48, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(144, i4_0 * 48 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 48])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #24: GFLOPs: 567.6611. Time: 0.0128 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_4_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(36, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(144, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                rc = T.axis.reduce(144, i4_0 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 4, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[36, 1, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 112])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #26: GFLOPs: 294.8214. Time: 0.0246 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #27: GFLOPs: 61.6935. Time: 0.1175 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #28: GFLOPs: 225.8183. Time: 0.0321 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_conv2d_add_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 144, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 144, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 144, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 144, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(144, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(144, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 2, 6, 1, 1, 1, 2, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(144, i4_0 * 12 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 144, 28, 28], "float32"], ["TENSOR", [32, 144, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[12, 2, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #30: GFLOPs: 60.1903. Time: 0.1205 ms. Best GFLOPs: 644.6897
[04:12:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_conv2d_add_2"] Trial #31: GFLOPs: 256.2228. Time: 0.0283 ms. Best GFLOPs: 644.6897
[04:12:05] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_conv2d_add_2"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |            N/A |          N/A |                   N/A |      0 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 383
Total latency (us): 224.924

[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #0: GFLOPs: 106.8452. Time: 0.0906 ms. Best GFLOPs: 106.8452
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #1: GFLOPs: 473.0852. Time: 0.0205 ms. Best GFLOPs: 473.0852
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #2: GFLOPs: 162.8620. Time: 0.0595 ms. Best GFLOPs: 473.0852
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #3: GFLOPs: 158.1888. Time: 0.0612 ms. Best GFLOPs: 473.0852
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #4: GFLOPs: 360.3350. Time: 0.0269 ms. Best GFLOPs: 473.0852
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #5: GFLOPs: 1203.7274. Time: 0.0080 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #6: GFLOPs: 164.5035. Time: 0.0589 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #7: GFLOPs: 74.9163. Time: 0.1293 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #8: GFLOPs: 282.3072. Time: 0.0343 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #9: GFLOPs: 765.0246. Time: 0.0127 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #10: GFLOPs: 333.3114. Time: 0.0291 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #11: GFLOPs: 175.3518. Time: 0.0552 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 56 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(192, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 48)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 48 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 12, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #13: GFLOPs: 43.5942. Time: 0.2221 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #14: GFLOPs: 93.6056. Time: 0.1035 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #15: GFLOPs: 32.7069. Time: 0.2961 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #16: GFLOPs: 363.2311. Time: 0.0267 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #17: GFLOPs: 48.8495. Time: 0.1982 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #18: GFLOPs: 830.2152. Time: 0.0117 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i3_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(192, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 128)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                rc = T.axis.reduce(192, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused // 14 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 112])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    with T.block("conv2d_nchw_init"):
                        nn = T.axis.spatial(1, 0)
                        ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2)
                        yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                        xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28)
                        T.reads()
                        T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(192, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28)
                                rc = T.axis.reduce(192, i4_0 * 32 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 28, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #21: GFLOPs: 154.2637. Time: 0.0628 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #22: GFLOPs: 185.1777. Time: 0.0523 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(2, 28):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(192, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(192, i4_0)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 16)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 28, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(28, i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(192, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 28])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[192, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #24: GFLOPs: 232.3808. Time: 0.0417 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #25: GFLOPs: 29.0850. Time: 0.3330 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                    v1 = T.axis.spatial(192, i4_0 * 3 + ax0_ax1_ax2_ax3_fused_1 % 3)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 48)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 8, 1, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 49])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #27: GFLOPs: 140.7690. Time: 0.0688 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(32, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 32, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 32, 28, 28), "float32"], T_add: T.Buffer[(1, 32, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([32, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(8, 28):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                            xx = T.axis.spatial(28, i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 8, 1, 1, 4, 1, 1, 1, 1, 1, 28):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                xx = T.axis.spatial(28, i3_4)
                                rc = T.axis.reduce(192, i4_0 * 12 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [32, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 28):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(32, i0_1_i1_1_i2_1_i3_1_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax2)
                            v3 = T.axis.spatial(28, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 28])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 3, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 56, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #29: GFLOPs: 240.4860. Time: 0.0403 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #30: GFLOPs: 709.0215. Time: 0.0137 ms. Best GFLOPs: 1203.7274
[04:12:05] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_conv2d_add_add_1"] Trial #31: GFLOPs: 190.1530. Time: 0.0509 ms. Best GFLOPs: 1203.7274
[04:12:08] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_conv2d_add_add_1"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |            N/A |          N/A |                   N/A |      0 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 415
Total latency (us): 241.014

[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #0: GFLOPs: 605.8109. Time: 0.0164 ms. Best GFLOPs: 605.8109
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #1: GFLOPs: 213.1421. Time: 0.0466 ms. Best GFLOPs: 605.8109
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #2: GFLOPs: 14.8431. Time: 0.6693 ms. Best GFLOPs: 605.8109
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #3: GFLOPs: 158.3614. Time: 0.0627 ms. Best GFLOPs: 605.8109
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #4: GFLOPs: 706.0107. Time: 0.0141 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #5: GFLOPs: 524.0606. Time: 0.0190 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(6, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 98)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 98 // 7)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(55):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 3072)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 6, 2, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #7: GFLOPs: 183.0022. Time: 0.0543 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(168, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(16, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(168, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 < 784)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(168, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) // 4)
                                    v1 = T.axis.spatial(32, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 < 768)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 4 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 12, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 168])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 168])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(3, 16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 2 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + i1_3_init * 16 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 784 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 784 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 784 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 784 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 784 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 16, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 2 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + i1_3 * 16 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 48, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_1_i1_1_i2_1_i3_1_fused // 2 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 48 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 3, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 4, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 392, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 392, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #10: GFLOPs: 202.7514. Time: 0.0490 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i1_3_init, i2_4_init in T.grid(2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1 < 224)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(32, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(32, i4_0 * 2 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 3, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 42])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 42, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(448, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(448, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 < 6144)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 3, 1, 1):
                            for i2_4_init in T.serial(4):
                                with T.block("conv2d_nchw_init"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4_init)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                            for i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 4, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3)
                                    yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i2_4)
                                    xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    rc = T.axis.reduce(32, i4_2)
                                    ry = T.axis.reduce(1, 0)
                                    rx = T.axis.reduce(1, 0)
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 32])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 448])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 448])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l181)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #13: GFLOPs: 518.7843. Time: 0.0192 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #14: GFLOPs: 612.8766. Time: 0.0162 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #15: GFLOPs: 484.8938. Time: 0.0205 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(896, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(3, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_3_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 3584 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 3584 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 3584 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(896, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 3, 14, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + i2_3)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 96 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 56 // 28 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 16, 3, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 896, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 896, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 98 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init, i3_4_init in T.grid(7, 32, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4_init)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3_init * 2 + i3_4_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 1, 1, 7, 8, 1, 1, 1, 32, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(32, i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_2_i1_2_i2_2_i3_2_fused // 7 * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 6, 1, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 42, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 42, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l178)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #18: GFLOPs: 675.1883. Time: 0.0147 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #19: GFLOPs: 557.5592. Time: 0.0178 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #20: GFLOPs: 206.6057. Time: 0.0481 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #21: GFLOPs: 51.4537. Time: 0.1931 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #22: GFLOPs: 480.0957. Time: 0.0207 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(3, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + i1_3_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 14)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 784 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(32, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 3, 1, 2, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + i1_3)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 14)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 6 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 14 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 2, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #24: GFLOPs: 181.0237. Time: 0.0549 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #25: GFLOPs: 648.6605. Time: 0.0153 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #26: GFLOPs: 10.9201. Time: 0.9098 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init in T.grid(2, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6272)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) // 16)
                                    v1 = T.axis.spatial(32, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1 < 768)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 14, 2, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(32, i4_0 * 16 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 6, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 42, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 42])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #28: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 32, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 32, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 32, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 32, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(42, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(32, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 4, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init * 4 + i1_4_init)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4_init)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 16, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                rc = T.axis.reduce(32, i4_1 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 32, 28, 28], "float32"], ["TENSOR", [192, 32, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 2, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 112])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #29: GFLOPs: 407.3777. Time: 0.0244 ms. Best GFLOPs: 706.0107
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #30: GFLOPs: 907.7082. Time: 0.0109 ms. Best GFLOPs: 907.7082
[04:12:09] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_conv2d_add_nn_relu_8"] Trial #31: GFLOPs: 471.8438. Time: 0.0211 ms. Best GFLOPs: 907.7082
[04:12:12] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_conv2d_add_nn_relu_8"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |            N/A |          N/A |                   N/A |      0 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 447
Total latency (us): 273.849

[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #0: GFLOPs: 70.1090. Time: 0.0429 ms. Best GFLOPs: 70.1090
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #1: GFLOPs: 302.1931. Time: 0.0100 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #2: GFLOPs: 48.4005. Time: 0.0622 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i3_3_init, i2_4_init in T.grid(16, 2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3_init)
                            i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3_init * 2 + i2_4_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10752 // 112)
                                        v2 = T.axis.spatial(30, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 16, 2, 2, 1, 1, 1, 1, 2, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_3)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i2_3 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 4, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 16, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 42, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #4: GFLOPs: 28.8148. Time: 0.1045 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #5: GFLOPs: 103.1011. Time: 0.0292 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init in T.serial(3):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + i1_3_init)
                                i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 10080 // 840)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 840 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + i1_3)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 56 * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #7: GFLOPs: 148.4394. Time: 0.0203 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #8: GFLOPs: 128.1453. Time: 0.0235 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(24, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 24 + i1_3_init)
                            i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(168):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 96 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 10752 // 112)
                                        v2 = T.axis.spatial(30, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 24, 7, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 24 + i1_3)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 24 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 8 // 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 24, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #10: GFLOPs: 99.0428. Time: 0.0304 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #11: GFLOPs: 2.2859. Time: 1.3170 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 3072 // 96)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 96 // 16)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 1, 1):
                            for i3_4_init in T.serial(7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 14 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 128, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4320 // 180)
                                        v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 180 // 6)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4320)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 2, 1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 6, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 84, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(168, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 1152 // 36)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 36 // 9)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1152)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, i4_0)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 96)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 2, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 8, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 56])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 2, 3):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 6 + i1_3_init * 3 + i1_4_init)
                            i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 2 + i2_3_init)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 5376 // 448)
                                        v2 = T.axis.spatial(30, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 448 // 16)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 5376)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 2, 1, 1, 3, 1, 3, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 6 + i1_3 * 3 + i1_4)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 2 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 196 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 196 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 2, 2, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 392, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 392, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(336, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) % 6144 // 64)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) % 64 // 4)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 < 6144)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 864)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init, i3_3_init in T.grid(4, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 4, 1, 2, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 24, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 336])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 336, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #17: GFLOPs: 117.1271. Time: 0.0257 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(24, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(69):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7680 // 480)
                                    v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 480 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 7680)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 144)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 1, 8, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 112])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l163)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_3_init, i2_4_init, i3_4_init in T.grid(2, 7, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3_init)
                                i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + i2_3_init * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(40):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5040 // 420)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 420 // 14)
                                            v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + ax0_ax1_ax2_ax3_fused_1 // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused_1 % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 < 36)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 7, 1, 3, 1, 1, 1, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_3)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 3, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 42])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #20: GFLOPs: 87.5097. Time: 0.0344 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(21, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4_init)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 7680 // 120)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 120 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 7680)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 4, 1, 1, 1, 1, 1, 2, 2, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 8, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #22: GFLOPs: 71.9169. Time: 0.0419 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #23: GFLOPs: 26.2124. Time: 0.1149 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #24: GFLOPs: 74.9311. Time: 0.0402 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #25: GFLOPs: 83.6821. Time: 0.0360 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #26: GFLOPs: 115.4056. Time: 0.0261 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(21, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 2, 2, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 2 + i1_4_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4_init)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 7168 // 112)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 112 // 28)
                                    v3 = T.axis.spatial(30, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 64)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 2, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 1, 8, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 112])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l155)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #28: GFLOPs: 64.5753. Time: 0.0466 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #29: GFLOPs: 24.7373. Time: 0.1217 ms. Best GFLOPs: 302.1931
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(192, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 192, 1, 1), "float32"], T_relu: T.Buffer[(1, 192, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 192, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([192, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4_init)
                                i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5760 // 120)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 120 // 4)
                                            v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 5760)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 2, 3, 1, 1, 4, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_4)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [192, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(192, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 6, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 84, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:13] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_nn_conv2d_add_nn_relu_9"] Trial #31: GFLOPs: 42.6205. Time: 0.0706 ms. Best GFLOPs: 302.1931
[04:12:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_nn_conv2d_add_nn_relu_9"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |            N/A |          N/A |                   N/A |      0 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 479
Total latency (us): 303.736

[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #0: GFLOPs: 78.9286. Time: 0.2447 ms. Best GFLOPs: 78.9286
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 56)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 56 // 2)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(192, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 16, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(192, i4_0 * 24 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 1, 16, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 6, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i3_4_init in T.grid(2, 7, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(98):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 7, 1, 6, 1, 1, 1, 2, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 12 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 7, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 2, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 32])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i2_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 896 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9408)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 2, 12, 1, 1, 1, 1, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(192, i4_0 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 1, 12])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #4: GFLOPs: 64.2309. Time: 0.3008 ms. Best GFLOPs: 78.9286
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(28, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(28, i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2352)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(192, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 28, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(28, i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 28, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 28])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 3, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2352)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 6)
                                        v1 = T.axis.spatial(192, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 384)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 6 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 3, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 224, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #7: GFLOPs: 9.1404. Time: 2.1134 ms. Best GFLOPs: 78.9286
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #8: GFLOPs: 5.1499. Time: 3.7511 ms. Best GFLOPs: 78.9286
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #9: GFLOPs: 471.7809. Time: 0.0409 ms. Best GFLOPs: 471.7809
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #10: GFLOPs: 1096.4038. Time: 0.0176 ms. Best GFLOPs: 1096.4038
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_4_init, i3_4_init in T.grid(4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i2_4_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(49):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 784 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 4)
                                        v1 = T.axis.spatial(192, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + i2_4)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 4 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 14 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[48, 2, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 64, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #12: GFLOPs: 626.5425. Time: 0.0308 ms. Best GFLOPs: 1096.4038
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 56)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 56 // 2)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 24)
                                    v1 = T.axis.spatial(192, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 24)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1536)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 16, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(192, i4_0 * 24 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 32, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 1, 16, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 6, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 56])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(4, 4, 2, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 12)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 4, 2, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 12 + i4_1 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 2, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 4, 4, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #15: GFLOPs: 1096.9488. Time: 0.0176 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #16: GFLOPs: 167.8040. Time: 0.1151 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #17: GFLOPs: 746.9051. Time: 0.0259 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #18: GFLOPs: 78.3427. Time: 0.2466 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #19: GFLOPs: 173.0919. Time: 0.1116 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 8, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(192, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(192, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1024)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 7, 1, 1, 1, 1, 8, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(192, i4_0 * 16 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 7 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 8])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[12, 16, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #21: GFLOPs: 316.1185. Time: 0.0611 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(2, 28):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 1176)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(192, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 28, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                yy = T.axis.spatial(28, i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(192, i4_0 * 3 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 28, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(28, ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 28])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[64, 1, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 2])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #23: GFLOPs: 140.0025. Time: 0.1380 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3_init * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(74):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 392 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 < 4704)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 12)
                                    v1 = T.axis.spatial(192, i4_0 * 12 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 12)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_3 * 7 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 12 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 2, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 6, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 64])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #25: GFLOPs: 736.1918. Time: 0.0262 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #26: GFLOPs: 413.9977. Time: 0.0467 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #27: GFLOPs: 168.2161. Time: 0.1148 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #28: GFLOPs: 246.4841. Time: 0.0784 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #29: GFLOPs: 430.5013. Time: 0.0449 ms. Best GFLOPs: 1096.9488
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #15: "fused_nn_conv2d_add_3"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 192, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 192, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 192, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 192, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(98, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(96, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(192, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 98)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 98 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1 < 196)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 2)
                                    v1 = T.axis.spatial(192, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 49 // 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(192, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 192, 28, 28], "float32"], ["TENSOR", [64, 192, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 49 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 7 + i0_1_i1_1_i2_1_i3_1_fused % 49 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[96, 1, 2])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 32])
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l159, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l159, ann_key="pragma_unroll_explicit", ann_val=1)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
[04:12:17] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_nn_conv2d_add_3"] Trial #31: GFLOPs: 663.1185. Time: 0.0291 ms. Best GFLOPs: 1096.9488
[04:12:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_nn_conv2d_add_3"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |            N/A |          N/A |                   N/A |      0 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 511
Total latency (us): 321.347

[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #0: GFLOPs: 199.2845. Time: 0.0302 ms. Best GFLOPs: 199.2845
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #1: GFLOPs: 66.3682. Time: 0.0907 ms. Best GFLOPs: 199.2845
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #2: GFLOPs: 86.3767. Time: 0.0697 ms. Best GFLOPs: 199.2845
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(48, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(118):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11520 // 480)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 480 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 11520)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 216)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i2_3_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3_init)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i2_3)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 24, 1, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 49, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 49])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #4: GFLOPs: 106.3168. Time: 0.0566 ms. Best GFLOPs: 199.2845
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(64, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i2_4_init, i3_4_init in T.grid(7, 2, 4):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i3_3_init * 4 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5376 // 448)
                                            v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 448 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 3, 1, 1, 1, 2, 4):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                    j = T.axis.spatial(28, i3_3 * 4 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 28):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 12, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 84, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(90):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 180)
                                        v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 180 // 6)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 6, 7, 1):
                            for i2_4_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_3)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3 * 2 + i2_4_init)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 2, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + i1_3)
                                    i = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 8, 6, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 7, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 32, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #7: GFLOPs: 71.1600. Time: 0.0846 ms. Best GFLOPs: 199.2845
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #8: GFLOPs: 292.2919. Time: 0.0206 ms. Best GFLOPs: 292.2919
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(336, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 180)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 180 // 30)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(4, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7)
                                j = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 12, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 336, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 336, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #10: GFLOPs: 110.3169. Time: 0.0546 ms. Best GFLOPs: 292.2919
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init in T.grid(3, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(72):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 4032 // 42)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 42 // 7)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 288)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 4 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 8, 4, 1, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 56])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 56, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #12: GFLOPs: 147.0395. Time: 0.0409 ms. Best GFLOPs: 292.2919
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 4, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 4 + i1_4_init)
                            i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2688 // 112)
                                        v2 = T.axis.spatial(30, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 112 // 4)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(1, 0)
                                    v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 24)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 4 + i1_4)
                                i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 3, 2, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 42])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #14: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_4_init in T.grid(3, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 3 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 3072 // 32)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 32 // 2)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 3, 1, 1, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 3 + i1_3)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 28 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 3 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 28 // 14 * 14 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 32, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 32])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 32, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #15: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init in T.serial(16):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3_init)
                            i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                            j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(36):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9216 // 24)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 24 // 6)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 16, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 128 + i0_2_i1_2_i2_2_i3_2_fused // 8 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 8 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 8, 16, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 64, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 64, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #16: GFLOPs: 292.5975. Time: 0.0206 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #17: GFLOPs: 82.2534. Time: 0.0732 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #18: GFLOPs: 222.1962. Time: 0.0271 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #19: GFLOPs: 222.1929. Time: 0.0271 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #20: GFLOPs: 267.8617. Time: 0.0225 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i2_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + i2_4_init)
                                j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 5376 // 168)
                                            v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 168 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + i1_4)
                                    i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + i2_4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 2, 8, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6144 // 96)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 96 // 16)
                                        v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6144)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 576)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i2_3_init, i1_4_init in T.grid(4, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3_init)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 4, 1, 1, 3, 1, 2, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i2_3)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 4, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 32, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 4, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 224, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 224, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #23: GFLOPs: 112.5516. Time: 0.0535 ms. Best GFLOPs: 292.5975
[04:12:20] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(84, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":64, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    with T.block("DepthwiseConv2d_init"):
                        b = T.axis.spatial(1, 0)
                        c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                        i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                        j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                        T.reads()
                        T.writes(DepthwiseConv2d_local[b, c, i, j])
                        T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                        DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(37):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 4096 // 64)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 64 // 16)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 16)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 < 4096)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 64 + i0_1_i1_1_i2_1_i3_1_fused // 4 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 14 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 2 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 8, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 4, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 112])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 112, 4])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #25: GFLOPs: 162.3453. Time: 0.0371 ms. Best GFLOPs: 292.5975
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(168, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(45):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10080 // 840)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 840 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 28 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 // 7 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 6, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #27: GFLOPs: 162.8422. Time: 0.0370 ms. Best GFLOPs: 292.5975
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #28: GFLOPs: 145.6773. Time: 0.0413 ms. Best GFLOPs: 292.5975
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(55):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 4608 // 96)
                                    v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 96 // 6)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 6)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 < 4608)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i3_3_init in T.grid(8, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3_init)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 8, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_3)
                                i = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2)
                                j = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + i3_3)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 3, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 84])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 84, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l159)
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #30: GFLOPs: 190.1230. Time: 0.0317 ms. Best GFLOPs: 292.5975
[04:12:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_conv2d_add_nn_relu_10"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(84, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                            i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4)
                            j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(23):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 10080 // 840)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 840 // 28)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 10080)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 56)
                                    i = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4)
                                    j = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 56 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 56 // 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 4 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 6, 2, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 14, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 4, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_conv2d_add_nn_relu_10"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |            N/A |          N/A |                   N/A |      0 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 543
Total latency (us): 383.081

[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(196, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i2_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i2_3_init)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4704)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 6)
                                    v1 = T.axis.spatial(384, i4_0 * 6 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 6)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 4, 1, 6, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + i2_3)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28)
                                rc = T.axis.reduce(384, i4_0 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 4, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused // 28 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 28 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 64, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 4, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 28, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 4, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(384, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #2: GFLOPs: 172.2905. Time: 0.2242 ms. Best GFLOPs: 172.2905
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i3_3_init, i3_4_init in T.grid(7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 256 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 896)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 7, 4, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 4 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 4 + i0_1_i1_1_i2_1_i3_1_fused % 4 // 2 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 32, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 2, 1, 7, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 64, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 64])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(224, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(7, 8, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + i2_4_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 784)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 784 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(224, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 7, 8, 1, 1, 1, 8, 4, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + i2_4)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + i3_3)
                                rc = T.axis.reduce(384, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 4, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 4 * 4 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 4, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 224, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 224])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #5: GFLOPs: 27.4430. Time: 1.4078 ms. Best GFLOPs: 172.2905
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #6: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init, i3_4_init in T.grid(2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(112):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 28 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 128)
                                        v1 = T.axis.spatial(384, i4_0 * 128 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 128)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 128 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 64, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #7: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(2, 2, 4, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(6, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 112)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 112 // 4)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 4)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 64)
                                        v1 = T.axis.spatial(384, i4_0 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 64)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2048)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 2, 16, 1, 1, 1, 4, 2, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + i2_4)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 64 + i4_1 * 16 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[6, 4, 16])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #8: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(112, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(128, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(384, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 96)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(384, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 16, 2, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 2, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[128, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #9: GFLOPs: 457.3018. Time: 0.0845 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #10: GFLOPs: 324.4241. Time: 0.1191 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #11: GFLOPs: 344.6529. Time: 0.1121 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #12: GFLOPs: 171.7465. Time: 0.2250 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #13: GFLOPs: 354.9363. Time: 0.1089 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #14: GFLOPs: 375.4600. Time: 0.1029 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #15: GFLOPs: 445.2278. Time: 0.0868 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #16: GFLOPs: 31.2613. Time: 1.2359 ms. Best GFLOPs: 457.3018
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #17: GFLOPs: 521.8343. Time: 0.0740 ms. Best GFLOPs: 521.8343
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #18: GFLOPs: 1365.0616. Time: 0.0283 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #19: GFLOPs: 9.6609. Time: 3.9992 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #20: GFLOPs: 521.1642. Time: 0.0741 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #21: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init in T.serial(4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 98)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 98 // 7)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 784)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(10):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 512)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 4, 1, 1, 4, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(384, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_1_i1_1_i2_1_i3_1_fused // 7 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 4 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused // 4 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 4, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[4, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i3_4_init in T.grid(16, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(384, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 16, 1, 1, 2, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + i1_3)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 24 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_2_i1_2_i2_2_i3_2_fused // 28 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 1, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 12, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #23: GFLOPs: 1017.7915. Time: 0.0380 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #24: GFLOPs: 624.3015. Time: 0.0619 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #25: GFLOPs: 667.4648. Time: 0.0579 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #26: GFLOPs: 43.3788. Time: 0.8907 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #27: GFLOPs: 508.8963. Time: 0.0759 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #28: GFLOPs: 507.4629. Time: 0.0761 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(64, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 64, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 64, 28, 28), "float32"], T_add: T.Buffer[(1, 64, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([64, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_4_init, i3_4_init in T.grid(2, 4):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(384, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 256)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 8, 1, 1, 1, 2, 1, 4):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [64, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_i3_0_fused // 2 * 32 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 14 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 56, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 56])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #30: GFLOPs: 348.4574. Time: 0.1109 ms. Best GFLOPs: 1365.0616
[04:12:25] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_nn_conv2d_add_add_2"] Trial #31: GFLOPs: 1020.1358. Time: 0.0379 ms. Best GFLOPs: 1365.0616
[04:12:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_nn_conv2d_add_add_2"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |            N/A |          N/A |                   N/A |      0 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 575
Total latency (us): 467.991

[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #0: GFLOPs: 157.5311. Time: 0.2484 ms. Best GFLOPs: 157.5311
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #1: GFLOPs: 937.7427. Time: 0.0417 ms. Best GFLOPs: 937.7427
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #2: GFLOPs: 754.2128. Time: 0.0519 ms. Best GFLOPs: 937.7427
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #3: GFLOPs: 1125.0285. Time: 0.0348 ms. Best GFLOPs: 1125.0285
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #4: GFLOPs: 98.1318. Time: 0.3988 ms. Best GFLOPs: 1125.0285
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #5: GFLOPs: 115.9158. Time: 0.3376 ms. Best GFLOPs: 1125.0285
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #6: GFLOPs: 1183.6497. Time: 0.0331 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #7: GFLOPs: 168.2017. Time: 0.2327 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #8: GFLOPs: 40.8287. Time: 0.9586 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #9: GFLOPs: 311.9046. Time: 0.1255 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #10: GFLOPs: 18.2983. Time: 2.1389 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #11: GFLOPs: 359.1216. Time: 0.1090 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #12: GFLOPs: 69.1005. Time: 0.5664 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 392)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 392 // 14)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 1, 2, 7, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 2, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 4 + i0_2_i1_2_i2_2_i3_2_fused // 28 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 // 2 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 2 * 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 12, 4, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 1, 2, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #14: GFLOPs: 386.8989. Time: 0.1012 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #15: GFLOPs: 79.5713. Time: 0.4919 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #16: GFLOPs: 17.7676. Time: 2.2027 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(4, 14, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3_init)
                            xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 768 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 768 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 392 // 14)
                                        v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 768 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3136)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 384 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(8, 1, 1, 1, 4, 14, 1, 1, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + i2_3)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 14, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_2_i1_2_i2_2_i3_2_fused // 4 * 8 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 * 14 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 48, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 14, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 192, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 192, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #18: GFLOPs: 420.5408. Time: 0.0931 ms. Best GFLOPs: 1183.6497
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i3_4_init in T.grid(2, 8, 14):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(64, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1)
                                    v1 = T.axis.spatial(64, i4_0)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 384)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 8, 1, 14):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + i3_4)
                                rc = T.axis.reduce(64, i4_0)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 14):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_1_i1_1_i2_1_i3_1_fused // 2 * 64 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 16 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused * 14 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 2 * 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 6, 4, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 14])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #20: GFLOPs: 1238.9581. Time: 0.0316 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #21: GFLOPs: 840.5015. Time: 0.0466 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #22: GFLOPs: 2.0248. Time: 19.3290 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(392, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init in T.grid(2, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + i1_4_init)
                            yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_3_init)
                            xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) // 784)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 784 // 28)
                                    v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 28)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(392, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 128 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) // 8)
                                    v1 = T.axis.spatial(64, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1) % 8)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 392 + ax0_ax1_ax2_ax3_fused_1 < 1024)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 2, 4, 8, 1, 1, 1, 2, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + i1_4)
                                yy = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + i2_3)
                                xx = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + i3_3)
                                rc = T.axis.reduce(64, i4_0 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 128 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 16 + i0_2_i1_2_i2_2_i3_2_fused // 49 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 2 * 14 + i0_2_i1_2_i2_2_i3_2_fused % 49 // 7 * 2 + ax2)
                            v3 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 8, 8, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 7, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 4, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 1, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 392])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 392])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #24: GFLOPs: 132.4584. Time: 0.2955 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(128, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init in T.serial(6):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 392)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 392 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 16)
                                        v1 = T.axis.spatial(64, i4_0 * 16 + (ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 6, 1, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(64, i4_0 * 16 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 96 + i0_1_i1_1_i2_1_i3_1_fused // 8 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i0_1_i1_1_i2_1_i3_1_fused % 8 // 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 4 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 16, 1, 6, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 2, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 2, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 49, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 49, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #26: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i3_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_3_init * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 112)
                                        v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 112 // 28)
                                        v3 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 28)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3584)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 192 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 32)
                                    v1 = T.axis.spatial(64, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 32)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 2, 8, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                yy = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                xx = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + i3_3 * 2 + i3_4)
                                rc = T.axis.reduce(64, i4_0 * 32 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 4):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + i0_1_i1_1_i2_1_i3_1_fused % 14 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax2)
                            v3 = T.axis.spatial(28, i0_1_i1_1_i2_1_i3_1_fused % 7 * 4 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 4, 24, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 2, 2, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[2, 4, 8])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 48])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #27: GFLOPs: 8.6492. Time: 4.5250 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #28: GFLOPs: 109.9397. Time: 0.3560 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 64, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 64, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 28, 28), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 28, 28], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 64, 28, 28], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 64, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(224, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i4_0, i5_0, i6_0 in T.grid(1, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) // 56)
                                    v2 = T.axis.spatial(28, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 56 // 2)
                                    v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) % 2)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 64)
                                        v1 = T.axis.spatial(64, (ax0_ax1_ax2_ax3_fused_0 * 448 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 64)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1536)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i3_3_init, i1_4_init in T.grid(2, 6):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + i1_4_init)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(16, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 6, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + i1_4)
                                yy = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28)
                                xx = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i3_3)
                                rc = T.axis.reduce(64, i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 64, 28, 28], "float32"], ["TENSOR", [384, 64, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + ax1)
                            v2 = T.axis.spatial(28, i0_2_i1_2_i2_2_i3_2_fused % 28 + ax2)
                            v3 = T.axis.spatial(28, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 4, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 28, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[14, 1, 1, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 16, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 112])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l176)
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #30: GFLOPs: 911.9207. Time: 0.0429 ms. Best GFLOPs: 1238.9581
[04:12:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_conv2d_add_nn_relu_11"] Trial #31: GFLOPs: 1874.6864. Time: 0.0209 ms. Best GFLOPs: 1874.6864
[04:12:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_conv2d_add_nn_relu_11"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 607
Total latency (us): 551.497

[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #0: GFLOPs: 93.6705. Time: 0.0161 ms. Best GFLOPs: 93.6705
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #1: GFLOPs: 87.3727. Time: 0.0172 ms. Best GFLOPs: 93.6705
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #2: GFLOPs: 60.2179. Time: 0.0250 ms. Best GFLOPs: 93.6705
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #3: GFLOPs: 40.1506. Time: 0.0375 ms. Best GFLOPs: 93.6705
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #4: GFLOPs: 143.8074. Time: 0.0105 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #5: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(94):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 10440 // 435)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 435 // 29)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 10440)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_4_init, i2_4_init in T.grid(3, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + i1_4_init)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_4_init)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 3, 1, 3, 7, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + i1_4)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_4)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                di, dj = T.axis.remap("RR", [i4_1, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 3 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 4, 1, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #6: GFLOPs: 34.7403. Time: 0.0433 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #7: GFLOPs: 28.5085. Time: 0.0528 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #8: GFLOPs: 73.1914. Time: 0.0206 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #9: GFLOPs: 129.0827. Time: 0.0117 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #10: GFLOPs: 18.3063. Time: 0.0822 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #11: GFLOPs: 15.8969. Time: 0.0947 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(112, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(42):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 3480 // 145)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 145 // 29)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 3480)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 216)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 2, 2, 1):
                            for i3_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i2_3)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_3)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i2_3)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 24 + i0_1_i1_1_i2_1_i3_1_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 2, 6, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 42, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(56, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 6960 // 145)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 145 // 29)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6960)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 3, 1, 1):
                            for i1_4_init, i3_4_init in T.grid(2, 2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_3 * 2 + i1_4_init)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 1, 2):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 48 + i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 6 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 3, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l167)
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #14: GFLOPs: 23.1422. Time: 0.0650 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #15: GFLOPs: 21.9342. Time: 0.0686 ms. Best GFLOPs: 143.8074
[04:12:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(32, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_3_init, i2_4_init in T.grid(7, 2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(14, i2_3_init * 2 + i2_4_init)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(38):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 9396 // 783)
                                            v2 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 783 // 27)
                                            v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 9396)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 36)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 7, 2, 3, 1, 1, 1, 2, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    i = T.axis.spatial(14, i2_3 * 2 + i2_4)
                                    j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 12 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 12, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 84, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #17: GFLOPs: 264.7014. Time: 0.0057 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #18: GFLOPs: 63.1883. Time: 0.0238 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(96, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_4_init in T.grid(7, 2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_3_init)
                            j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 3016 // 377)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 2 * 14 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 377 // 29)
                                        v3 = T.axis.spatial(30, ((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 29)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3016)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 24)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 1, 7, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i2_3)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 2 * 8 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[48, 1, 8, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #20: GFLOPs: 79.6726. Time: 0.0189 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #21: GFLOPs: 109.9128. Time: 0.0137 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #22: GFLOPs: 60.9266. Time: 0.0247 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #23: GFLOPs: 20.8584. Time: 0.0722 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #24: GFLOPs: 37.8772. Time: 0.0397 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #25: GFLOPs: 67.9263. Time: 0.0222 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #26: GFLOPs: 64.1562. Time: 0.0235 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #27: GFLOPs: 25.0199. Time: 0.0602 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #28: GFLOPs: 53.5255. Time: 0.0281 ms. Best GFLOPs: 264.7014
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #29: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(42, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(145):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 9280 // 145)
                                    v2 = T.axis.spatial(30, (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 145 // 5)
                                    v3 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 7 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 64 + ax0_ax1_ax2_ax3_fused_1) % 5)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 1, 2, 1):
                            for i2_4_init in T.serial(7):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(14, i2_3 * 7 + i2_4_init)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 1, 7, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i = T.axis.spatial(14, i2_3 * 7 + i2_4)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 14, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 7 * 64 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 64, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 2, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 64])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 64, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 28, 28), "float32"], placeholder_1: T.Buffer[(384, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 384, 30, 30], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(192, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                            j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 3):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 192 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 5184 // 27)
                                        v2 = T.axis.spatial(30, i0_0_i1_0_i2_0_i3_0_fused % 14 * 2 + i4_0 + 0)
                                        v3 = T.axis.spatial(30, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 27)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 192 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 5184)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 29 and 1 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(192, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 192 + (ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2))
                                        v1 = T.axis.spatial(1, 0)
                                        v2, v3 = T.axis.remap("SS", [i4_0, i5_0])
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2 < 192)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + i1_3)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14)
                                j = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_0])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 384, 28, 28], "float32"], ["TENSOR", [384, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 14 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 2 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 14 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 96, 2, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 192, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 192, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_nn_conv2d_add_nn_relu_12"] Trial #31: GFLOPs: 36.7857. Time: 0.0409 ms. Best GFLOPs: 264.7014
[04:12:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_nn_conv2d_add_nn_relu_12"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |            N/A |          N/A |                   N/A |      0 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 639
Total latency (us): 557.184

[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #0: GFLOPs: 6.8247. Time: 2.1202 ms. Best GFLOPs: 6.8247
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #1: GFLOPs: 40.2181. Time: 0.3598 ms. Best GFLOPs: 40.2181
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #2: GFLOPs: 1074.2018. Time: 0.0135 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #3: GFLOPs: 283.4982. Time: 0.0510 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #4: GFLOPs: 375.4020. Time: 0.0385 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #5: GFLOPs: 148.0243. Time: 0.0978 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #6: GFLOPs: 183.3370. Time: 0.0789 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #7: GFLOPs: 524.8990. Time: 0.0276 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #8: GFLOPs: 236.6810. Time: 0.0611 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #9: GFLOPs: 577.4602. Time: 0.0251 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #10: GFLOPs: 110.5869. Time: 0.1308 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_conv2d_add_4"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(28, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 3):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + i1_3_init * 3 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(384, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(4, 1, 1, 1, 2, 1, 1, 6, 1, 1, 1, 3, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + i1_3 * 3 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(384, i4_0 * 24 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 28 * 6 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 28 // 14 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 4, 2, 3])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[16, 4, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #12: GFLOPs: 231.9171. Time: 0.0624 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #13: GFLOPs: 773.7146. Time: 0.0187 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #14: GFLOPs: 543.7256. Time: 0.0266 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #15: GFLOPs: 744.0875. Time: 0.0194 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #16: GFLOPs: 6.3371. Time: 2.2833 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #17: GFLOPs: 317.5489. Time: 0.0456 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #18: GFLOPs: 302.2934. Time: 0.0479 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #19: GFLOPs: 309.0711. Time: 0.0468 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #20: GFLOPs: 83.3631. Time: 0.1736 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #21: GFLOPs: 68.4586. Time: 0.2114 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #22: GFLOPs: 881.8857. Time: 0.0164 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_conv2d_add_4"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(21, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_4_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(2, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i4_0 * 192 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 28)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 28 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 192)
                                        v1 = T.axis.spatial(384, i4_0 * 192 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 192)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(48, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 7)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(384, i4_0 * 192 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 1, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 7 * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[3, 1, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[2, 48, 4])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #24: GFLOPs: 481.7214. Time: 0.0300 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #20: "fused_nn_conv2d_add_4"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 384, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 384, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i1_3_init in T.serial(8):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_3_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                            xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(96):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 98)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 98 // 7)
                                    v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 48)
                                        v1 = T.axis.spatial(384, i4_0 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 48)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 8, 1, 1, 8, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + i1_3)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7)
                                xx = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(384, i4_0 * 48 + i4_1 * 8 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 384, 14, 14], "float32"], ["TENSOR", [96, 384, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 2 * 24 + i0_1_i1_1_i2_1_i3_1_fused // 2 * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 2 * 7 + i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 3, 1, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[8, 6, 8])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 49])
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 49, 3])
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l161, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l161, ann_key="pragma_unroll_explicit", ann_val=1)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #26: GFLOPs: 337.5009. Time: 0.0429 ms. Best GFLOPs: 1074.2018
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #27: GFLOPs: 1209.8350. Time: 0.0120 ms. Best GFLOPs: 1209.8350
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #28: GFLOPs: 558.5512. Time: 0.0259 ms. Best GFLOPs: 1209.8350
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #29: GFLOPs: 290.0898. Time: 0.0499 ms. Best GFLOPs: 1209.8350
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #30: GFLOPs: 193.4478. Time: 0.0748 ms. Best GFLOPs: 1209.8350
[04:12:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_nn_conv2d_add_4"] Trial #31: GFLOPs: 6.3973. Time: 2.2618 ms. Best GFLOPs: 1209.8350
[04:12:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_nn_conv2d_add_4"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |            N/A |          N/A |                   N/A |      0 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 671
Total latency (us): 569.144

[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #0: GFLOPs: 44.4649. Time: 0.0508 ms. Best GFLOPs: 44.4649
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #1: GFLOPs: 262.1755. Time: 0.0086 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #2: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(98, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 12):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 49 * 288 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 24 + i1_3_init * 12 + i1_4_init)
                            i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                            j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 49 * 288 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 2304 // 8)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8 // 4)
                                        v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 4)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 49 * 288 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 3, 1, 2, 1, 1, 1, 1, 1, 12, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 49 * 288 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 24 + i1_3 * 12 + i1_4)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2)
                                di, dj = T.axis.remap("RR", [i4_0, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 49 * 288 + i0_2_i1_2_i2_2_i3_2_fused // 4 * 24 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 49 // 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 4 // 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_2_i1_2_i2_2_i3_2_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 12, 2, 12])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 2, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 48, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 48, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #3: GFLOPs: 97.7009. Time: 0.0231 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #4: GFLOPs: 80.4525. Time: 0.0281 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #5: GFLOPs: 100.3956. Time: 0.0225 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #6: GFLOPs: 75.4838. Time: 0.0299 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #7: GFLOPs: 74.8602. Time: 0.0302 ms. Best GFLOPs: 262.1755
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #8: GFLOPs: 297.3311. Time: 0.0076 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #9: GFLOPs: 42.4100. Time: 0.0532 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #10: GFLOPs: 90.9513. Time: 0.0248 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #11: GFLOPs: 95.7231. Time: 0.0236 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #12: GFLOPs: 256.8287. Time: 0.0088 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #13: GFLOPs: 109.5341. Time: 0.0206 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #14: GFLOPs: 48.8798. Time: 0.0462 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #15: GFLOPs: 56.2849. Time: 0.0401 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #16: GFLOPs: 148.7866. Time: 0.0152 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(49, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i1_4_init in T.grid(2, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 72 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i2_3_init)
                                j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(64):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(576, ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 4608 // 8)
                                            v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 8 // 2)
                                            v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(576, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 108 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 3, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 72 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + i2_3)
                                    j = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 2, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 72 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused // 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + i0_1_i1_1_i2_1_i3_1_fused % 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 8, 36, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 2, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 36, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 36, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #18: GFLOPs: 211.7885. Time: 0.0107 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #19: GFLOPs: 108.5620. Time: 0.0208 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(110):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 36 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 256)
                                        v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 256 // 16)
                                        v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 16)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9216)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 36 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 324)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i1_3_init, i2_3_init, i3_4_init in T.grid(6, 14, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_3_init)
                                i = T.axis.spatial(14, i2_3_init)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 6, 14, 1, 1, 1, 1, 1, 1, 2):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_3)
                                i = T.axis.spatial(14, i2_3)
                                j = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                di, dj = T.axis.remap("RR", [i4_1, i5_1])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 14, 2):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 36 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[16, 1, 6, 6, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 14, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 42, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 42, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l161)
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #21: GFLOPs: 214.9301. Time: 0.0105 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #22: GFLOPs: 120.3557. Time: 0.0188 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #23: GFLOPs: 80.6029. Time: 0.0280 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #24: GFLOPs: 65.1059. Time: 0.0347 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #25: GFLOPs: 66.3486. Time: 0.0340 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #26: GFLOPs: 141.1710. Time: 0.0160 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #27: GFLOPs: 23.7547. Time: 0.0951 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #28: GFLOPs: 270.7311. Time: 0.0083 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #29: GFLOPs: 90.6191. Time: 0.0249 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #30: GFLOPs: 29.4860. Time: 0.0766 ms. Best GFLOPs: 297.3311
[04:12:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_nn_conv2d_add_nn_relu_13"] Trial #31: GFLOPs: 15.1105. Time: 0.1494 ms. Best GFLOPs: 297.3311
[04:13:03] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_nn_conv2d_add_nn_relu_13"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |            N/A |          N/A |                   N/A |      0 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 703
Total latency (us): 584.332

[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #0: GFLOPs: 129.8508. Time: 0.1672 ms. Best GFLOPs: 129.8508
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #1: GFLOPs: 27.9494. Time: 0.7769 ms. Best GFLOPs: 129.8508
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #2: GFLOPs: 44.8350. Time: 0.4843 ms. Best GFLOPs: 129.8508
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #3: GFLOPs: 462.8414. Time: 0.0469 ms. Best GFLOPs: 462.8414
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #4: GFLOPs: 151.9916. Time: 0.1429 ms. Best GFLOPs: 462.8414
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #5: GFLOPs: 98.5836. Time: 0.2203 ms. Best GFLOPs: 462.8414
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #6: GFLOPs: 45.3860. Time: 0.4784 ms. Best GFLOPs: 462.8414
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #7: GFLOPs: 477.6694. Time: 0.0455 ms. Best GFLOPs: 477.6694
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #8: GFLOPs: 10.9504. Time: 1.9829 ms. Best GFLOPs: 477.6694
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #9: GFLOPs: 101.5849. Time: 0.2137 ms. Best GFLOPs: 477.6694
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 576, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(3, 4, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_3_init * 4 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(63):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(31):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 18)
                                    v1 = T.axis.spatial(576, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 18)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 1728)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 3, 1, 1, 18, 1, 1, 1, 4, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_3 * 4 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused)
                                rc = T.axis.reduce(576, i4_0 * 18 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 3, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 18])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118 = sch.split(loop=l116, factors=[None, 56])
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
sch.annotate(block_or_loop=l124, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l124, ann_key="pragma_unroll_explicit", ann_val=1)
l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l132, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l132, ann_key="pragma_unroll_explicit", ann_val=1)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l140, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l140, ann_key="pragma_unroll_explicit", ann_val=1)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l160, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l160, ann_key="pragma_unroll_explicit", ann_val=1)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #11: GFLOPs: 593.8540. Time: 0.0366 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #12: GFLOPs: 255.7902. Time: 0.0849 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 576, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(24, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 28)
                                        v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 28 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(576, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 2304)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 2, 1, 2, 12, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(576, i4_0 * 24 + i4_1 * 12 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_0_i1_0_i2_0_i3_0_fused * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 6, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[24, 2, 12])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 42, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 42, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #14: GFLOPs: 97.1758. Time: 0.2234 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #15: GFLOPs: 242.1658. Time: 0.0897 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #16: GFLOPs: 139.1187. Time: 0.1561 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #17: GFLOPs: 354.5774. Time: 0.0612 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #18: GFLOPs: 286.2964. Time: 0.0758 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #19: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 576, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(98, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(6, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 196 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(98, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 24 + (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 18)
                                        v1 = T.axis.spatial(576, i4_0 * 18 + (ax0_ax1_ax2_ax3_fused_0 * 294 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 18)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 98 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 6, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14)
                                rc = T.axis.reduce(576, i4_0 * 18 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused * 24 + i0_1_i1_1_i2_1_i3_1_fused * 6 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 14, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 3, 6])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 98, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 98, 3])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #20: GFLOPs: 77.9587. Time: 0.2785 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #21: GFLOPs: 35.8771. Time: 0.6052 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #22: GFLOPs: 113.8752. Time: 0.1907 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(96, 576, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 96, 14, 14), "float32"], T_add: T.Buffer[(1, 96, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 576, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(42, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init, i2_4_init in T.grid(2, 8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(18, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(150):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1 < 6272)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(25):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(42, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 32)
                                        v1 = T.axis.spatial(576, i4_0 * 32 + (ax0_ax1_ax2_ax3_fused_0 * 126 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 32)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 42 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 3072)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(32, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 8, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(576, i4_0 * 32 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [96, 576, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 24 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 8 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 3, 1, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[18, 32, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 42])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 42, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #24: GFLOPs: 152.2381. Time: 0.1426 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #25: GFLOPs: 12.4805. Time: 1.7398 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #26: GFLOPs: 314.7094. Time: 0.0690 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #27: GFLOPs: 335.7981. Time: 0.0647 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #28: GFLOPs: 489.1850. Time: 0.0444 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #29: GFLOPs: 482.8919. Time: 0.0450 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #30: GFLOPs: 36.6916. Time: 0.5918 ms. Best GFLOPs: 593.8540
[04:13:04] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_nn_conv2d_add_add_3"] Trial #31: GFLOPs: 158.7448. Time: 0.1368 ms. Best GFLOPs: 593.8540
[04:13:18] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_nn_conv2d_add_add_3"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |            N/A |          N/A |                   N/A |      0 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 735
Total latency (us): 657.46

[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #0: GFLOPs: 676.2537. Time: 0.0324 ms. Best GFLOPs: 676.2537
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #1: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(7, 2, 9, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 144 + i0_1_i1_1_i2_1_i3_1_fused * 72 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 9 + i1_4_init)
                            yy = T.axis.spatial(14, i2_3_init * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(12, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(28):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 144 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 8)
                                        v1 = T.axis.spatial(96, i4_0 * 8 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 8)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 7, 2, 4, 1, 1, 1, 9, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 144 + i0_1_i1_1_i2_1_i3_1_fused * 72 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 9 + i1_4)
                                yy = T.axis.spatial(14, i2_3 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(96, i4_0 * 8 + i4_1 * 4 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 9, 14, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 144 + i0_1_i1_1_i2_1_i3_1_fused * 72 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 9 + ax1)
                            v2 = T.axis.spatial(14, ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 8, 1, 9])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[12, 2, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #2: GFLOPs: 256.3907. Time: 0.0854 ms. Best GFLOPs: 676.2537
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #3: GFLOPs: 940.0712. Time: 0.0233 ms. Best GFLOPs: 940.0712
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #4: GFLOPs: 648.4446. Time: 0.0338 ms. Best GFLOPs: 940.0712
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #5: GFLOPs: 352.8746. Time: 0.0621 ms. Best GFLOPs: 940.0712
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #6: GFLOPs: 1127.5035. Time: 0.0194 ms. Best GFLOPs: 1127.5035
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #7: GFLOPs: 1280.0776. Time: 0.0171 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #8: GFLOPs: 744.7603. Time: 0.0294 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #9: GFLOPs: 261.3523. Time: 0.0838 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #10: GFLOPs: 733.5410. Time: 0.0299 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #11: GFLOPs: 589.2555. Time: 0.0372 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #12: GFLOPs: 17.5717. Time: 1.2464 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(336, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init, i3_4_init in T.grid(2, 7, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 196)
                                        v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 196 // 14)
                                        v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 672 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 14)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(336, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + (ax0_ax1_ax2_ax3_fused_0 * 1344 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 1344 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 336 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 6912)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 7, 2):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_4)
                                rc = T.axis.reduce(96, i4_0 * 24 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 288 + i0_1_i1_1_i2_1_i3_1_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 14 * 2 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 14 // 7 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 6, 24, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 12, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 336, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 336, 4])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #14: GFLOPs: 18.8047. Time: 1.1647 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #15: GFLOPs: 20.9716. Time: 1.0444 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #16: GFLOPs: 73.3558. Time: 0.2986 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #17: GFLOPs: 995.9427. Time: 0.0220 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #18: GFLOPs: 91.2074. Time: 0.2401 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #19: GFLOPs: 378.5578. Time: 0.0579 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #20: GFLOPs: 16.8691. Time: 1.2983 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #21: GFLOPs: 56.1755. Time: 0.3899 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(12, 2, 2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 24 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_4_init)
                            xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(7):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 2)
                                        v1 = T.axis.spatial(96, i4_0 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 2)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 1152)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 12, 1, 2, 2, 1, 1, 1, 2, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 24 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + i2_4)
                                xx = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i3_3)
                                rc = T.axis.reduce(96, i4_0 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 24, 7, 2):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused // 2 * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 24 + ax1)
                            v2 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 2 * 7 + ax2)
                            v3 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 8, 12, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 2, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #23: GFLOPs: 501.7710. Time: 0.0436 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #24: GFLOPs: 381.2649. Time: 0.0574 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #25: GFLOPs: 507.8727. Time: 0.0431 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #26: GFLOPs: 1024.1800. Time: 0.0214 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #27: GFLOPs: 1016.2405. Time: 0.0216 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #28: GFLOPs: 527.0927. Time: 0.0416 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #29: GFLOPs: 501.7560. Time: 0.0437 ms. Best GFLOPs: 1280.0776
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 96, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 14, 14), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 576, 14, 14], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 96, 14, 14], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 96, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(56, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 2, 2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4_init)
                            xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(56):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 196)
                                    v2 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 196 // 14)
                                    v3 = T.axis.spatial(14, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 14)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 192 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(96, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 252 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4608)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(12, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 2, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + i2_4)
                                xx = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14)
                                rc = T.axis.reduce(96, i4_0 * 24 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 96, 14, 14], "float32"], ["TENSOR", [576, 96, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 2, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 14 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 4 + ax1)
                            v2 = T.axis.spatial(14, i0_2_i1_2_i2_2_i3_2_fused % 7 * 2 + ax2)
                            v3 = T.axis.spatial(14, i0_1_i1_1_i2_1_i3_1_fused % 14 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[3, 4, 12, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 14, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 12, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 84])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 84, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:19] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_conv2d_add_nn_relu_14"] Trial #31: GFLOPs: 1121.0198. Time: 0.0195 ms. Best GFLOPs: 1280.0776
[04:13:29] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_conv2d_add_nn_relu_14"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 767
Total latency (us): 708.789

[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #0: GFLOPs: 14.5975. Time: 0.0387 ms. Best GFLOPs: 14.5975
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #1: GFLOPs: 36.4304. Time: 0.0155 ms. Best GFLOPs: 36.4304
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #2: GFLOPs: 55.4080. Time: 0.0102 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #3: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(49, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(221):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) // 225)
                                    v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 225 // 15)
                                    v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1 < 10800)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(49, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 147 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 49 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3 in T.grid(1, 1, 1, 4, 1, 1):
                            for i1_4_init in T.serial(2):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 2 + i1_4_init)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 3, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 8 + i1_3 * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_2])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused // 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 6, 1, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 49])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 49, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l165)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(72, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i3_3_init, i1_4_init in T.grid(7, 2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                j = T.axis.spatial(7, i3_3_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(39):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 11232 // 39)
                                            v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 39 // 13)
                                            v3 = T.axis.spatial(16, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(12):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(72, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 3, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_3, i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 72, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 72, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 72])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #5: GFLOPs: 8.9760. Time: 0.0629 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #6: GFLOPs: 16.6102. Time: 0.0340 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #7: GFLOPs: 18.0460. Time: 0.0313 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #8: GFLOPs: 17.4205. Time: 0.0324 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #9: GFLOPs: 35.4489. Time: 0.0159 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(28, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init in T.serial(3):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 144 + i0_2_i1_2_i2_2_i3_2_fused * 3 + i1_3_init)
                                i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(117):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 144 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 5616 // 39)
                                        v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 39 // 13)
                                        v3 = T.axis.spatial(16, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 13)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(9):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 144 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 144 + i0_2_i1_2_i2_2_i3_2_fused * 3 + i1_3)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 144 + i0_2_i1_2_i2_2_i3_2_fused * 3 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 48, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 48])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107 = sch.split(loop=l105, factors=[None, 48])
sch.bind(loop=l107, thread_axis="threadIdx.x")
b108 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b108, ann_key="meta_schedule.unroll_explicit")
b109, b110, b111, b112 = sch.get_child_blocks(b108)
l113, l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b109)
sch.annotate(block_or_loop=l113, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l113, ann_key="pragma_unroll_explicit", ann_val=1)
l120, l121, l122, l123, l124, l125, l126 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l120, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l120, ann_key="pragma_unroll_explicit", ann_val=1)
l127, l128, l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l127, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l127, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
b151 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b151)
b169 = sch.decompose_reduction(block=b151, loop=l156)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #11: GFLOPs: 27.0268. Time: 0.0209 ms. Best GFLOPs: 55.4080
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #12: GFLOPs: 115.8025. Time: 0.0049 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #13: GFLOPs: 36.5485. Time: 0.0154 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #14: GFLOPs: 96.4823. Time: 0.0059 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #15: GFLOPs: 101.0011. Time: 0.0056 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init in T.serial(3):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3_init)
                            i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 9360 // 195)
                                        v2 = T.axis.spatial(16, i4_0 + ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 9360)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where(ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2 < 144)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 1, 16, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 112, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l159)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #17: GFLOPs: 19.4764. Time: 0.0290 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #18: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(144, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init in T.serial(2):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4_init)
                                i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                j = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(39):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 11232 // 39)
                                            v2 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused % 7 * 2 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 39 // 13)
                                            v3 = T.axis.spatial(16, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2) % 13)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(144, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 288 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_1_i1_1_i2_1_i3_1_fused, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused // 7 * 288 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 144, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 144, 2])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 144, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #19: GFLOPs: 43.8723. Time: 0.0129 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #20: GFLOPs: 17.3951. Time: 0.0325 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #21: GFLOPs: 5.7432. Time: 0.0983 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #22: GFLOPs: 21.7382. Time: 0.0260 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #23: GFLOPs: 13.8994. Time: 0.0406 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(7, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(84, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init in T.serial(12):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_4_init)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(26):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(576, ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 8640 // 15)
                                            v2 = T.axis.spatial(16, ((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                            v3 = T.axis.spatial(16, i0_0_i1_0_i2_0_i3_0_fused * 2 + i5_0 + 0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 8640)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(84, thread="threadIdx.x"):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                        v3 = T.axis.spatial(3, i5_0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 84 + ax0_ax1_ax2_ax3_fused_1 < 1728)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 12, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i0_0_i1_0_i2_0_i3_0_fused, i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_1_i1_1_i2_1_i3_1_fused * 144 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 12, 1, 12])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 84, 4])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 84])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #25: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(65):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 10800)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 9)
                                    v1 = T.axis.spatial(1, 0)
                                    v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 9 // 3)
                                    v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1 < 432)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init in T.serial(6):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_3_init)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 6, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + i1_3)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 6, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 6 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 1, 8, 6, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108 = sch.split(loop=l106, factors=[None, 56])
sch.bind(loop=l108, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120, l121 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l122, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l122, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l160)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #26: GFLOPs: 13.8101. Time: 0.0409 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 576, 14, 14), "float32"], placeholder_1: T.Buffer[(576, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 576, 1, 1), "float32"], T_relu: T.Buffer[(1, 576, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 576, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 576, 16, 16], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([576, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(12, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(14, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(65):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 225)
                                        v2 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 225 // 15)
                                        v3 = T.axis.spatial(16, (ax0_ax1_ax2_ax3_fused_0 * 168 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 10800)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 15 and 1 <= v3 and v3 < 15, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 9)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 9 // 3)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 432)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1 in T.serial(1):
                            for i1_3_init in T.serial(3):
                                with T.block("DepthwiseConv2d_init"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3_init)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    T.reads()
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                            for i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 3, 1, 1, 3, 1, 1, 1, 1, 1):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + i1_3)
                                    i = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                    j = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    di, dj = T.axis.remap("RR", [i4_2, i5_1])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 576, 14, 14], "float32"], ["TENSOR", [576, 1, 3, 3], "float32"], [2, 2], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i * 2 + di, j * 2 + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 1, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(576, i0_0_i1_0_i2_0_i3_0_fused * 48 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 3 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[12, 2, 8, 3, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 56, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 2])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l162)
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #28: GFLOPs: 21.7885. Time: 0.0259 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #29: GFLOPs: 23.6370. Time: 0.0239 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #30: GFLOPs: 27.7684. Time: 0.0203 ms. Best GFLOPs: 115.8025
[04:13:30] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_nn_conv2d_add_nn_relu_15"] Trial #31: GFLOPs: 20.2393. Time: 0.0279 ms. Best GFLOPs: 115.8025
[04:13:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_nn_conv2d_add_nn_relu_15"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |            N/A |          N/A |                   N/A |      0 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 799
Total latency (us): 713.664

[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #0: GFLOPs: 79.7514. Time: 0.1133 ms. Best GFLOPs: 79.7514
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #1: GFLOPs: 5.7326. Time: 1.5769 ms. Best GFLOPs: 79.7514
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #2: GFLOPs: 46.0221. Time: 0.1964 ms. Best GFLOPs: 79.7514
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #3: GFLOPs: 214.8826. Time: 0.0421 ms. Best GFLOPs: 214.8826
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #4: GFLOPs: 180.9320. Time: 0.0500 ms. Best GFLOPs: 214.8826
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #5: GFLOPs: 617.3490. Time: 0.0146 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #6: GFLOPs: 157.0389. Time: 0.0576 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #7: GFLOPs: 102.7270. Time: 0.0880 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #8: GFLOPs: 59.8650. Time: 0.1510 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #9: GFLOPs: 152.6499. Time: 0.0592 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #10: GFLOPs: 199.8397. Time: 0.0452 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #11: GFLOPs: 133.3585. Time: 0.0678 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #12: GFLOPs: 101.1286. Time: 0.0894 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #13: GFLOPs: 71.3019. Time: 0.1268 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #14: GFLOPs: 38.1238. Time: 0.2371 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #15: GFLOPs: 140.3219. Time: 0.0644 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #16: GFLOPs: 38.6495. Time: 0.2339 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #17: GFLOPs: 29.9302. Time: 0.3020 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #18: GFLOPs: 308.3806. Time: 0.0293 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #19: GFLOPs: 16.9509. Time: 0.5333 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #20: GFLOPs: 222.5997. Time: 0.0406 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #21: GFLOPs: 173.7346. Time: 0.0520 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #22: GFLOPs: 35.1213. Time: 0.2574 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #23: GFLOPs: 83.2711. Time: 0.1086 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #24: GFLOPs: 187.7716. Time: 0.0481 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #25: GFLOPs: 214.9397. Time: 0.0421 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #26: GFLOPs: 95.0376. Time: 0.0951 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #27: GFLOPs: 121.3600. Time: 0.0745 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #28: GFLOPs: 314.6887. Time: 0.0287 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #29: GFLOPs: 240.6102. Time: 0.0376 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #30: GFLOPs: 76.0378. Time: 0.1189 ms. Best GFLOPs: 617.3490
[04:13:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_nn_conv2d_add_5"] Trial #31: GFLOPs: 236.8359. Time: 0.0382 ms. Best GFLOPs: 617.3490
[04:13:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_nn_conv2d_add_5"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |            N/A |          N/A |                   N/A |      0 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 831
Total latency (us): 728.306

[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #0: GFLOPs: 442.9155. Time: 0.0340 ms. Best GFLOPs: 442.9155
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #1: GFLOPs: 45.8766. Time: 0.3285 ms. Best GFLOPs: 442.9155
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #2: GFLOPs: 473.9101. Time: 0.0318 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #3: GFLOPs: 363.0995. Time: 0.0415 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #4: GFLOPs: 74.3448. Time: 0.2027 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #5: GFLOPs: 367.2066. Time: 0.0410 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #6: GFLOPs: 278.9049. Time: 0.0540 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #7: GFLOPs: 461.5970. Time: 0.0326 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #8: GFLOPs: 57.9717. Time: 0.2599 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #9: GFLOPs: 152.8438. Time: 0.0986 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #10: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 7, 7), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([160, 960, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(80, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init in T.grid(2, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i2_3_init, i0_1_i1_1_i2_1_i3_1_fused])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(80, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 30 + (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1 < 1470)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(15):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(80, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(160, (ax0_ax1_ax2_ax3_fused_0 * 320 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 30)
                                        v1 = T.axis.spatial(960, i4_0 * 30 + (ax0_ax1_ax2_ax3_fused_0 * 320 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 2, 7, 1, 5, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy, xx = T.axis.remap("SS", [i2_3, i0_1_i1_1_i2_1_i3_1_fused])
                                rc = T.axis.reduce(960, i4_0 * 30 + i4_1 * 5 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 80, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 6, 5])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 80])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 80, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #11: GFLOPs: 292.0269. Time: 0.0516 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #12: GFLOPs: 147.8348. Time: 0.1019 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #13: GFLOPs: 137.5858. Time: 0.1095 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #14: GFLOPs: 93.9905. Time: 0.1603 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #15: GFLOPs: 408.3975. Time: 0.0369 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #16: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(160, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 160, 1, 1), "float32"], placeholder_3: T.Buffer[(1, 160, 7, 7), "float32"], T_add: T.Buffer[(1, 160, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([160, 960, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(56, thread="threadIdx.x"):
                    for i2_3_init, i1_4_init in T.grid(7, 20):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused // 7 * 20 + i1_4_init)
                            yy = T.axis.spatial(7, i2_3_init)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(40, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(21):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(56, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(160, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 24)
                                        v1 = T.axis.spatial(960, i4_0 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 24)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 56 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 3840)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 7, 1, 24, 1, 1, 1, 20, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused // 7 * 20 + i1_4)
                                yy = T.axis.spatial(7, i2_3)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(960, i4_0 * 24 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [160, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 20, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(160, i0_2_i1_2_i2_2_i3_2_fused // 7 * 20 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0], placeholder_3[v0, v1, v2, v3])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0] + placeholder_3[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_add_1", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 20])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[40, 1, 24])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 56])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #17: GFLOPs: 90.6874. Time: 0.1662 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #18: GFLOPs: 382.8236. Time: 0.0394 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #19: GFLOPs: 6.8128. Time: 2.2118 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #20: GFLOPs: 11.2261. Time: 1.3423 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #21: GFLOPs: 51.4930. Time: 0.2926 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #22: GFLOPs: 68.1071. Time: 0.2212 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #23: GFLOPs: 99.8914. Time: 0.1508 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #24: GFLOPs: 70.2067. Time: 0.2146 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #25: GFLOPs: 78.2000. Time: 0.1927 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #26: GFLOPs: 59.5981. Time: 0.2528 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #27: GFLOPs: 34.7324. Time: 0.4338 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #28: GFLOPs: 153.5668. Time: 0.0981 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #29: GFLOPs: 467.5974. Time: 0.0322 ms. Best GFLOPs: 473.9101
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #30: GFLOPs: 554.8162. Time: 0.0272 ms. Best GFLOPs: 554.8162
[04:13:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #26: "fused_nn_conv2d_add_add_4"] Trial #31: GFLOPs: 195.7391. Time: 0.0770 ms. Best GFLOPs: 554.8162
[04:13:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #26: "fused_nn_conv2d_add_add_4"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |            N/A |          N/A |                   N/A |      0 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 863
Total latency (us): 782.625

[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #0: GFLOPs: 27.5717. Time: 0.5494 ms. Best GFLOPs: 27.5717
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #1: GFLOPs: 1388.5589. Time: 0.0109 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #2: GFLOPs: 784.6525. Time: 0.0193 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #3: GFLOPs: 94.6413. Time: 0.1600 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #4: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init, i3_4_init in T.grid(5, 2, 7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_2_i1_2_i2_2_i3_2_fused * 10 + i1_3_init * 2 + i1_4_init)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_4_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(8, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(160, i4_0 * 20 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 980)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(200):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) // 20)
                                    v1 = T.axis.spatial(160, i4_0 * 20 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 20)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(20, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 7, 7):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_2_i1_2_i2_2_i3_2_fused * 10 + i1_3 * 2 + i1_4)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_4])
                                rc = T.axis.reduce(160, i4_0 * 20 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 10, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_2_i1_2_i2_2_i3_2_fused * 10 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 48, 5, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 20, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 48])
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #5: GFLOPs: 158.0906. Time: 0.0958 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #6: GFLOPs: 334.5631. Time: 0.0453 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #7: GFLOPs: 24.7777. Time: 0.6113 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #8: GFLOPs: 45.6960. Time: 0.3315 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #9: GFLOPs: 854.9462. Time: 0.0177 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #10: GFLOPs: 197.9893. Time: 0.0765 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #11: GFLOPs: 81.4195. Time: 0.1860 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #12: GFLOPs: 502.7149. Time: 0.0301 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #13: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(35, thread="threadIdx.x"):
                    for i3_3_init, i1_4_init in T.grid(7, 16):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 240 + i0_1_i1_1_i2_1_i3_1_fused * 80 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_4_init)
                            yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            xx = T.axis.spatial(7, i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(14):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(35, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 35 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 35 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 35 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(35):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(35, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 240 + (ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 10)
                                        v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 35 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 2400)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(2, 1, 1, 1, 1, 1, 7, 5, 1, 1, 1, 16, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 240 + i0_1_i1_1_i2_1_i3_1_fused * 80 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + i1_4)
                                yy = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                xx = T.axis.spatial(7, i3_3)
                                rc = T.axis.reduce(160, i4_0 * 10 + i4_1 * 5 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 240 + i0_1_i1_1_i2_1_i3_1_fused * 80 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 16 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 3, 5, 1, 16])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 2, 5])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 35])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 35, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #14: GFLOPs: 278.6664. Time: 0.0544 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #15: GFLOPs: 704.0171. Time: 0.0215 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #16: GFLOPs: 218.5153. Time: 0.0693 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #17: GFLOPs: 476.0684. Time: 0.0318 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #18: GFLOPs: 586.1637. Time: 0.0258 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #19: GFLOPs: 107.7193. Time: 0.1406 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #20: GFLOPs: 824.8369. Time: 0.0184 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #21: GFLOPs: 321.5803. Time: 0.0471 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #22: GFLOPs: 564.1250. Time: 0.0269 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #23: GFLOPs: 430.4718. Time: 0.0352 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #24: GFLOPs: 34.9572. Time: 0.4333 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #25: GFLOPs: 211.4864. Time: 0.0716 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #26: GFLOPs: 489.0184. Time: 0.0310 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #27: GFLOPs: 29.0477. Time: 0.5214 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #28: GFLOPs: 152.6149. Time: 0.0992 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #29: GFLOPs: 670.2368. Time: 0.0226 ms. Best GFLOPs: 1388.5589
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #30: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(84, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i2_3_init in T.serial(7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 40 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy = T.axis.spatial(7, i2_3_init)
                            xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(13):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) // 49)
                                    v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 49 // 7)
                                    v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1 < 490)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(30):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 10)
                                        v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(5, 1, 1, 1, 1, 7, 1, 2, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 40 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy = T.axis.spatial(7, i2_3)
                                xx = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7)
                                rc = T.axis.reduce(160, i4_0 * 10 + i4_1 * 2 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 480 + i0_1_i1_1_i2_1_i3_1_fused // 7 * 40 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 12, 40, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 5, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 40])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 40, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[04:13:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #27: "fused_nn_conv2d_add_nn_relu_16"] Trial #31: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 160, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 160, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 160, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 160, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(5, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(112, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init, i2_4_init in T.grid(2, 6, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(960, i0_1_i1_1_i2_1_i3_1_fused * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_3_init * 6 + i1_4_init)
                            yy = T.axis.spatial(7, i2_4_init)
                            xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(16, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 490)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(43):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(112, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(960, (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 10)
                                        v1 = T.axis.spatial(160, i4_0 * 10 + (ax0_ax1_ax2_ax3_fused_0 * 224 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 10)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 112 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 9600)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 1, 10, 1, 1, 1, 6, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(960, i0_1_i1_1_i2_1_i3_1_fused * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + i1_3 * 6 + i1_4)
                                yy = T.axis.spatial(7, i2_4)
                                xx = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                rc = T.axis.reduce(160, i4_0 * 10 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 160, 7, 7], "float32"], ["TENSOR", [960, 160, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 7, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_1_i1_1_i2_1_i3_1_fused * 192 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 12 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 5, 16, 2, 6])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 1, 10])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 112, 2])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 112, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[04:14:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #27: "fused_nn_conv2d_add_nn_relu_16"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |            N/A |          N/A |                   N/A |      0 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 895
Total latency (us): 815.35

[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #0: GFLOPs: 50.0981. Time: 0.0188 ms. Best GFLOPs: 50.0981
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #1: GFLOPs: 33.7673. Time: 0.0279 ms. Best GFLOPs: 50.0981
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #2: GFLOPs: 235.7654. Time: 0.0040 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #3: GFLOPs: 54.2778. Time: 0.0173 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #4: GFLOPs: 83.9531. Time: 0.0112 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #5: GFLOPs: 24.0315. Time: 0.0391 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #6: GFLOPs: 64.0934. Time: 0.0147 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #7: GFLOPs: 89.7528. Time: 0.0105 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #8: GFLOPs: 133.9903. Time: 0.0070 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #9: GFLOPs: 44.7423. Time: 0.0210 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #10: GFLOPs: 20.7847. Time: 0.0453 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 960, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i2_3_init, i3_4_init in T.grid(7, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 120 + i0_1_i1_1_i2_1_i3_1_fused * 40 + i0_2_i1_2_i2_2_i3_2_fused)
                                i, j = T.axis.remap("SS", [i2_3_init, i3_4_init])
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(63):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("PaddedInput_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 120 + ((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7560 // 63)
                                            v2 = T.axis.spatial(9, ((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 63 // 7)
                                            v3 = T.axis.spatial(9, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                            T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                            T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                            PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 120 + (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 360)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 120 + i0_1_i1_1_i2_1_i3_1_fused * 40 + i0_2_i1_2_i2_2_i3_2_fused)
                                    i, j, di, dj = T.axis.remap("SSRR", [i2_3, i3_4, i4_1, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 120 + i0_1_i1_1_i2_1_i3_1_fused * 40 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 3, 40, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99, l100 = sch.split(loop=l97, factors=[None, 40, 3])
sch.vectorize(loop=l100)
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b79)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 40, 4])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
b110 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b110, ann_key="meta_schedule.unroll_explicit")
b111, b112, b113, b114 = sch.get_child_blocks(b110)
l115, l116, l117, l118, l119, l120, l121, l122 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l115, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l115, ann_key="pragma_unroll_explicit", ann_val=1)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l123, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l123, ann_key="pragma_unroll_explicit", ann_val=1)
l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145, l146, l147 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l131, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l131, ann_key="pragma_unroll_explicit", ann_val=1)
l148, l149, l150, l151, l152, l153, l154 = sch.get_loops(block=b114)
sch.annotate(block_or_loop=l148, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l148, ann_key="pragma_unroll_explicit", ann_val=1)
b155 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170, l171, l172 = sch.get_loops(block=b155)
b173 = sch.decompose_reduction(block=b155, loop=l160)
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #12: GFLOPs: 52.1287. Time: 0.0180 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #13: GFLOPs: 100.5585. Time: 0.0094 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #14: GFLOPs: 101.6419. Time: 0.0093 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #15: GFLOPs: 105.4486. Time: 0.0089 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #16: GFLOPs: 109.8526. Time: 0.0086 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #17: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 960, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(14, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i1_4_init, i2_4_init in T.grid(12, 7):
                        with T.block("DepthwiseConv2d_init"):
                            b = T.axis.spatial(1, 0)
                            c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused // 7 * 480 + i0_2_i1_2_i2_2_i3_2_fused * 12 + i1_4_init)
                            i = T.axis.spatial(7, i2_4_init)
                            j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                            T.reads()
                            T.writes(DepthwiseConv2d_local[b, c, i, j])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                    for i4_0, i5_0 in T.grid(3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(252):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                with T.block("PaddedInput_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused // 7 * 480 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 10080 // 21)
                                    v2 = T.axis.spatial(9, i4_0 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 21 // 3)
                                    v3 = T.axis.spatial(9, i0_0_i1_0_i2_0_i3_0_fused % 7 + (ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) % 3)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                    PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(18):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused // 7 * 480 + (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(1, 0)
                                        v2 = T.axis.spatial(3, i4_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 80 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 1, 3, 1, 12, 7, 1):
                            with T.block("DepthwiseConv2d_update"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused // 7 * 480 + i0_2_i1_2_i2_2_i3_2_fused * 12 + i1_4)
                                i = T.axis.spatial(7, i2_4)
                                j = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7)
                                di, dj = T.axis.remap("RR", [i4_0, i5_2])
                                T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 12, 7, 1):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused // 7 * 480 + i0_2_i1_2_i2_2_i3_2_fused * 12 + ax1)
                            v2 = T.axis.spatial(7, ax2)
                            v3 = T.axis.spatial(7, i0_0_i1_0_i2_0_i3_0_fused % 7 + ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 40, 1, 12])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[7, 1, 1, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 40])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 40, 2])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l157)
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #18: GFLOPs: 64.8568. Time: 0.0145 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #19: GFLOPs: 69.1890. Time: 0.0136 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #20: GFLOPs: 155.2137. Time: 0.0061 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #21: GFLOPs: 102.6547. Time: 0.0092 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #22: GFLOPs: 124.6876. Time: 0.0075 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #23: GFLOPs: 58.7682. Time: 0.0160 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #24: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(960, 1, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 960, 1, 1), "float32"], T_relu: T.Buffer[(1, 960, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        DepthwiseConv2d_local = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="local")
        PaddedInput_shared = T.alloc_buffer([1, 960, 9, 9], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([960, 1, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(70, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_4_init, i3_4_init in T.grid(2, 7):
                            with T.block("DepthwiseConv2d_init"):
                                b = T.axis.spatial(1, 0)
                                c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + i0_1_i1_1_i2_1_i3_1_fused * 20 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4_init)
                                i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                j = T.axis.spatial(7, i3_4_init)
                                T.reads()
                                T.writes(DepthwiseConv2d_local[b, c, i, j])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                DepthwiseConv2d_local[b, c, i, j] = T.float32(0)
                        for i5_0 in T.serial(3):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(144):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(70, thread="threadIdx.x"):
                                    with T.block("PaddedInput_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + (ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1) % 10080 // 63)
                                        v2 = T.axis.spatial(9, (ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1) % 63 // 7)
                                        v3 = T.axis.spatial(9, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1) % 7)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(PaddedInput_shared[v0, v1, v2, v3])
                                        PaddedInput_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 8 and 1 <= v3 and v3 < 8, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(3):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(70, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + (ax0_ax1_ax2_ax3_fused_0 * 210 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                            v1 = T.axis.spatial(1, 0)
                                            v2 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 210 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                            v3 = T.axis.spatial(3, i5_0)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 70 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 480)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 7):
                                with T.block("DepthwiseConv2d_update"):
                                    b = T.axis.spatial(1, 0)
                                    c = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + i0_1_i1_1_i2_1_i3_1_fused * 20 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + i1_4)
                                    i = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7)
                                    j, di, dj = T.axis.remap("SRR", [i3_4, i4_2, i5_0])
                                    T.reads(DepthwiseConv2d_local[b, c, i, j], PaddedInput_shared[b, c, i + di, j + dj], placeholder_shared[c, 0, di, dj])
                                    T.writes(DepthwiseConv2d_local[b, c, i, j])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["depthwise_conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [960, 1, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    DepthwiseConv2d_local[b, c, i, j] = DepthwiseConv2d_local[b, c, i, j] + PaddedInput_shared[b, c, i + di, j + dj] * placeholder_shared[c, 0, di, dj]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 7):
                        with T.block("DepthwiseConv2d_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(960, i0_0_i1_0_i2_0_i3_0_fused * 160 + i0_1_i1_1_i2_1_i3_1_fused * 20 + i0_2_i1_2_i2_2_i3_2_fused // 7 * 2 + ax1)
                            v2 = T.axis.spatial(7, i0_2_i1_2_i2_2_i3_2_fused % 7 + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(DepthwiseConv2d_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(DepthwiseConv2d_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="PaddedInput", func_name="main")
b1 = sch.get_block(name="DepthwiseConv2d", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l5, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 8, 10, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l6, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 7, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l7, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l46, l47, l48, l49, l50 = sch.split(loop=l8, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l9, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l10, factors=[v57, v58, v59])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l55, l61, l19, l29, l39, l49, l56, l62, l20, l30, l40, l50)
l63 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l63, thread_axis="blockIdx.x")
l64 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l64, thread_axis="vthread.x")
l65 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l65, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b66 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b66, loop=l65, preserve_unit_loops=True)
b67 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b67, loop=l60, preserve_unit_loops=True)
l68, l69, l70, l71, l72, l73, l74, l75, l76 = sch.get_loops(block=b67)
l77 = sch.fuse(l73, l74, l75, l76)
v78 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch", ann_val=v78)
b79 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b79, loop=l60, preserve_unit_loops=True)
l80, l81, l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b79)
l89 = sch.fuse(l85, l86, l87, l88)
v90 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch", ann_val=v90)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v91 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v91)
sch.enter_postproc()
sch.unannotate(block_or_loop=b67, ann_key="meta_schedule.cooperative_fetch")
l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b67)
l98, l99 = sch.split(loop=l97, factors=[None, 70])
sch.bind(loop=l99, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b79, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b79)
l106, l107, l108 = sch.split(loop=l105, factors=[None, 70, 3])
sch.vectorize(loop=l108)
sch.bind(loop=l107, thread_axis="threadIdx.x")
b109 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b109, ann_key="meta_schedule.unroll_explicit")
b110, b111, b112, b113 = sch.get_child_blocks(b109)
l114, l115, l116, l117, l118, l119, l120 = sch.get_loops(block=b110)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
l121, l122, l123, l124, l125, l126, l127, l128 = sch.get_loops(block=b111)
sch.annotate(block_or_loop=l121, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l121, ann_key="pragma_unroll_explicit", ann_val=1)
l129, l130, l131, l132, l133, l134, l135, l136, l137, l138, l139, l140, l141, l142, l143, l144, l145 = sch.get_loops(block=b112)
sch.annotate(block_or_loop=l129, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l129, ann_key="pragma_unroll_explicit", ann_val=1)
l146, l147, l148, l149, l150, l151, l152 = sch.get_loops(block=b113)
sch.annotate(block_or_loop=l146, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l146, ann_key="pragma_unroll_explicit", ann_val=1)
b153 = sch.get_block(name="DepthwiseConv2d", func_name="main")
l154, l155, l156, l157, l158, l159, l160, l161, l162, l163, l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b153)
b171 = sch.decompose_reduction(block=b153, loop=l158)
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #25: GFLOPs: 137.1036. Time: 0.0069 ms. Best GFLOPs: 235.7654
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #26: GFLOPs: 257.1140. Time: 0.0037 ms. Best GFLOPs: 257.1140
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #27: GFLOPs: 142.7646. Time: 0.0066 ms. Best GFLOPs: 257.1140
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #28: GFLOPs: 57.8494. Time: 0.0163 ms. Best GFLOPs: 257.1140
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #29: GFLOPs: 52.5470. Time: 0.0179 ms. Best GFLOPs: 257.1140
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #30: GFLOPs: 77.0959. Time: 0.0122 ms. Best GFLOPs: 257.1140
[04:14:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #28: "fused_nn_conv2d_add_nn_relu_17"] Trial #31: GFLOPs: 57.6217. Time: 0.0163 ms. Best GFLOPs: 257.1140
[04:14:09] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #28: "fused_nn_conv2d_add_nn_relu_17"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |            N/A |          N/A |                   N/A |      0 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 927
Total latency (us): 826.327

[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #0: GFLOPs: 321.2482. Time: 0.0938 ms. Best GFLOPs: 321.2482
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #1: GFLOPs: 128.5388. Time: 0.2343 ms. Best GFLOPs: 321.2482
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #2: GFLOPs: 357.8327. Time: 0.0842 ms. Best GFLOPs: 357.8327
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #3: GFLOPs: 156.1426. Time: 0.1929 ms. Best GFLOPs: 357.8327
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #4: GFLOPs: 243.3213. Time: 0.1238 ms. Best GFLOPs: 357.8327
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #5: GFLOPs: 37.1356. Time: 0.8111 ms. Best GFLOPs: 357.8327
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #6: GFLOPs: 251.8485. Time: 0.1196 ms. Best GFLOPs: 357.8327
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #7: GFLOPs: 358.8195. Time: 0.0839 ms. Best GFLOPs: 358.8195
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_conv2d_add_6"] Trial #8: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(320, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_add: T.Buffer[(1, 320, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 320, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([320, 960, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(7, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(40, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init in T.grid(8, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(320, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3_init)
                            yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(320, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(2):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(960, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 120 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 120 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 120 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 40 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 147)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(6):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(40, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(320, (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1 = T.axis.spatial(960, i4_0 * 3 + (ax0_ax1_ax2_ax3_fused_0 * 160 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 8, 1, 7, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(320, i0_2_i1_2_i2_2_i3_2_fused * 8 + i1_3)
                                yy, xx = T.axis.remap("SS", [i0_1_i1_1_i2_1_i3_1_fused, i3_3])
                                rc = T.axis.reduce(960, i4_0 * 3 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(320, i0_2_i1_2_i2_2_i3_2_fused * 8 + ax1)
                            v2 = T.axis.spatial(7, i0_1_i1_1_i2_1_i3_1_fused + ax2)
                            v3 = T.axis.spatial(7, ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 40, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 7, 1, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[320, 3, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 40, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 40, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #9: GFLOPs: 142.5068. Time: 0.2114 ms. Best GFLOPs: 358.8195
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #10: GFLOPs: 68.0024. Time: 0.4429 ms. Best GFLOPs: 358.8195
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #11: GFLOPs: 731.1768. Time: 0.0412 ms. Best GFLOPs: 731.1768
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #12: GFLOPs: 470.8586. Time: 0.0640 ms. Best GFLOPs: 731.1768
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #13: GFLOPs: 97.4889. Time: 0.3090 ms. Best GFLOPs: 731.1768
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #14: GFLOPs: 122.9358. Time: 0.2450 ms. Best GFLOPs: 731.1768
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #15: GFLOPs: 792.2555. Time: 0.0380 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #16: GFLOPs: 539.7285. Time: 0.0558 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #17: GFLOPs: 240.2422. Time: 0.1254 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #18: GFLOPs: 153.6885. Time: 0.1960 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #19: GFLOPs: 168.9038. Time: 0.1783 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #20: GFLOPs: 215.7872. Time: 0.1396 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #21: GFLOPs: 621.1157. Time: 0.0485 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #22: GFLOPs: 405.6035. Time: 0.0743 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #23: GFLOPs: 566.2769. Time: 0.0532 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #24: GFLOPs: 287.9453. Time: 0.1046 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #25: GFLOPs: 9.2514. Time: 3.2559 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #26: GFLOPs: 267.5565. Time: 0.1126 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #27: GFLOPs: 193.5035. Time: 0.1557 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #28: GFLOPs: 42.9420. Time: 0.7014 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #29: "fused_nn_conv2d_add_6"] Trial #29: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 960, 7, 7), "float32"], placeholder_1: T.Buffer[(320, 960, 1, 1), "float32"], placeholder_2: T.Buffer[(1, 320, 1, 1), "float32"], T_add: T.Buffer[(1, 320, 7, 7), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 320, 7, 7], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 960, 7, 7], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([320, 960, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":16, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(10, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i3_3_init, i2_4_init in T.grid(7, 7):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(320, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                            yy, xx = T.axis.remap("SS", [i2_4_init, i3_3_init])
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(16):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(960, i4_0 * 30 + (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 49)
                                        v2 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 49 // 7)
                                        v3 = T.axis.spatial(7, (ax0_ax1_ax2_ax3_fused_0 * 96 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 7)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 32 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 1470)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(75):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(320, (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) // 30)
                                        v1 = T.axis.spatial(960, i4_0 * 30 + (ax0_ax1_ax2_ax3_fused_0 * 128 + ax0_ax1_ax2_ax3_fused_1 * 4 + ax0_ax1_ax2_ax3_fused_2) % 30)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(5, 1, 1, 1, 1, 1, 7, 6, 1, 1, 1, 1, 7, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(320, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused)
                                yy, xx = T.axis.remap("SS", [i2_4, i3_3])
                                rc = T.axis.reduce(960, i4_0 * 30 + i4_1 * 6 + i4_2)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 960, 7, 7], "float32"], ["TENSOR", [320, 960, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 1, 7, 7):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(320, i0_1_i1_1_i2_1_i3_1_fused * 32 + i0_2_i1_2_i2_2_i3_2_fused + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_add[v0, v1, v2, v3])
                            T_add[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15])
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 10, 32, 1, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25])
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 7])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35])
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 7, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45])
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[32, 5, 6])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53])
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59])
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65])
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3])
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l134, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l134, ann_key="pragma_unroll_explicit", ann_val=1)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l163, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l163, ann_key="pragma_unroll_explicit", ann_val=1)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #30: GFLOPs: 31.8056. Time: 0.9470 ms. Best GFLOPs: 792.2555
[04:14:10] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #29: "fused_nn_conv2d_add_6"] Trial #31: GFLOPs: 214.3974. Time: 0.1405 ms. Best GFLOPs: 792.2555
[04:14:16] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #29: "fused_nn_conv2d_add_6"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |            N/A |          N/A |                   N/A |      0 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 959
Total latency (us): 864.347

[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #0: GFLOPs: 2134.6296. Time: 0.0189 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #1: GFLOPs: 152.4991. Time: 0.2640 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #2: GFLOPs: 40.8444. Time: 0.9858 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #3: GFLOPs: 44.1182. Time: 0.9127 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #4: GFLOPs: 586.8984. Time: 0.0686 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #5: GFLOPs: 1688.7580. Time: 0.0238 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #6: GFLOPs: 289.3397. Time: 0.1392 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #7: GFLOPs: 1400.4346. Time: 0.0288 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #8: GFLOPs: 288.6577. Time: 0.1395 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #9: GFLOPs: 42.1450. Time: 0.9554 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #10: GFLOPs: 883.9795. Time: 0.0456 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #11: GFLOPs: 1190.6325. Time: 0.0338 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #12: GFLOPs: 110.9022. Time: 0.3631 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #13: GFLOPs: 1063.7908. Time: 0.0379 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #14: GFLOPs: 16.8381. Time: 2.3914 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #15: GFLOPs: 80.3317. Time: 0.5012 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #16: GFLOPs: 77.3237. Time: 0.5207 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #17: GFLOPs: 648.4360. Time: 0.0621 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #18: GFLOPs: 1716.2968. Time: 0.0235 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #19: GFLOPs: 943.4357. Time: 0.0427 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #20: GFLOPs: 1886.8771. Time: 0.0213 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #21: GFLOPs: 1068.9938. Time: 0.0377 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #22: GFLOPs: 1275.0956. Time: 0.0316 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #23: GFLOPs: 340.4373. Time: 0.1183 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #24: GFLOPs: 513.0352. Time: 0.0785 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #25: GFLOPs: 935.7712. Time: 0.0430 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #26: GFLOPs: 207.2508. Time: 0.1943 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #27: GFLOPs: 95.0844. Time: 0.4235 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #28: GFLOPs: 800.6335. Time: 0.0503 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #29: GFLOPs: 160.7232. Time: 0.2505 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #30: GFLOPs: 177.8501. Time: 0.2264 ms. Best GFLOPs: 2134.6296
[04:14:16] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #30: "fused_nn_conv2d_add_nn_relu_18"] Trial #31: GFLOPs: 62.3353. Time: 0.6460 ms. Best GFLOPs: 2134.6296
[04:14:24] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #30: "fused_nn_conv2d_add_nn_relu_18"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 991
Total latency (us): 883.21

[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #0: GFLOPs: 6.4035. Time: 0.0100 ms. Best GFLOPs: 6.4035
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #1: GFLOPs: 9.6112. Time: 0.0067 ms. Best GFLOPs: 9.6112
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #2: GFLOPs: 3.9220. Time: 0.0163 ms. Best GFLOPs: 9.6112
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #3: GFLOPs: 15.8713. Time: 0.0040 ms. Best GFLOPs: 15.8713
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #4: GFLOPs: 17.2276. Time: 0.0037 ms. Best GFLOPs: 17.2276
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #5: GFLOPs: 17.9547. Time: 0.0036 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #6: GFLOPs: 4.0887. Time: 0.0157 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #7: GFLOPs: 17.7209. Time: 0.0036 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #8: GFLOPs: 12.1130. Time: 0.0053 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #9: GFLOPs: 10.3055. Time: 0.0062 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #10: GFLOPs: 10.9376. Time: 0.0059 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #11: GFLOPs: 3.9335. Time: 0.0163 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #12: GFLOPs: 4.8553. Time: 0.0132 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #13: GFLOPs: 16.3380. Time: 0.0039 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #14: GFLOPs: 8.8930. Time: 0.0072 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #15: GFLOPs: 11.0131. Time: 0.0058 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #16: GFLOPs: 5.2351. Time: 0.0122 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #17: GFLOPs: 16.4671. Time: 0.0039 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #18: GFLOPs: 4.9321. Time: 0.0130 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #19: GFLOPs: 3.2445. Time: 0.0197 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #20: GFLOPs: 7.7623. Time: 0.0082 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #21: GFLOPs: 10.8945. Time: 0.0059 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #22: GFLOPs: 14.4322. Time: 0.0044 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #23: GFLOPs: 11.5679. Time: 0.0055 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #24: GFLOPs: 4.7373. Time: 0.0135 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #25: GFLOPs: 5.4323. Time: 0.0118 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #26: GFLOPs: 16.6847. Time: 0.0038 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #27: GFLOPs: 13.2437. Time: 0.0048 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #28: GFLOPs: 10.2768. Time: 0.0062 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #29: GFLOPs: 7.9468. Time: 0.0081 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #30: GFLOPs: 12.2626. Time: 0.0052 ms. Best GFLOPs: 17.9547
[04:14:24] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #31: "fused_nn_global_avg_pool2d"] Trial #31: GFLOPs: 3.2801. Time: 0.0195 ms. Best GFLOPs: 17.9547
[04:14:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #31: "fused_nn_global_avg_pool2d"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |            N/A |          N/A |                   N/A |      0 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1023
Total latency (us): 886.775

[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #0: GFLOPs: 24.4634. Time: 0.1046 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #1: GFLOPs: 8.0588. Time: 0.3177 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #2: GFLOPs: 6.0705. Time: 0.4217 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #3: GFLOPs: 4.0224. Time: 0.6364 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #4: GFLOPs: 7.6456. Time: 0.3348 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #5: GFLOPs: 5.3110. Time: 0.4820 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #6: GFLOPs: 11.8985. Time: 0.2152 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #7: GFLOPs: 5.8565. Time: 0.4371 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #8: GFLOPs: 3.6018. Time: 0.7108 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #9: GFLOPs: 3.4044. Time: 0.7520 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #10: GFLOPs: 21.2751. Time: 0.1203 ms. Best GFLOPs: 24.4634
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #11: GFLOPs: 99.8852. Time: 0.0256 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #12: GFLOPs: 12.0582. Time: 0.2123 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #13: GFLOPs: 2.1902. Time: 1.1689 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #14: GFLOPs: 26.1343. Time: 0.0980 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #15: GFLOPs: 5.1104. Time: 0.5009 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #16: GFLOPs: 13.5383. Time: 0.1891 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #17: GFLOPs: 29.3527. Time: 0.0872 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #18: GFLOPs: 21.1395. Time: 0.1211 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #19: GFLOPs: 5.9085. Time: 0.4333 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #20: GFLOPs: 3.8098. Time: 0.6719 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #21: GFLOPs: 5.4208. Time: 0.4723 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #22: GFLOPs: 26.4780. Time: 0.0967 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #32: "fused_nn_conv2d"] Trial #23: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 1280, 1, 1), "float32"], placeholder_1: T.Buffer[(1000, 1280, 1, 1), "float32"], conv2d_nchw: T.Buffer[(1, 1000, 1, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 1000, 1, 1], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 1280, 1, 1], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([1000, 1280, 1, 1], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(1, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(50, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 10):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(1000, i0_2_i1_2_i2_2_i3_2_fused * 20 + i1_3_init * 10 + i1_4_init)
                            yy = T.axis.spatial(1, 0)
                            xx = T.axis.spatial(1, 0)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1280, 1, 1], "float32"], ["TENSOR", [1000, 1280, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 1, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(1280, i4_0 * 5 + ax0_ax1_ax2_ax3_fused_1)
                                    v2 = T.axis.spatial(1, 0)
                                    v3 = T.axis.spatial(1, 0)
                                    T.where(ax0_ax1_ax2_ax3_fused_1 < 5)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(50):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(50, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1000, (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 5)
                                        v1 = T.axis.spatial(1280, i4_0 * 5 + (ax0_ax1_ax2_ax3_fused_0 * 100 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 5)
                                        v2 = T.axis.spatial(1, 0)
                                        v3 = T.axis.spatial(1, 0)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(5, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 10, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(1000, i0_2_i1_2_i2_2_i3_2_fused * 20 + i1_3 * 10 + i1_4)
                                yy = T.axis.spatial(1, 0)
                                xx = T.axis.spatial(1, 0)
                                rc = T.axis.reduce(1280, i4_0 * 5 + i4_1)
                                ry = T.axis.reduce(1, 0)
                                rx = T.axis.reduce(1, 0)
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 1280, 1, 1], "float32"], ["TENSOR", [1000, 1280, 1, 1], "float32"], [1, 1], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 20, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(1000, i0_2_i1_2_i2_2_i3_2_fused * 20 + ax1)
                            v2, v3 = T.axis.remap("SS", [ax2, ax3])
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3])
                            T.writes(conv2d_nchw[v0, v1, v2, v3])
                            conv2d_nchw[v0, v1, v2, v3] = conv2d_nchw_local[v0, v1, v2, v3]
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5, l6, l7, l8, l9 = sch.get_loops(block=b1)
v10, v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l15, l16, l17, l18, l19 = sch.split(loop=l3, factors=[v10, v11, v12, v13, v14])
v20, v21, v22, v23, v24 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 50, 2, 10])
l25, l26, l27, l28, l29 = sch.split(loop=l4, factors=[v20, v21, v22, v23, v24])
v30, v31, v32, v33, v34 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l35, l36, l37, l38, l39 = sch.split(loop=l5, factors=[v30, v31, v32, v33, v34])
v40, v41, v42, v43, v44 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l45, l46, l47, l48, l49 = sch.split(loop=l6, factors=[v40, v41, v42, v43, v44])
v50, v51, v52 = sch.sample_perfect_tile(loop=l7, n=3, max_innermost_factor=64, decision=[256, 5, 1])
l53, l54, l55 = sch.split(loop=l7, factors=[v50, v51, v52])
v56, v57, v58 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l59, l60, l61 = sch.split(loop=l8, factors=[v56, v57, v58])
v62, v63, v64 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 1])
l65, l66, l67 = sch.split(loop=l9, factors=[v62, v63, v64])
sch.reorder(l15, l25, l35, l45, l16, l26, l36, l46, l17, l27, l37, l47, l53, l59, l65, l54, l60, l66, l18, l28, l38, l48, l55, l61, l67, l19, l29, l39, l49)
l68 = sch.fuse(l15, l25, l35, l45)
sch.bind(loop=l68, thread_axis="blockIdx.x")
l69 = sch.fuse(l16, l26, l36, l46)
sch.bind(loop=l69, thread_axis="vthread.x")
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b71 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b71, loop=l70, preserve_unit_loops=True)
b72 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b72, loop=l65, preserve_unit_loops=True)
l73, l74, l75, l76, l77, l78, l79, l80, l81, l82 = sch.get_loops(block=b72)
l83 = sch.fuse(l79, l80, l81, l82)
v84 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch", ann_val=v84)
b85 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b85, loop=l65, preserve_unit_loops=True)
l86, l87, l88, l89, l90, l91, l92, l93, l94, l95 = sch.get_loops(block=b85)
l96 = sch.fuse(l92, l93, l94, l95)
v97 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch", ann_val=v97)
sch.compute_inline(block=b0)
v98 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v98)
sch.enter_postproc()
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.cooperative_fetch")
l99, l100, l101, l102, l103, l104, l105 = sch.get_loops(block=b72)
l106, l107 = sch.split(loop=l105, factors=[None, 50])
sch.bind(loop=l107, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b85, ann_key="meta_schedule.cooperative_fetch")
l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b85)
l115, l116, l117 = sch.split(loop=l114, factors=[None, 50, 2])
sch.vectorize(loop=l117)
sch.bind(loop=l116, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
l131, l132, l133, l134, l135, l136, l137, l138, l139 = sch.get_loops(block=b120)
l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159 = sch.get_loops(block=b121)
l160, l161, l162, l163, l164, l165, l166 = sch.get_loops(block=b122)
b167 = sch.get_block(name="conv2d_nchw", func_name="main")
l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187 = sch.get_loops(block=b167)
b188 = sch.decompose_reduction(block=b167, loop=l171)
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #24: GFLOPs: 4.7767. Time: 0.5359 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #25: GFLOPs: 15.6473. Time: 0.1636 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #26: GFLOPs: 27.6864. Time: 0.0925 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #27: GFLOPs: 30.2561. Time: 0.0846 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #28: GFLOPs: 46.3564. Time: 0.0552 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #29: GFLOPs: 30.2963. Time: 0.0845 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #30: GFLOPs: 3.1673. Time: 0.8083 ms. Best GFLOPs: 99.8852
[04:14:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #32: "fused_nn_conv2d"] Trial #31: GFLOPs: 12.0952. Time: 0.2117 ms. Best GFLOPs: 99.8852
[04:14:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #32: "fused_nn_conv2d"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |            
 33 |                  fused_reshape |        1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1055
Total latency (us): 912.404

[04:14:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[04:14:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[04:14:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[04:14:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0050 ms. Best GFLOPs: 0.0000
[04:14:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #33: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_reshape"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |            
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |            
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |            
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |            
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     31 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |            
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |            
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |            
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |            
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |            
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |            
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |            
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |            
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |            
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |            
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |            
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |            
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |            
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |            
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |            
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |            
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |            
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1060
Total latency (us): 915.353

[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #17: "fused_nn_conv2d_add_add_2"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #17 has finished. Remaining task(s): 33
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #18: "fused_nn_conv2d_add_nn_relu_11"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #18 has finished. Remaining task(s): 32
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #22: "fused_nn_conv2d_add_add_3"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #22 has finished. Remaining task(s): 31
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #16: "fused_nn_conv2d_add_nn_relu_10"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #16 has finished. Remaining task(s): 30
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #26: "fused_nn_conv2d_add_add_4"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #26 has finished. Remaining task(s): 29
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #23: "fused_nn_conv2d_add_nn_relu_14"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #23 has finished. Remaining task(s): 28
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #5: "fused_nn_conv2d_add_nn_relu_4"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #5 has finished. Remaining task(s): 27
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #4: "fused_nn_conv2d_add_nn_relu_3"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #4 has finished. Remaining task(s): 26
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #29: "fused_nn_conv2d_add_6"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #29 has finished. Remaining task(s): 25
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #9: "fused_nn_conv2d_add_nn_relu_6"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #9 has finished. Remaining task(s): 24
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #13: "fused_nn_conv2d_add_nn_relu_8"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #13 has finished. Remaining task(s): 23
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #27: "fused_nn_conv2d_add_nn_relu_16"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #27 has finished. Remaining task(s): 22
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #14: "fused_nn_conv2d_add_nn_relu_9"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #14 has finished. Remaining task(s): 21
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #32: "fused_nn_conv2d"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #32 has finished. Remaining task(s): 20
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #30: "fused_nn_conv2d_add_nn_relu_18"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #30 has finished. Remaining task(s): 19
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #15: "fused_nn_conv2d_add_3"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #15 has finished. Remaining task(s): 18
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #1: "fused_nn_conv2d_add_nn_relu_1"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #1 has finished. Remaining task(s): 17
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #0 has finished. Remaining task(s): 16
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #12: "fused_nn_conv2d_add_add_1"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #12 has finished. Remaining task(s): 15
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #21: "fused_nn_conv2d_add_nn_relu_13"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #21 has finished. Remaining task(s): 14
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #25: "fused_nn_conv2d_add_5"
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #25 has finished. Remaining task(s): 13
[04:14:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_1"
[04:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:14:56] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 31 candidate(s) from database
[04:15:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 1884 failure(s)
[04:15:09] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 133 candidate(s)
[04:15:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 414 failure(s)
[04:16:07] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 377 failure(s)
[04:16:45] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 395 failure(s)
[04:17:23] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70d6718)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d71fdd08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d76dfc88)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d6fbf4c8)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d7107ba8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7021c18)]: 347 failure(s)
[04:17:41] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 1 candidates:
[1 : 1]:	1.1112
[04:17:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 1 candidate(s) with evolutionary search
[04:17:42] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 1 candidates(s) for measurement
[04:17:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 1 sample(s) to builder
[04:17:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 1 sample(s) to runner
[04:17:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_1"] Trial #31: GFLOPs: 304.9534. Time: 0.0476 ms. Best GFLOPs: 1041.6910
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_1"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |            
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |            
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |          Y 
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |          Y 
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     32 |            
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |            
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |            
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |          Y 
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |            
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |            
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |          Y 
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |          Y 
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |          Y 
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |          Y 
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |          Y 
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |          Y 
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |            
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |            
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |          Y 
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |          Y 
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |            
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |          Y 
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |          Y 
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |          Y 
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |            
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |          Y 
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |          Y 
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |            
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |          Y 
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1061
Total latency (us): 915.353

[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #20: "fused_nn_conv2d_add_4"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #20 has finished. Remaining task(s): 12
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #11: "fused_nn_conv2d_add_2"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #11 has finished. Remaining task(s): 11
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #28: "fused_nn_conv2d_add_nn_relu_17"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #28 has finished. Remaining task(s): 10
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #2: "fused_nn_conv2d_add_nn_relu_2"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #2 has finished. Remaining task(s): 9
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #8: "fused_nn_conv2d_add_add"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #8 has finished. Remaining task(s): 8
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_5"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #7 has finished. Remaining task(s): 7
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #3: "fused_nn_conv2d_add"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #3 has finished. Remaining task(s): 6
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #10: "fused_nn_conv2d_add_nn_relu_7"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #10 has finished. Remaining task(s): 5
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #6: "fused_nn_conv2d_add_1"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #6 has finished. Remaining task(s): 4
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #19: "fused_nn_conv2d_add_nn_relu_12"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #19 has finished. Remaining task(s): 3
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #24: "fused_nn_conv2d_add_nn_relu_15"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #24 has finished. Remaining task(s): 2
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #31: "fused_nn_global_avg_pool2d"
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #31 has finished. Remaining task(s): 1
[04:17:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:17:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:17:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:17:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:17:47] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:17:50] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:17:52] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:17:55] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:17:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_reshape"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |          Y 
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |          Y 
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |          Y 
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |          Y 
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |          Y 
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |          Y 
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |          Y 
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |          Y 
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |          Y 
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |          Y 
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |          Y 
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |          Y 
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |          Y 
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |          Y 
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |          Y 
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |          Y 
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |          Y 
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |          Y 
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |          Y 
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |          Y 
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |          Y 
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |          Y 
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |          Y 
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |          Y 
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |          Y 
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |          Y 
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1061
Total latency (us): 915.353

[04:18:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:00] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:18:03] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:06] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:08] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:12] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_reshape"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |          Y 
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |          Y 
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |          Y 
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |          Y 
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |          Y 
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |          Y 
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |          Y 
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |          Y 
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |          Y 
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |          Y 
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |          Y 
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |          Y 
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |          Y 
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |          Y 
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |          Y 
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |          Y 
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |          Y 
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |          Y 
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |          Y 
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |          Y 
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |          Y 
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |          Y 
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |          Y 
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |          Y 
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |          Y 
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |          Y 
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1061
Total latency (us): 915.353

[04:18:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:18:15] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:18:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:16] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:18:20] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:22] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:26] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:29] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:18:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:18:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:18:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_reshape"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |          Y 
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |          Y 
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |          Y 
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |          Y 
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |          Y 
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |          Y 
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |          Y 
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |          Y 
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |          Y 
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |          Y 
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |          Y 
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |          Y 
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |          Y 
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |          Y 
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |          Y 
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |          Y 
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |          Y 
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |          Y 
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |          Y 
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |          Y 
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |          Y 
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |          Y 
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |          Y 
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |          Y 
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |          Y 
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |          Y 
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1061
Total latency (us): 915.353

[04:18:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:18:32] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:18:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:33] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:18:35] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:39] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:43] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:46] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:18:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 0 sample(s) to builder
[04:18:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:54: Sending 0 sample(s) to runner
[04:18:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #33: "fused_reshape"
 ID |                           Name |     FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |    fused_nn_conv2d_add_nn_relu | 22478848 |      1 |      1353.9251 |      16.6027 |               16.6027 |     32 |          Y 
  1 |  fused_nn_conv2d_add_nn_relu_1 | 26492928 |      1 |      1513.4228 |      17.5053 |               17.5053 |     32 |          Y 
  2 |  fused_nn_conv2d_add_nn_relu_2 |  8028160 |      1 |       741.6027 |      10.8254 |               10.8254 |     32 |          Y 
  3 |            fused_nn_conv2d_add | 13045760 |      1 |      1625.6850 |       8.0248 |                8.0248 |     32 |          Y 
  4 |  fused_nn_conv2d_add_nn_relu_3 | 40943616 |      1 |      1069.4028 |      38.2864 |               38.2864 |     32 |          Y 
  5 |  fused_nn_conv2d_add_nn_relu_4 |  6021120 |      1 |       136.2195 |      44.2016 |               44.2016 |     32 |          Y 
  6 |          fused_nn_conv2d_add_1 | 14525952 |      1 |      1041.6910 |      13.9446 |               13.9446 |     32 |          Y 
  7 |  fused_nn_conv2d_add_nn_relu_5 |  9031680 |      1 |      1039.9801 |       8.6845 |                8.6845 |     32 |          Y 
  8 |        fused_nn_conv2d_add_add | 21826560 |      1 |      2104.8289 |      10.3698 |               10.3698 |     32 |          Y 
  9 |  fused_nn_conv2d_add_nn_relu_6 | 22579200 |      2 |      1208.7109 |      18.6804 |               37.3608 |     32 |          Y 
 10 |  fused_nn_conv2d_add_nn_relu_7 |  2257920 |      1 |       286.8284 |       7.8720 |                7.8720 |     32 |          Y 
 11 |          fused_nn_conv2d_add_2 |  7250432 |      1 |       644.6897 |      11.2464 |               11.2464 |     32 |          Y 
 12 |      fused_nn_conv2d_add_add_1 |  9683968 |      2 |      1203.7274 |       8.0450 |               16.0900 |     32 |          Y 
 13 |  fused_nn_conv2d_add_nn_relu_8 |  9934848 |      3 |       907.7082 |      10.9450 |               32.8349 |     32 |          Y 
 14 |  fused_nn_conv2d_add_nn_relu_9 |  3010560 |      3 |       302.1931 |       9.9624 |               29.8871 |     32 |          Y 
 15 |          fused_nn_conv2d_add_3 | 19317760 |      1 |      1096.9488 |      17.6104 |               17.6104 |     32 |          Y 
 16 | fused_nn_conv2d_add_nn_relu_10 |  6021120 |      3 |       292.5975 |      20.5782 |               61.7345 |     32 |          Y 
 17 |      fused_nn_conv2d_add_add_2 | 38635520 |      3 |      1365.0616 |      28.3031 |               84.9094 |     32 |          Y 
 18 | fused_nn_conv2d_add_nn_relu_11 | 39137280 |      4 |      1874.6864 |      20.8767 |               83.5068 |     32 |          Y 
 19 | fused_nn_conv2d_add_nn_relu_12 |  1505280 |      1 |       264.7014 |       5.6867 |                5.6867 |     32 |          Y 
 20 |          fused_nn_conv2d_add_4 | 14469504 |      1 |      1209.8350 |      11.9599 |               11.9599 |     32 |          Y 
 21 | fused_nn_conv2d_add_nn_relu_13 |  2257920 |      2 |       297.3311 |       7.5940 |               15.1879 |     32 |          Y 
 22 |      fused_nn_conv2d_add_add_3 | 21713664 |      2 |       593.8540 |      36.5640 |               73.1280 |     32 |          Y 
 23 | fused_nn_conv2d_add_nn_relu_14 | 21901824 |      3 |      1280.0776 |      17.1098 |               51.3293 |     32 |          Y 
 24 | fused_nn_conv2d_add_nn_relu_15 |   564480 |      1 |       115.8025 |       4.8745 |                4.8745 |     32 |          Y 
 25 |          fused_nn_conv2d_add_5 |  9039520 |      1 |       617.3490 |      14.6425 |               14.6425 |     32 |          Y 
 26 |      fused_nn_conv2d_add_add_4 | 15068480 |      2 |       554.8162 |      27.1594 |               54.3188 |     32 |          Y 
 27 | fused_nn_conv2d_add_nn_relu_16 | 15146880 |      3 |      1388.5589 |      10.9083 |               32.7250 |     32 |          Y 
 28 | fused_nn_conv2d_add_nn_relu_17 |   940800 |      3 |       257.1140 |       3.6591 |               10.9772 |     32 |          Y 
 29 |          fused_nn_conv2d_add_6 | 30121280 |      1 |       792.2555 |      38.0197 |               38.0197 |     32 |          Y 
 30 | fused_nn_conv2d_add_nn_relu_18 | 40266240 |      1 |      2134.6296 |      18.8633 |               18.8633 |     32 |          Y 
 31 |     fused_nn_global_avg_pool2d |    64000 |      1 |        17.9547 |       3.5645 |                3.5645 |     32 |          Y 
 32 |                fused_nn_conv2d |  2560000 |      1 |        99.8852 |      25.6294 |               25.6294 |     32 |          Y 
 33 |                  fused_reshape |        1 |      1 |         0.0003 |       2.9484 |                2.9484 |      5 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 1061
Total latency (us): 915.353

[04:18:48] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:126: Scheduler picks Task #33: "fused_reshape"
[04:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:656: Generating candidates......
[04:18:48] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:658: Picked top 5 candidate(s) from database
[04:18:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:494: Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:49] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:660: Sampled 2043 candidate(s)
[04:18:53] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:57] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:18:59] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:19:02] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:571: Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x55b5d70a7ae8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x55b5d77f91f8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x55b5d7c190e8)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x55b5d7c18018)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x55b5d71aff58)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x55b5d7c18f28)]: 0 failure(s)
[04:19:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:595: Scores of the best 0 candidates:
[04:19:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:664: Got 0 candidate(s) with evolutionary search
[04:19:05] /home/yj/tvm/src/meta_schedule/search_strategy/evolutionary_search.cc:666: Sending 0 candidates(s) for measurement
[04:19:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:139: Task #33 has finished. Remaining task(s): 0
Starting to build with relay.
/home/yj/anaconda3/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
The result is correct!
