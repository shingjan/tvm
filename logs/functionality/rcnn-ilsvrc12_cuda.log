nohup: ignoring input
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_nn_conv2d_add_nn_relu"
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 3, 224, 224], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 96, 54, 54], dtype="float32")
        T_add = T.alloc_buffer([1, 96, 54, 54], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 3, 224, 224):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, i3_1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = placeholder[i0_1, i1_1, i2_1, i3_1]
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 96, 54, 54, 3, 11, 11):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 96, 54, 54):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 96, 54, 54):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 96, 54, 54], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([96, 3, 11, 11], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(27, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 11, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(4977):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 4977 // 1659)
                                    v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused // 3 * 24 + i5_0 + ax0_ax1_ax2_ax3_fused % 1659 // 79)
                                    v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 3 * 72 + ax0_ax1_ax2_ax3_fused % 79)
                                    T.reads(placeholder[v0, v1, v2, v3])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused in T.serial(3168):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(96, ax0_ax1_ax2_ax3_fused // 33)
                                    v1 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 33 // 11)
                                    v2 = T.axis.spatial(11, i5_0)
                                    v3 = T.axis.spatial(11, ax0_ax1_ax2_ax3_fused % 11)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 11, 1, 96, 1, 3, 1, 1, 1, 1, 1, 6, 2):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i1_3)
                                    yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 3 * 6 + i2_4)
                                    xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 3 * 18 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i3_3 * 2 + i3_4)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_1])
                                    T.reads(pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 96, 6, 6):
                            with T.block("conv2d_nchw_local"):
                                v0, v1 = T.axis.remap("SS", [ax0, ax1])
                                v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 3 * 6 + ax2)
                                v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 3 * 18 + i0_1_i1_1_i2_1_i3_1_fused * 6 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 1, 96, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[9, 1, 1, 1, 6])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 3, 1, 3, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 11, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_nn_max_pool2d"
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 54, 54), "float32"], tensor: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 96, 56, 56], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 56, 56):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 54 and ax3 < 54, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 96, 27, 27, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 54, 54), "float32"], tensor: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 96, 27, 27, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(ax2 * 2 + rv0 < 54 and ax3 * 2 + rv1 < 54, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_nn_lrn"
[13:32:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], T_divide: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_data = T.alloc_buffer([1, 100, 27, 27], dtype="float32")
        tensor = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 100, 27, 27):
            with T.block("pad_data"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 - 2, ax2, ax3])
                T.writes(pad_data[ax0, ax1, ax2, ax3])
                pad_data[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax1 and ax1 < 98, placeholder[ax0, ax1 - 2, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 96, 27, 27, 5):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                T.reads(pad_data[ax0, ax1 + rxs, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + pad_data[ax0, ax1 + rxs, ax2, ax3] * pad_data[ax0, ax1 + rxs, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor[ax0, ax1, ax2, ax3])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                tensor_1[ax0, ax1, ax2, ax3] = T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], tensor_1[ax0, ax1, ax2, ax3])
                T.writes(T_divide[ax0, ax1, ax2, ax3])
                T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / tensor_1[ax0, ax1, ax2, ax3]
    

[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], T_divide: T.Buffer[(1, 96, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            tensor = T.alloc_buffer([1, 96, 27, 27], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 96, 27, 27, 5):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 + rxs - 2, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + T.if_then_else(2 <= ax1 + rxs and ax1 + rxs < 98, placeholder[ax0, ax1 + rxs - 2, ax2, ax3], T.float32(0), dtype="float32") * T.if_then_else(2 <= ax1 + rxs and ax1 + rxs < 98, placeholder[ax0, ax1 + rxs - 2, ax2, ax3], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 96, 27, 27):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], tensor[ax0, ax1, ax2, ax3])
                    T.writes(T_divide[ax0, ax1, ax2, ax3])
                    T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
    

b0 = sch.get_block(name="pad_data", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_nn_conv2d_add_nn_relu_1"
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 96, 31, 31], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 27, 27], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 96, 31, 31):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(2 <= i2_1 and i2_1 < 29 and 2 <= i3_1 and i3_1 < 29, placeholder[i0_1, i1_1, i2_1 - 2, i3_1 - 2], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 27, 27, 48, 5, 5):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 128 * 48 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 128 * 48 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 27, 27):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 96, 27, 27), "float32"], placeholder_1: T.Buffer[(256, 48, 5, 5), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 27, 27), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            group_conv2d_nchw_local = T.alloc_buffer([1, 256, 27, 27], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 96, 31, 31], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 48, 5, 5], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(81, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(8, 1, 5):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2430):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(96, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 2430 // 45)
                                    v2 = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_fused // 3 + ax0_ax1_ax2_ax3_fused % 45 // 9)
                                    v3 = T.axis.spatial(31, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i6_0 + ax0_ax1_ax2_ax3_fused % 9)
                                    T.reads(placeholder[v0, v1, v2 - 2, v3 - 2])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(2 <= v2 and v2 < 29 and 2 <= v3 and v3 < 29, placeholder[v0, v1, v2 - 2, v3 - 2], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(7680):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, ax0_ax1_ax2_ax3_fused // 30)
                                    v1 = T.axis.spatial(48, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 30 // 5)
                                    v2 = T.axis.spatial(5, ax0_ax1_ax2_ax3_fused % 5)
                                    v3 = T.axis.spatial(5, i6_0)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(6, 1, 1, 1, 16, 1, 1, 1, 5, 1, 1, 4, 1, 9):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + i1_3 * 4 + i1_4)
                                    yy = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3)
                                    xx = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + i3_4)
                                    rc = T.axis.reduce(48, i4_0 * 6 + i4_1)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_0])
                                    T.reads(pad_temp_shared[nn, ff // 128 * 48 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 96, 27, 27], "float32"], ["TENSOR", [256, 48, 5, 5], "float32"], [1, 1], [2, 2, 2, 2], [1, 1], 2, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 128 * 48 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 64, 1, 9):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_1_i1_1_i2_1_i3_1_fused * 64 + ax1)
                                v2 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused // 3 + ax2)
                                v3 = T.axis.spatial(27, i0_0_i1_0_i2_0_i3_0_fused % 3 * 9 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 1, 16, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[27, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 1, 1, 1, 9])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 6, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 5])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[5, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_nn_max_pool2d_1"
[13:32:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 28, 28], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 28, 28):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 27 and ax3 < 27, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 27, 27), "float32"], tensor: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 13, 13, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(ax2 * 2 + rv0 < 27 and ax3 * 2 + rv1 < 27, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_nn_lrn_1"
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_divide: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_data = T.alloc_buffer([1, 260, 13, 13], dtype="float32")
        tensor = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        tensor_1 = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 260, 13, 13):
            with T.block("pad_data"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1 - 2, ax2, ax3])
                T.writes(pad_data[ax0, ax1, ax2, ax3])
                pad_data[ax0, ax1, ax2, ax3] = T.if_then_else(2 <= ax1 and ax1 < 258, placeholder[ax0, ax1 - 2, ax2, ax3], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 5):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                T.reads(pad_data[ax0, ax1 + rxs, ax2, ax3])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + pad_data[ax0, ax1 + rxs, ax2, ax3] * pad_data[ax0, ax1 + rxs, ax2, ax3]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("tensor_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(tensor[ax0, ax1, ax2, ax3])
                T.writes(tensor_1[ax0, ax1, ax2, ax3])
                tensor_1[ax0, ax1, ax2, ax3] = T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3], tensor_1[ax0, ax1, ax2, ax3])
                T.writes(T_divide[ax0, ax1, ax2, ax3])
                T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / tensor_1[ax0, ax1, ax2, ax3]
    

[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], T_divide: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            tensor = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
            for i0, i1, i2, i3, i4 in T.grid(1, 256, 13, 13, 5):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rxs = T.axis.remap("SSSSR", [i0, i1, i2, i3, i4])
                    T.reads(placeholder[ax0, ax1 + rxs - 2, ax2, ax3])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(0)
                    tensor[ax0, ax1, ax2, ax3] = tensor[ax0, ax1, ax2, ax3] + T.if_then_else(2 <= ax1 + rxs and ax1 + rxs < 258, placeholder[ax0, ax1 + rxs - 2, ax2, ax3], T.float32(0), dtype="float32") * T.if_then_else(2 <= ax1 + rxs and ax1 + rxs < 258, placeholder[ax0, ax1 + rxs - 2, ax2, ax3], T.float32(0), dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
                with T.block("tensor_1"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, ax2, ax3], tensor[ax0, ax1, ax2, ax3])
                    T.writes(T_divide[ax0, ax1, ax2, ax3])
                    T_divide[ax0, ax1, ax2, ax3] = placeholder[ax0, ax1, ax2, ax3] / T.pow(T.float32(1) + T.float32(9.9999997473787516e-05) * tensor[ax0, ax1, ax2, ax3] * T.float32(0.20000000000000001), T.float32(0.75), dtype="float32")
    

b0 = sch.get_block(name="pad_data", func_name="main")
b1 = sch.get_block(name="T_divide", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_nn_conv2d_add_nn_relu_2"
[13:32:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 15, 15], dtype="float32")
        conv2d_nchw = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 384, 13, 13, 256, 3, 3):
            with T.block("conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [384, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                with T.init():
                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            conv2d_nchw_local = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([384, 256, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(312, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(13, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(4, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2880):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 2880 // 45)
                                    v2 = T.axis.spatial(15, ax0_ax1_ax2_ax3_fused % 45 // 3)
                                    v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(9216):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + ax0_ax1_ax2_ax3_fused // 576)
                                    v1 = T.axis.spatial(256, i4_0 * 64 + ax0_ax1_ax2_ax3_fused % 576 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(64, 1, 1, 1, 16, 1, 1, 1, 3, 3, 1, 1, 1, 1):
                                with T.block("conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + i1_3)
                                    yy = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(256, i4_0 * 64 + i4_1)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_2])
                                    T.reads(pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [384, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                    with T.init():
                                        conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 1, 1):
                            with T.block("conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 13 * 16 + ax1)
                                v2 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused + ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[24, 1, 1, 16, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 64, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_conv2d_add_nn_relu_3"
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 384, 13, 13, 192, 3, 3):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 192 * 192 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 192 * 192 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 384, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            group_conv2d_nchw_local = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([384, 192, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(676, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(32, 1, 1):
                            for ax0_ax1_ax2_ax3_fused in T.serial(54):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 338 * 192 + i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 54 // 9)
                                    v2 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 + ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(15, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(5184):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 169 * 96 + ax0_ax1_ax2_ax3_fused // 54)
                                    v1 = T.axis.spatial(192, i4_0 * 6 + ax0_ax1_ax2_ax3_fused % 54 // 9)
                                    v2 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 9 // 3)
                                    v3 = T.axis.spatial(3, ax0_ax1_ax2_ax3_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 3, 1, 3, 1, 1, 2, 3, 1, 1, 32, 1, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 169 * 96 + i1_3 * 32 + i1_4)
                                    yy = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(192, i4_0 * 6 + i4_1 * 2 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_2, i6_1])
                                    T.reads(pad_temp_shared[nn, ff // 192 * 192 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 192 * 192 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 96, 1, 1):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 169 * 96 + ax1)
                                v2 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 + ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 1, 3, 32])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 3, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_nn_conv2d_add_nn_relu_4"
[13:32:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 384, 15, 15], dtype="float32")
        group_conv2d_nchw = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        T_add = T.alloc_buffer([1, 256, 13, 13], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 15, 15):
            with T.block("pad_temp"):
                i0_1, i1_1, i2_1, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1])
                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])
                pad_temp[i0_1, i1_1, i2_1, i3_1] = T.if_then_else(1 <= i2_1 and i2_1 < 14 and 1 <= i3_1 and i3_1 < 14, placeholder[i0_1, i1_1, i2_1 - 1, i3_1 - 1], T.float32(0), dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6 in T.grid(1, 256, 13, 13, 192, 3, 3):
            with T.block("group_conv2d_nchw"):
                nn, ff, yy, xx, rc, ry, rx = T.axis.remap("SSSSRRR", [i0, i1, i2, i3, i4, i5, i6])
                T.reads(pad_temp[nn, ff // 128 * 192 + rc, yy + ry, xx + rx], placeholder_1[ff, rc, ry, rx])
                T.writes(group_conv2d_nchw[nn, ff, yy, xx])
                T.block_attr({"workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                with T.init():
                    group_conv2d_nchw[nn, ff, yy, xx] = T.float32(0)
                group_conv2d_nchw[nn, ff, yy, xx] = group_conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, ff // 128 * 192 + rc, yy + ry, xx + rx] * placeholder_1[ff, rc, ry, rx]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(group_conv2d_nchw[ax0, ax1, ax2, ax3], placeholder_2[ax0, ax1, 0, 0])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = group_conv2d_nchw[ax0, ax1, ax2, ax3] + placeholder_2[ax0, ax1, 0, 0]
        for i0, i1, i2, i3 in T.grid(1, 256, 13, 13):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[ax0, ax1, ax2, ax3])
                T.writes(T_relu[ax0, ax1, ax2, ax3])
                T_relu[ax0, ax1, ax2, ax3] = T.max(T_add[ax0, ax1, ax2, ax3], T.float32(0))
    

[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(256, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 256, 1, 1), "float32"], T_relu: T.Buffer[(1, 256, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            group_conv2d_nchw_local = T.alloc_buffer([1, 256, 13, 13], dtype="float32", scope="local")
            pad_temp_shared = T.alloc_buffer([1, 384, 15, 15], dtype="float32", scope="shared")
            placeholder_shared = T.alloc_buffer([256, 192, 3, 3], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(26, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i4_0, i5_0, i6_0 in T.grid(1, 3, 3):
                            for ax0_ax1_ax2_ax3_fused in T.serial(2496):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 13 * 192 + ax0_ax1_ax2_ax3_fused % 2496 // 13)
                                    v2 = T.axis.spatial(15, i5_0 + ax0_ax1_ax2_ax3_fused % 13)
                                    v3 = T.axis.spatial(15, i6_0 + i0_0_i1_0_i2_0_i3_0_fused % 13 + 0)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                            for ax0_ax1_ax2_ax3_fused in T.serial(24576):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + ax0_ax1_ax2_ax3_fused // 192)
                                    v1 = T.axis.spatial(192, ax0_ax1_ax2_ax3_fused % 192)
                                    v2, v3 = T.axis.remap("SS", [i5_0, i6_0])
                                    T.reads(placeholder_1[v0, v1, v2, v3])
                                    T.writes(placeholder_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 8, 13, 1, 64, 1, 1, 1, 2, 1, 1):
                                with T.block("group_conv2d_nchw"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 16 + i1_3 * 2 + i1_4)
                                    yy = T.axis.spatial(13, i2_3)
                                    xx = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13)
                                    rc = T.axis.reduce(192, i4_1 * 64 + i4_2)
                                    ry, rx = T.axis.remap("RR", [i5_0, i6_0])
                                    T.reads(pad_temp_shared[nn, ff // 128 * 192 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [256, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                                    with T.init():
                                        group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                                    group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 128 * 192 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                        for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                            with T.block("group_conv2d_nchw_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(256, i0_0_i1_0_i2_0_i3_0_fused // 13 * 128 + i0_2_i1_2_i2_2_i3_2_fused * 16 + ax1)
                                v2 = T.axis.spatial(13, ax2)
                                v3 = T.axis.spatial(13, i0_0_i1_0_i2_0_i3_0_fused % 13 + ax3)
                                T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                                T.writes(T_relu[v0, v1, v2, v3])
                                T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 8, 8, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[13, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 64])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_max_pool2d_2"
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        pad_temp = T.alloc_buffer([1, 256, 14, 14], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 256, 14, 14):
            with T.block("pad_temp"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, ax2, ax3])
                T.writes(pad_temp[ax0, ax1, ax2, ax3])
                pad_temp[ax0, ax1, ax2, ax3] = T.if_then_else(ax2 < 13 and ax3 < 13, placeholder[ax0, ax1, ax2, ax3], T.float32(-3.4028234663852886e+38), dtype="float32")
        for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 6, 6, 3, 3):
            with T.block("tensor"):
                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                T.reads(pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                T.writes(tensor[ax0, ax1, ax2, ax3])
                with T.init():
                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], pad_temp[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
    

[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], tensor: T.Buffer[(1, 256, 6, 6), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2, i3, i4, i5 in T.grid(1, 256, 6, 6, 3, 3):
                with T.block("tensor"):
                    ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap("SSSSRR", [i0, i1, i2, i3, i4, i5])
                    T.reads(placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])
                    T.writes(tensor[ax0, ax1, ax2, ax3])
                    with T.init():
                        tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)
                    tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], T.if_then_else(ax2 * 2 + rv0 < 13 and ax3 * 2 + rv1 < 13, placeholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1], T.float32(-3.4028234663852886e+38), dtype="float32"))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_reshape"
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 6, 6), "float32"], T_reshape: T.Buffer[(1, 9216), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 9216):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6]
    

[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 6, 6), "float32"], T_reshape: T.Buffer[(1, 9216), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1 in T.grid(1, 9216):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, ax1 % 9216 // 36, ax1 % 36 // 6, ax1 % 6]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_dense_add_nn_relu"
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
        T_add = T.alloc_buffer([1, 4096], dtype="float32")
        for i0, i1, i2 in T.grid(1, 4096, 9216):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_relu"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_add[ax0, ax1])
                T.writes(T_relu[ax0, ax1])
                T_relu[ax0, ax1] = T.max(T_add[ax0, ax1], T.float32(0))
    

[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 9216], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([4096, 9216], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(16, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(4, thread="threadIdx.x"):
                        for i2_0 in T.serial(1536):
                            for ax0_ax1_fused in T.serial(6):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(9216, i2_0 * 6 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(1536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + ax0_ax1_fused // 6)
                                    v1 = T.axis.spatial(9216, i2_0 * 6 + ax0_ax1_fused % 6)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(2, 1, 32, 3, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused * 128 + i0_2_i1_2_fused * 32 + i1_3)
                                    k = T.axis.reduce(9216, i2_0 * 6 + i2_1 * 3 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 32):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused * 128 + i0_2_i1_2_fused * 32 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 2, 4, 32, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1536, 2, 3])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_nn_dense_add_nn_relu_1"
[13:32:35] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 4096], dtype="float32")
        T_add = T.alloc_buffer([1, 4096], dtype="float32")
        for i0, i1, i2 in T.grid(1, 4096, 4096):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [4096, 4096], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
        for i0, i1 in T.grid(1, 4096):
            with T.block("T_relu"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_add[ax0, ax1])
                T.writes(T_relu[ax0, ax1])
                T_relu[ax0, ax1] = T.max(T_add[ax0, ax1], T.float32(0))
    

[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 4096], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([4096, 4096], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(4, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(64):
                            for ax0_ax1_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(4096, i2_0 * 64 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(262144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(4096, ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(4096, i2_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(4, 1, 1024, 16, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(4096, i0_1_i1_1_fused * 1024 + i1_3)
                                    k = T.axis.reduce(4096, i2_0 * 64 + i2_1 * 16 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [4096, 4096], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 1024):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(4096, i0_1_i1_1_fused * 1024 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_relu[v0, v1])
                                T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1024, 1])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[64, 4, 16])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_nn_dense_add"
[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_matmul_NT = T.alloc_buffer([1, 200], dtype="float32")
        for i0, i1, i2 in T.grid(1, 200, 4096):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [200, 4096], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
        for i0, i1 in T.grid(1, 200):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_matmul_NT[ax0, ax1], placeholder_2[ax0, ax1])
                T.writes(T_add[ax0, ax1])
                T_add[ax0, ax1] = T_matmul_NT[ax0, ax1] + placeholder_2[ax0, ax1]
    

[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(200, 4096), "float32"], placeholder_2: T.Buffer[(1, 200), "float32"], T_add: T.Buffer[(1, 200), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_matmul_NT_local = T.alloc_buffer([1, 200], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([1, 4096], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([200, 4096], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(100, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(64):
                            for ax0_ax1_fused in T.serial(64):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(4096, i2_0 * 64 + ax0_ax1_fused)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(12800):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(200, ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(4096, i2_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(16, 1, 2, 4, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(1, 0)
                                    j = T.axis.spatial(200, i0_1_i1_1_fused * 2 + i1_3)
                                    k = T.axis.reduce(4096, i2_0 * 64 + i2_1 * 4 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [200, 4096], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(1, 2):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(1, ax0)
                                v1 = T.axis.spatial(200, i0_1_i1_1_fused * 2 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                                T.writes(T_add[v0, v1])
                                T_add[v0, v1] = T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l3, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 100, 1, 2, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l4, factors=[v16, v17, v18, v19, v20])
v26, v27, v28 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 16, 4])
l29, l30, l31 = sch.split(loop=l5, factors=[v26, v27, v28])
sch.reorder(l11, l21, l12, l22, l13, l23, l29, l30, l14, l24, l31, l15, l25)
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="blockIdx.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="vthread.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b35 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b35, loop=l34, preserve_unit_loops=True)
b36 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b36, loop=l29, preserve_unit_loops=True)
l37, l38, l39, l40, l41, l42 = sch.get_loops(block=b36)
l43 = sch.fuse(l41, l42)
v44 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b36, ann_key="meta_schedule.cooperative_fetch", ann_val=v44)
b45 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b45, loop=l29, preserve_unit_loops=True)
l46, l47, l48, l49, l50, l51 = sch.get_loops(block=b45)
l52 = sch.fuse(l50, l51)
v53 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b45, ann_key="meta_schedule.cooperative_fetch", ann_val=v53)
sch.reverse_compute_inline(block=b1)
v54 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v54)
[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                  fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[13:32:36] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[13:32:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:33:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:33:12] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000559fbb205bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:33:14] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055de96753bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:33:16] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055b120015bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:33:21] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055f30a41cbc2
  96: 0xffffffffffffffff


[13:33:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_max_pool2d"
[13:33:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:33:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:33:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_lrn"
[13:33:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:33:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:33:42] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_nn_relu_1"
[13:33:46] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:34:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:34:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_max_pool2d_1"
[13:34:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:34:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:34:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_lrn_1"
[13:34:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:34:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:34:34] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_conv2d_add_nn_relu_2"
[13:34:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_3"
[13:35:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:52] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_conv2d_add_nn_relu_4"
[13:36:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:36:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_max_pool2d_2"
[13:36:30] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:36:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_reshape"
[13:36:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:36:41] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_dense_add_nn_relu"
[13:37:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:37:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:37:18] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x00005579e3f12bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:37:38] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055f563103bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:37:56] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000563eae6e7bc2
  96: 0xffffffffffffffff


terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:38:20] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x0000564640e4dbc2
  96: 0xffffffffffffffff


[13:38:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_dense_add_nn_relu_1"
[13:38:25] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:38:27] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:38:31] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_MISALIGNED_ADDRESS
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055b11a3f5bc2
  96: 0xffffffffffffffff


[13:39:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_dense_add"
[13:39:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:39:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #0: GFLOPs: 451.8643. Time: 0.4510 ms. Best GFLOPs: 451.8643
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #1: GFLOPs: 86.8022. Time: 2.3478 ms. Best GFLOPs: 451.8643
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #2: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 54, 54], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 3, 11, 11], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(216, thread="blockIdx.x"):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0, i5_0 in T.grid(1, 1):
                        for i2_3_init, i1_4_init in T.grid(18, 2):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 54 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_4_init)
                                yy = T.axis.spatial(54, i0_2_i1_2_i2_2_i3_2_fused % 3 * 18 + i2_3_init)
                                xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 54)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i6_0 in T.serial(11):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(19):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 669 // 223)
                                        v2 = T.axis.spatial(224, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 223)
                                        v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 54 * 4 + i6_0 + 0)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 669)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(11):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 54 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 33)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 33 // 11)
                                            v2 = T.axis.spatial(11, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                            v3 = T.axis.spatial(11, i6_0)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 11, 1, 1, 1, 18, 1, 3, 1, 1, 1, 2, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 54 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + i1_4)
                                    yy = T.axis.spatial(54, i0_2_i1_2_i2_2_i3_2_fused % 3 * 18 + i2_3)
                                    xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 54)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_1, i6_0])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 18, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 54 * 24 + i0_2_i1_2_i2_2_i3_2_fused // 3 * 2 + ax1)
                            v2 = T.axis.spatial(54, i0_2_i1_2_i2_2_i3_2_fused % 3 * 18 + ax2)
                            v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 54 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 1, 12, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 3, 18, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[54, 1, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 11, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 36, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l175)
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #3: GFLOPs: 3100.3544. Time: 0.0657 ms. Best GFLOPs: 3100.3544
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #4: GFLOPs: 652.7002. Time: 0.3122 ms. Best GFLOPs: 3100.3544
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #5: GFLOPs: 1106.7892. Time: 0.1841 ms. Best GFLOPs: 3100.3544
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #6: GFLOPs: 50.8433. Time: 4.0083 ms. Best GFLOPs: 3100.3544
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #7: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 54, 54], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 3, 11, 11], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(27, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i2_4_init in T.grid(3, 3):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_3_init)
                                yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 9 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 3 + i2_4_init)
                                xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(11, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(179):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 6417 // 2139)
                                        v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused // 9 * 72 + i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 2139 // 31)
                                        v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 9 * 24 + (ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) % 31)
                                        T.where(ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1 < 6417)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(44):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(96, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 33)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 33 // 11)
                                            v2 = T.axis.spatial(11, i5_0)
                                            v3 = T.axis.spatial(11, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 11, 1, 3, 1, 1, 3, 1, 1, 1, 1, 3, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 3 + i1_3)
                                    yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 9 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 3 + i2_4)
                                    xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_2, i5_0, i6_1])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 3, 3, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused * 3 + ax1)
                            v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 9 * 18 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 3 + ax2)
                            v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 32, 1, 3, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 6, 1, 3])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[9, 1, 6, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 11, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 36])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 36, 2])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l174)
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #8: GFLOPs: 1182.7322. Time: 0.1723 ms. Best GFLOPs: 3100.3544
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #9: GFLOPs: 3689.6696. Time: 0.0552 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #10: GFLOPs: 2245.6188. Time: 0.0908 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #11: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 54, 54], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 3, 11, 11], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(54, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(3, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(36, thread="threadIdx.x"):
                    for i4_0 in T.serial(1):
                        for i1_3_init, i3_3_init in T.grid(8, 6):
                            with T.block("conv2d_nchw_init"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 27 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 8 + i1_3_init)
                                yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 27 // 9 * 18 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6)
                                xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + i3_3_init)
                                T.reads()
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                        for i5_0, i6_0 in T.grid(11, 1):
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(60):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                        with T.block("pad_temp_shared"):
                                            v0 = T.axis.spatial(1, 0)
                                            v1 = T.axis.spatial(3, ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 6417 // 2139)
                                            v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 27 // 9 * 72 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 2139 // 31)
                                            v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 9 * 24 + ((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 31)
                                            T.where((ax0_ax1_ax2_ax3_fused_0 * 36 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 6417)
                                            T.reads(placeholder[v0, v1, v2, v3])
                                            T.writes(pad_temp_shared[v0, v1, v2, v3])
                                            pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                            for ax0_ax1_ax2_ax3_fused_0 in T.serial(22):
                                for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(36, thread="threadIdx.x"):
                                    for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                        with T.block("placeholder_shared"):
                                            v0 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 27 * 48 + (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 33)
                                            v1 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 33 // 11)
                                            v2 = T.axis.spatial(11, i5_0)
                                            v3 = T.axis.spatial(11, (ax0_ax1_ax2_ax3_fused_0 * 72 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 11)
                                            T.reads(placeholder_1[v0, v1, v2, v3])
                                            T.writes(placeholder_shared[v0, v1, v2, v3])
                                            placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                            for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(3, 1, 1, 1, 8, 1, 6, 1, 1, 11, 1, 1, 1, 1):
                                with T.block("conv2d_nchw_update"):
                                    nn = T.axis.spatial(1, 0)
                                    ff = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 27 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 8 + i1_3)
                                    yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 27 // 9 * 18 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6)
                                    xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + i3_3)
                                    rc, ry, rx = T.axis.remap("RRR", [i4_1, i5_0, i6_2])
                                    T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_shared[ff, rc, ry, rx])
                                    T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                    conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 8, 1, 6):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_0_i1_0_i2_0_i3_0_fused // 27 * 48 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 8 + ax1)
                            v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 27 // 9 * 18 + i0_1_i1_1_i2_1_i3_1_fused * 6 + i0_2_i1_2_i2_2_i3_2_fused % 6 + ax2)
                            v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 9 * 6 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 1, 6, 8, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 3, 6, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[9, 1, 1, 6, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 11])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 36, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 36, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l176)
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #12: GFLOPs: 1361.7715. Time: 0.1497 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #13: GFLOPs: 1704.8040. Time: 0.1195 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #14: GFLOPs: 662.2378. Time: 0.3077 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #15: GFLOPs: 1114.8676. Time: 0.1828 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #16: GFLOPs: 1311.9245. Time: 0.1553 ms. Best GFLOPs: 3689.6696
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #17: GFLOPs: 3895.2311. Time: 0.0523 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #18: GFLOPs: 1239.3854. Time: 0.1644 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #19: GFLOPs: 85.8876. Time: 2.3728 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #20: GFLOPs: 909.8447. Time: 0.2240 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #21: GFLOPs: 1705.9344. Time: 0.1195 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #22: GFLOPs: 30.0954. Time: 6.7716 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #23: GFLOPs: 550.8280. Time: 0.3700 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #24: GFLOPs: 1443.8613. Time: 0.1411 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #25: GFLOPs: 652.0031. Time: 0.3126 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #26: GFLOPs: 311.8084. Time: 0.6536 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #27: GFLOPs: 1337.7181. Time: 0.1523 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #28: GFLOPs: 1337.7364. Time: 0.1523 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #29: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 3, 224, 224), "float32"], placeholder_1: T.Buffer[(96, 3, 11, 11), "float32"], placeholder_2: T.Buffer[(1, 96, 1, 1), "float32"], T_relu: T.Buffer[(1, 96, 54, 54), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 96, 54, 54], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 3, 224, 224], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([96, 3, 11, 11], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(9, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(6, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i3_3_init, i1_4_init, i2_4_init in T.grid(2, 3, 2, 9):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 4 + i1_3_init * 2 + i1_4_init)
                            yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 3 * 18 + i0_2_i1_2_i2_2_i3_2_fused % 6 // 3 * 9 + i2_4_init)
                            xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 3 * 18 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3_init)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(3, 11, 11):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(34):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(3, i4_0 + 0)
                                        v2 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused // 3 * 72 + i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 4761 // 69)
                                        v3 = T.axis.spatial(224, i0_0_i1_0_i2_0_i3_0_fused % 3 * 72 + i6_0 + ((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2) % 69)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) * 3 + ax0_ax1_ax2_ax3_fused_2 < 4761)
                                        T.reads(placeholder[v0, v1, v2, v3])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = placeholder[v0, v1, v2, v3]
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(1):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(96, ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2)
                                        v1, v2, v3 = T.axis.remap("SSS", [i4_0, i5_0, i6_0])
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 2, 9, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 4 + i1_3 * 2 + i1_4)
                                yy = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 3 * 18 + i0_2_i1_2_i2_2_i3_2_fused % 6 // 3 * 9 + i2_4)
                                xx = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 3 * 18 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + i3_3)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_0])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 3, 224, 224], "float32"], ["TENSOR", [96, 3, 11, 11], "float32"], [4, 4], [0, 0, 0, 0], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy * 4 + ry, xx * 4 + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 4, 9, 3):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(96, i0_1_i1_1_i2_1_i3_1_fused // 2 * 32 + i0_2_i1_2_i2_2_i3_2_fused // 6 * 4 + ax1)
                            v2 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused // 3 * 18 + i0_2_i1_2_i2_2_i3_2_fused % 6 // 3 * 9 + ax2)
                            v3 = T.axis.spatial(54, i0_0_i1_0_i2_0_i3_0_fused % 3 * 18 + i0_1_i1_1_i2_1_i3_1_fused % 2 * 9 + i0_2_i1_2_i2_2_i3_2_fused % 3 * 3 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 3, 8, 2, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[3, 1, 2, 1, 9])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[3, 2, 3, 3, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[11, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 48, 3])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 48, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #30: GFLOPs: 1634.0719. Time: 0.1247 ms. Best GFLOPs: 3895.2311
[13:39:21] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_nn_conv2d_add_nn_relu"] Trial #31: GFLOPs: 2027.0453. Time: 0.1005 ms. Best GFLOPs: 3895.2311
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[13:39:27] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_nn_conv2d_add_nn_relu"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |            N/A |          N/A |                   N/A |      0 |            
  2 |                  fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 52.3187

[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #0: GFLOPs: 157.0522. Time: 0.0040 ms. Best GFLOPs: 157.0522
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #1: GFLOPs: 160.4422. Time: 0.0039 ms. Best GFLOPs: 160.4422
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #2: GFLOPs: 184.1087. Time: 0.0034 ms. Best GFLOPs: 184.1087
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #3: GFLOPs: 184.7656. Time: 0.0034 ms. Best GFLOPs: 184.7656
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #4: GFLOPs: 156.5424. Time: 0.0040 ms. Best GFLOPs: 184.7656
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #5: GFLOPs: 185.8005. Time: 0.0034 ms. Best GFLOPs: 185.8005
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #6: GFLOPs: 171.3701. Time: 0.0037 ms. Best GFLOPs: 185.8005
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #7: GFLOPs: 185.5469. Time: 0.0034 ms. Best GFLOPs: 185.8005
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #8: GFLOPs: 178.7859. Time: 0.0035 ms. Best GFLOPs: 185.8005
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #9: GFLOPs: 186.2372. Time: 0.0034 ms. Best GFLOPs: 186.2372
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #10: GFLOPs: 174.4142. Time: 0.0036 ms. Best GFLOPs: 186.2372
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #11: GFLOPs: 178.7827. Time: 0.0035 ms. Best GFLOPs: 186.2372
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #12: GFLOPs: 178.5889. Time: 0.0035 ms. Best GFLOPs: 186.2372
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #13: GFLOPs: 186.0584. Time: 0.0034 ms. Best GFLOPs: 186.2372
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #14: GFLOPs: 186.6247. Time: 0.0034 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #15: GFLOPs: 167.3599. Time: 0.0038 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #16: GFLOPs: 186.5445. Time: 0.0034 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #17: GFLOPs: 161.4376. Time: 0.0039 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #18: GFLOPs: 171.5356. Time: 0.0037 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #19: GFLOPs: 162.1984. Time: 0.0039 ms. Best GFLOPs: 186.6247
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #20: GFLOPs: 186.7319. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #21: GFLOPs: 186.6811. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #22: GFLOPs: 173.4794. Time: 0.0036 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #23: GFLOPs: 151.1265. Time: 0.0042 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #24: GFLOPs: 183.3035. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #25: GFLOPs: 185.5544. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #26: GFLOPs: 185.5966. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #27: GFLOPs: 176.2360. Time: 0.0036 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #28: GFLOPs: 186.2243. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #29: GFLOPs: 186.0361. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #30: GFLOPs: 178.8542. Time: 0.0035 ms. Best GFLOPs: 186.7319
[13:39:28] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_nn_max_pool2d"] Trial #31: GFLOPs: 185.9796. Time: 0.0034 ms. Best GFLOPs: 186.7319
[13:39:30] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_nn_max_pool2d"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 55.6917

[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #0: GFLOPs: 161.4350. Time: 0.0061 ms. Best GFLOPs: 161.4350
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #1: GFLOPs: 157.7512. Time: 0.0062 ms. Best GFLOPs: 161.4350
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #2: GFLOPs: 161.9158. Time: 0.0061 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #3: GFLOPs: 146.1569. Time: 0.0067 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #4: GFLOPs: 158.6192. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #5: GFLOPs: 158.3057. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #6: GFLOPs: 156.1372. Time: 0.0063 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #7: GFLOPs: 157.6855. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #8: GFLOPs: 156.0223. Time: 0.0063 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #9: GFLOPs: 150.7510. Time: 0.0065 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #10: GFLOPs: 148.1527. Time: 0.0066 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #11: GFLOPs: 152.5100. Time: 0.0064 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #12: GFLOPs: 135.2374. Time: 0.0072 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #13: GFLOPs: 158.6074. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #14: GFLOPs: 158.1904. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #15: GFLOPs: 142.2096. Time: 0.0069 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #16: GFLOPs: 150.5738. Time: 0.0065 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #17: GFLOPs: 158.2684. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #18: GFLOPs: 158.1828. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #19: GFLOPs: 158.1366. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #20: GFLOPs: 158.1945. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #21: GFLOPs: 161.3899. Time: 0.0061 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #22: GFLOPs: 158.6961. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #23: GFLOPs: 150.9514. Time: 0.0065 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #24: GFLOPs: 147.9211. Time: 0.0066 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #25: GFLOPs: 157.7551. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #26: GFLOPs: 158.0762. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #27: GFLOPs: 158.1897. Time: 0.0062 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #28: GFLOPs: 153.1820. Time: 0.0064 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #29: GFLOPs: 146.7530. Time: 0.0067 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #30: GFLOPs: 150.6605. Time: 0.0065 ms. Best GFLOPs: 161.9158
[13:39:31] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_nn_lrn"] Trial #31: GFLOPs: 161.4062. Time: 0.0061 ms. Best GFLOPs: 161.9158
[13:39:32] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_nn_lrn"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |            N/A |          N/A |                   N/A |      0 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 61.7429

[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #0: GFLOPs: 592.4235. Time: 0.7567 ms. Best GFLOPs: 592.4235
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #1: GFLOPs: 265.5203. Time: 1.6883 ms. Best GFLOPs: 592.4235
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #2: GFLOPs: 1230.9072. Time: 0.3642 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #3: GFLOPs: 477.2561. Time: 0.9393 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #4: GFLOPs: 539.7771. Time: 0.8305 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #5: GFLOPs: 7.7645. Time: 57.7336 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #6: GFLOPs: 609.1802. Time: 0.7359 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #7: GFLOPs: 111.1298. Time: 4.0338 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #8: GFLOPs: 644.4789. Time: 0.6956 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #9: GFLOPs: 583.6112. Time: 0.7681 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #10: GFLOPs: 42.4048. Time: 10.5712 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #11: GFLOPs: 585.8566. Time: 0.7652 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #12: GFLOPs: 186.5994. Time: 2.4023 ms. Best GFLOPs: 1230.9072
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #13: GFLOPs: 2487.8290. Time: 0.1802 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #14: GFLOPs: 536.1116. Time: 0.8362 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #15: GFLOPs: 672.1574. Time: 0.6669 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #16: GFLOPs: 303.8321. Time: 1.4754 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #17: GFLOPs: 204.7493. Time: 2.1894 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #18: GFLOPs: 477.9265. Time: 0.9379 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #19: GFLOPs: 170.1322. Time: 2.6348 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #20: GFLOPs: 1085.9979. Time: 0.4128 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #21: GFLOPs: 96.0056. Time: 4.6692 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #22: GFLOPs: 393.8052. Time: 1.1383 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #23: GFLOPs: 556.3475. Time: 0.8057 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #24: GFLOPs: 262.8344. Time: 1.7055 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #25: GFLOPs: 1064.5873. Time: 0.4211 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #26: GFLOPs: 2455.5542. Time: 0.1826 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #27: GFLOPs: 1402.7543. Time: 0.3196 ms. Best GFLOPs: 2487.8290
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #28: GFLOPs: 3075.5410. Time: 0.1458 ms. Best GFLOPs: 3075.5410
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #29: GFLOPs: 1526.3142. Time: 0.2937 ms. Best GFLOPs: 3075.5410
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #30: GFLOPs: 103.0869. Time: 4.3485 ms. Best GFLOPs: 3075.5410
[13:39:33] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_nn_conv2d_add_nn_relu_1"] Trial #31: GFLOPs: 164.6701. Time: 2.7222 ms. Best GFLOPs: 3075.5410
[13:39:34] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_nn_conv2d_add_nn_relu_1"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |            N/A |          N/A |                   N/A |      0 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 207.496

[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #0: GFLOPs: 79.9806. Time: 0.0049 ms. Best GFLOPs: 79.9806
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #1: GFLOPs: 79.0067. Time: 0.0049 ms. Best GFLOPs: 79.9806
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #2: GFLOPs: 83.9575. Time: 0.0046 ms. Best GFLOPs: 83.9575
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #3: GFLOPs: 83.8561. Time: 0.0046 ms. Best GFLOPs: 83.9575
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #4: GFLOPs: 82.9655. Time: 0.0047 ms. Best GFLOPs: 83.9575
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #5: GFLOPs: 84.1153. Time: 0.0046 ms. Best GFLOPs: 84.1153
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #6: GFLOPs: 84.9801. Time: 0.0046 ms. Best GFLOPs: 84.9801
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #7: GFLOPs: 79.2534. Time: 0.0049 ms. Best GFLOPs: 84.9801
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #8: GFLOPs: 95.9252. Time: 0.0041 ms. Best GFLOPs: 95.9252
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #9: GFLOPs: 106.3706. Time: 0.0037 ms. Best GFLOPs: 106.3706
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #10: GFLOPs: 111.2663. Time: 0.0035 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #11: GFLOPs: 102.5968. Time: 0.0038 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #12: GFLOPs: 97.3268. Time: 0.0040 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #13: GFLOPs: 94.1651. Time: 0.0041 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #14: GFLOPs: 88.2997. Time: 0.0044 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #15: GFLOPs: 98.3498. Time: 0.0040 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #16: GFLOPs: 83.8802. Time: 0.0046 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #17: GFLOPs: 80.1441. Time: 0.0049 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #18: GFLOPs: 84.6114. Time: 0.0046 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #19: GFLOPs: 84.7722. Time: 0.0046 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #20: GFLOPs: 79.8362. Time: 0.0049 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #21: GFLOPs: 78.6992. Time: 0.0049 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #22: GFLOPs: 83.5821. Time: 0.0047 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #23: GFLOPs: 77.3712. Time: 0.0050 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #24: GFLOPs: 76.7072. Time: 0.0051 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #25: GFLOPs: 81.6685. Time: 0.0048 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #26: GFLOPs: 82.1289. Time: 0.0047 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #27: GFLOPs: 83.7268. Time: 0.0047 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #28: GFLOPs: 84.5539. Time: 0.0046 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #29: GFLOPs: 64.6188. Time: 0.0060 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #30: GFLOPs: 103.1873. Time: 0.0038 ms. Best GFLOPs: 111.2663
[13:39:34] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_nn_max_pool2d_1"] Trial #31: GFLOPs: 126.9556. Time: 0.0031 ms. Best GFLOPs: 126.9556
[13:39:35] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_nn_max_pool2d_1"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 210.563

[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #0: GFLOPs: 131.2151. Time: 0.0046 ms. Best GFLOPs: 131.2151
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #1: GFLOPs: 142.6481. Time: 0.0042 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #2: GFLOPs: 139.6573. Time: 0.0043 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #3: GFLOPs: 140.3910. Time: 0.0043 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #4: GFLOPs: 131.0540. Time: 0.0046 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #5: GFLOPs: 140.2802. Time: 0.0043 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #6: GFLOPs: 140.3704. Time: 0.0043 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #7: GFLOPs: 139.9052. Time: 0.0043 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #8: GFLOPs: 137.7683. Time: 0.0044 ms. Best GFLOPs: 142.6481
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #9: GFLOPs: 142.9203. Time: 0.0042 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #10: GFLOPs: 137.3714. Time: 0.0044 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #11: GFLOPs: 139.2594. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #12: GFLOPs: 141.3708. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #13: GFLOPs: 141.4109. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #14: GFLOPs: 140.3077. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #15: GFLOPs: 50.8904. Time: 0.0119 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #16: GFLOPs: 51.3713. Time: 0.0118 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #17: GFLOPs: 49.5490. Time: 0.0122 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #18: GFLOPs: 52.7941. Time: 0.0115 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #19: GFLOPs: 64.3588. Time: 0.0094 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #20: GFLOPs: 77.4136. Time: 0.0078 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #21: GFLOPs: 79.6884. Time: 0.0076 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #22: GFLOPs: 76.3695. Time: 0.0079 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #23: GFLOPs: 87.6219. Time: 0.0069 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #24: GFLOPs: 130.1228. Time: 0.0047 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #25: GFLOPs: 92.8491. Time: 0.0065 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #26: GFLOPs: 129.6776. Time: 0.0047 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #27: GFLOPs: 93.0750. Time: 0.0065 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #28: GFLOPs: 93.4543. Time: 0.0065 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #29: GFLOPs: 138.7234. Time: 0.0044 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #30: GFLOPs: 140.4933. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:35] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_nn_lrn_1"] Trial #31: GFLOPs: 140.7333. Time: 0.0043 ms. Best GFLOPs: 142.9203
[13:39:36] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_nn_lrn_1"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |            N/A |          N/A |                   N/A |      0 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 214.801

[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #0: GFLOPs: 246.9621. Time: 1.2114 ms. Best GFLOPs: 246.9621
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #1: GFLOPs: 1314.3627. Time: 0.2276 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #2: GFLOPs: 363.6355. Time: 0.8227 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #3: GFLOPs: 427.0112. Time: 0.7006 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #4: GFLOPs: 587.6958. Time: 0.5091 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #5: GFLOPs: 97.0012. Time: 3.0842 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #6: GFLOPs: 55.2729. Time: 5.4126 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #7: GFLOPs: 21.2932. Time: 14.0501 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #8: GFLOPs: 657.6487. Time: 0.4549 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #9: GFLOPs: 776.0992. Time: 0.3855 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #10: GFLOPs: 1030.1426. Time: 0.2904 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #11: GFLOPs: 1259.8607. Time: 0.2375 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #12: GFLOPs: 32.8122. Time: 9.1177 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #13: GFLOPs: 26.8513. Time: 11.1418 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #14: GFLOPs: 481.0170. Time: 0.6220 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #15: GFLOPs: 1240.6175. Time: 0.2411 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #16: GFLOPs: 192.0631. Time: 1.5577 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #17: GFLOPs: 358.2770. Time: 0.8350 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #18: GFLOPs: 717.8215. Time: 0.4168 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #19: GFLOPs: 113.5592. Time: 2.6345 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #20: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 256, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 256, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        conv2d_nchw_local = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 256, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 256, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(338, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init in T.serial(2):
                        with T.block("conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 169 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3_init)
                            yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 169 // 13)
                            xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                            T.reads()
                            T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [384, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                            conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(256, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(5):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(256, i4_0 + 0)
                                    v2 = T.axis.spatial(15, i5_0 + (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 195 // 15)
                                    v3 = T.axis.spatial(15, (ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1) % 15)
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * 48 + ax0_ax1_ax2_ax3_fused_1 < 195)
                                    T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 192 + (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) // 3)
                                        v1, v2 = T.axis.remap("SS", [i4_0, i5_0])
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 144 + ax0_ax1_ax2_ax3_fused_1 * 3 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1):
                            with T.block("conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 169 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 2 + i1_3)
                                yy = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 169 // 13)
                                xx = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13)
                                rc, ry, rx = T.axis.remap("RRR", [i4_0, i5_0, i6_1])
                                T.reads(conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["conv2d_nchw.cuda", ["TENSOR", [1, 256, 13, 13], "float32"], ["TENSOR", [384, 256, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], "float32"]})
                                conv2d_nchw_local[nn, ff, yy, xx] = conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 2, 1, 1):
                        with T.block("conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 192 + i0_1_i1_1_i2_1_i3_1_fused // 169 * 96 + i0_2_i1_2_i2_2_i3_2_fused * 2 + ax1)
                            v2 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 169 // 13 + ax2)
                            v3 = T.axis.spatial(13, i0_1_i1_1_i2_1_i3_1_fused % 13 + ax3)
                            T.reads(conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[2, 2, 48, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 13, 1, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[256, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 48])
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 48, 3])
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l125, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l125, ann_key="pragma_unroll_explicit", ann_val=1)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l133, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l133, ann_key="pragma_unroll_explicit", ann_val=1)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l162, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l162, ann_key="pragma_unroll_explicit", ann_val=1)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #21: GFLOPs: 20.8999. Time: 14.3144 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #22: GFLOPs: 521.2190. Time: 0.5740 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #23: GFLOPs: 147.6867. Time: 2.0257 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #24: GFLOPs: 320.2516. Time: 0.9342 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #25: GFLOPs: 1062.1428. Time: 0.2817 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #26: GFLOPs: 1059.1230. Time: 0.2825 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #27: GFLOPs: 886.8503. Time: 0.3373 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #28: GFLOPs: 582.9759. Time: 0.5132 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #29: GFLOPs: 159.3895. Time: 1.8770 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #30: GFLOPs: 522.0286. Time: 0.5731 ms. Best GFLOPs: 1314.3627
[13:39:36] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_nn_conv2d_add_nn_relu_2"] Trial #31: GFLOPs: 308.1639. Time: 0.9708 ms. Best GFLOPs: 1314.3627
[13:39:37] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_nn_conv2d_add_nn_relu_2"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |            N/A |          N/A |                   N/A |      0 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 442.418

[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #0: GFLOPs: 169.1739. Time: 1.3265 ms. Best GFLOPs: 169.1739
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #1: GFLOPs: 18.1226. Time: 12.3829 ms. Best GFLOPs: 169.1739
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #2: GFLOPs: 1853.0010. Time: 0.1211 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #3: GFLOPs: 199.4312. Time: 1.1253 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #4: GFLOPs: 1416.4872. Time: 0.1584 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #5: GFLOPs: 204.3908. Time: 1.0979 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #6: GFLOPs: 103.8413. Time: 2.1611 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #7: GFLOPs: 33.3150. Time: 6.7360 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #8: GFLOPs: 269.2018. Time: 0.8336 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #9: GFLOPs: 311.9470. Time: 0.7194 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #10: GFLOPs: 15.5942. Time: 14.3907 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 13, 13), "float32"], placeholder_1: T.Buffer[(384, 192, 3, 3), "float32"], placeholder_2: T.Buffer[(1, 384, 1, 1), "float32"], T_relu: T.Buffer[(1, 384, 13, 13), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        group_conv2d_nchw_local = T.alloc_buffer([1, 384, 13, 13], dtype="float32", scope="local")
        pad_temp_shared = T.alloc_buffer([1, 384, 15, 15], dtype="float32", scope="shared")
        placeholder_shared = T.alloc_buffer([384, 192, 3, 3], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_i3_0_fused in T.thread_binding(6, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_i3_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_i2_2_i3_2_fused in T.thread_binding(52, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init in T.grid(2, 13, 8):
                        with T.block("group_conv2d_nchw_init"):
                            nn = T.axis.spatial(1, 0)
                            ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3_init * 8 + i1_4_init)
                            yy = T.axis.spatial(13, i2_3_init)
                            xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                            T.reads()
                            T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                            group_conv2d_nchw_local[nn, ff, yy, xx] = T.float32(0)
                    for i4_0, i5_0, i6_0 in T.grid(48, 3, 1):
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(4):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(4):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused // 3 * 192 + i4_0 * 4 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 780 // 195)
                                        v2 = T.axis.spatial(15, i5_0 + ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 195 // 15)
                                        v3 = T.axis.spatial(15, ((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2) % 15)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 4 + ax0_ax1_ax2_ax3_fused_2 < 780)
                                        T.reads(placeholder[v0, v1, v2 - 1, v3 - 1])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(1 <= v2 and v2 < 14 and 1 <= v3 and v3 < 14, placeholder[v0, v1, v2 - 1, v3 - 1], T.float32(0), dtype="float32")
                        for ax0_ax1_ax2_ax3_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(52, thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 64 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) // 12)
                                        v1 = T.axis.spatial(192, i4_0 * 4 + (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 12 // 3)
                                        v2 = T.axis.spatial(3, i5_0)
                                        v3 = T.axis.spatial(3, (ax0_ax1_ax2_ax3_fused_0 * 104 + ax0_ax1_ax2_ax3_fused_1 * 2 + ax0_ax1_ax2_ax3_fused_2) % 3)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * 52 + ax0_ax1_ax2_ax3_fused_1) * 2 + ax0_ax1_ax2_ax3_fused_2 < 768)
                                        T.reads(placeholder_1[v0, v1, v2, v3])
                                        T.writes(placeholder_shared[v0, v1, v2, v3])
                                        placeholder_shared[v0, v1, v2, v3] = placeholder_1[v0, v1, v2, v3]
                        for i4_1, i5_1, i6_1, i0_3, i1_3, i2_3, i3_3, i4_2, i5_2, i6_2, i0_4, i1_4, i2_4, i3_4 in T.grid(1, 1, 3, 1, 2, 13, 1, 4, 1, 1, 1, 8, 1, 1):
                            with T.block("group_conv2d_nchw_update"):
                                nn = T.axis.spatial(1, 0)
                                ff = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + i1_3 * 8 + i1_4)
                                yy = T.axis.spatial(13, i2_3)
                                xx = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13)
                                rc = T.axis.reduce(192, i4_0 * 4 + i4_2)
                                ry, rx = T.axis.remap("RR", [i5_0, i6_1])
                                T.reads(group_conv2d_nchw_local[nn, ff, yy, xx], pad_temp_shared[nn, ff // 192 * 192 + rc, yy + ry, xx + rx], placeholder_shared[ff, rc, ry, rx])
                                T.writes(group_conv2d_nchw_local[nn, ff, yy, xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["group_conv2d_nchw.cuda", ["TENSOR", [1, 384, 13, 13], "float32"], ["TENSOR", [384, 192, 3, 3], "float32"], [1, 1], [1, 1, 1, 1], [1, 1], 2, "float32"]})
                                group_conv2d_nchw_local[nn, ff, yy, xx] = group_conv2d_nchw_local[nn, ff, yy, xx] + pad_temp_shared[nn, ff // 192 * 192 + rc, yy + ry, xx + rx] * placeholder_shared[ff, rc, ry, rx]
                    for ax0, ax1, ax2, ax3 in T.grid(1, 16, 13, 1):
                        with T.block("group_conv2d_nchw_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_i3_0_fused * 64 + i0_2_i1_2_i2_2_i3_2_fused // 13 * 16 + ax1)
                            v2 = T.axis.spatial(13, ax2)
                            v3 = T.axis.spatial(13, i0_2_i1_2_i2_2_i3_2_fused % 13 + ax3)
                            T.reads(group_conv2d_nchw_local[v0, v1, v2, v3], placeholder_2[v0, v1, 0, 0])
                            T.writes(T_relu[v0, v1, v2, v3])
                            T_relu[v0, v1, v2, v3] = T.max(group_conv2d_nchw_local[v0, v1, v2, v3] + placeholder_2[v0, v1, 0, 0], T.float32(0))
    

b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="group_conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="T_relu", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16])
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[6, 1, 4, 2, 8])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26])
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 1, 1, 13, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36])
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 1, 13, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46])
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[48, 1, 4])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54])
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60])
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66])
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 52, 4])
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 52, 2])
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l126, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l126, ann_key="pragma_unroll_explicit", ann_val=1)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l135, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l135, ann_key="pragma_unroll_explicit", ann_val=1)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
sch.annotate(block_or_loop=l164, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l164, ann_key="pragma_unroll_explicit", ann_val=1)
b171 = sch.get_block(name="group_conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #12: GFLOPs: 7.5743. Time: 29.6279 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #13: GFLOPs: 43.6938. Time: 5.1360 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #14: GFLOPs: 18.8556. Time: 11.9015 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #15: GFLOPs: 5.5711. Time: 40.2813 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #16: GFLOPs: 145.4604. Time: 1.5428 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #17: GFLOPs: 456.8040. Time: 0.4913 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #18: GFLOPs: 180.8698. Time: 1.2407 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #19: GFLOPs: 72.5773. Time: 3.0920 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #20: GFLOPs: 144.0163. Time: 1.5582 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #21: GFLOPs: 213.5134. Time: 1.0510 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #22: GFLOPs: 477.7732. Time: 0.4697 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #23: GFLOPs: 350.9431. Time: 0.6394 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #24: GFLOPs: 145.1542. Time: 1.5460 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #25: GFLOPs: 576.2258. Time: 0.3894 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #26: GFLOPs: 69.1032. Time: 3.2475 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #27: GFLOPs: 149.0172. Time: 1.5059 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #28: GFLOPs: 855.8871. Time: 0.2622 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #29: GFLOPs: 395.5769. Time: 0.5673 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #30: GFLOPs: 283.2985. Time: 0.7921 ms. Best GFLOPs: 1853.0010
[13:39:38] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_conv2d_add_nn_relu_3"] Trial #31: GFLOPs: 220.0242. Time: 1.0199 ms. Best GFLOPs: 1853.0010
[13:39:39] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_conv2d_add_nn_relu_3"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |            N/A |          N/A |                   N/A |      0 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 563.524

[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #0: GFLOPs: 131.3761. Time: 1.1388 ms. Best GFLOPs: 131.3761
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #1: GFLOPs: 466.7339. Time: 0.3205 ms. Best GFLOPs: 466.7339
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #2: GFLOPs: 373.2992. Time: 0.4008 ms. Best GFLOPs: 466.7339
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #3: GFLOPs: 180.8854. Time: 0.8271 ms. Best GFLOPs: 466.7339
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #4: GFLOPs: 20.1899. Time: 7.4100 ms. Best GFLOPs: 466.7339
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #5: GFLOPs: 954.1382. Time: 0.1568 ms. Best GFLOPs: 954.1382
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #6: GFLOPs: 461.9700. Time: 0.3238 ms. Best GFLOPs: 954.1382
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #7: GFLOPs: 337.8502. Time: 0.4428 ms. Best GFLOPs: 954.1382
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #8: GFLOPs: 986.3024. Time: 0.1517 ms. Best GFLOPs: 986.3024
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #9: GFLOPs: 1853.1185. Time: 0.0807 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #10: GFLOPs: 701.3428. Time: 0.2133 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #11: GFLOPs: 591.7269. Time: 0.2528 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #12: GFLOPs: 1087.3019. Time: 0.1376 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #13: GFLOPs: 771.3638. Time: 0.1940 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #14: GFLOPs: 82.0951. Time: 1.8224 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #15: GFLOPs: 957.3813. Time: 0.1563 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #16: GFLOPs: 564.5767. Time: 0.2650 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #17: GFLOPs: 24.2828. Time: 6.1610 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #18: GFLOPs: 61.0226. Time: 2.4517 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #19: GFLOPs: 203.4930. Time: 0.7352 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #20: GFLOPs: 39.6434. Time: 3.7738 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #21: GFLOPs: 250.6492. Time: 0.5969 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #22: GFLOPs: 963.0090. Time: 0.1554 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #23: GFLOPs: 200.7711. Time: 0.7452 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #24: GFLOPs: 340.7687. Time: 0.4390 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #25: GFLOPs: 836.5555. Time: 0.1788 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #26: GFLOPs: 31.1449. Time: 4.8036 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #27: GFLOPs: 148.4126. Time: 1.0080 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #28: GFLOPs: 183.9601. Time: 0.8133 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #29: GFLOPs: 693.1444. Time: 0.2158 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #30: GFLOPs: 199.7958. Time: 0.7488 ms. Best GFLOPs: 1853.1185
[13:39:39] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_nn_conv2d_add_nn_relu_4"] Trial #31: GFLOPs: 1398.1735. Time: 0.1070 ms. Best GFLOPs: 1853.1185
[13:39:40] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_nn_conv2d_add_nn_relu_4"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |            N/A |          N/A |                   N/A |      0 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 644.257

[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #0: GFLOPs: 37.2558. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #1: GFLOPs: 32.4644. Time: 0.0026 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #2: GFLOPs: 37.1865. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #3: GFLOPs: 37.1268. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #4: GFLOPs: 32.7711. Time: 0.0025 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #5: GFLOPs: 20.2693. Time: 0.0041 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #6: GFLOPs: 36.7044. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #7: GFLOPs: 37.0699. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #8: GFLOPs: 36.8748. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #9: GFLOPs: 35.4246. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #10: GFLOPs: 19.1525. Time: 0.0043 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #11: GFLOPs: 37.1265. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #12: GFLOPs: 36.2761. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #13: GFLOPs: 37.1718. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #14: GFLOPs: 37.0752. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #15: GFLOPs: 20.2907. Time: 0.0041 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #16: GFLOPs: 36.6779. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #17: GFLOPs: 36.9171. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #18: GFLOPs: 37.1730. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #19: GFLOPs: 36.5858. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #20: GFLOPs: 23.6813. Time: 0.0035 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #21: GFLOPs: 37.1727. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #22: GFLOPs: 37.1433. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #23: GFLOPs: 37.0253. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #24: GFLOPs: 25.2622. Time: 0.0033 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #25: GFLOPs: 37.1739. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #26: GFLOPs: 36.7232. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #27: GFLOPs: 37.0778. Time: 0.0022 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #28: GFLOPs: 36.5012. Time: 0.0023 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #29: GFLOPs: 24.3819. Time: 0.0034 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #30: GFLOPs: 32.2243. Time: 0.0026 ms. Best GFLOPs: 37.2558
[13:39:40] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_max_pool2d_2"] Trial #31: GFLOPs: 24.4501. Time: 0.0034 ms. Best GFLOPs: 37.2558
[13:39:41] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_max_pool2d_2"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |        37.2558 |       2.2263 |                2.2263 |     32 |            
 10 |                 fused_reshape |         1 |      1 |            N/A |          N/A |                   N/A |      0 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 646.483

[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0044 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0034 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0024 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0035 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0032 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0033 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:39:41] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:39:42] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_reshape"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |        37.2558 |       2.2263 |                2.2263 |     32 |            
 10 |                 fused_reshape |         1 |      1 |         0.0005 |       2.1825 |                2.1825 |     32 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |            N/A |          N/A |                   N/A |      0 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 648.666

[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #0: GFLOPs: 51.0884. Time: 1.4779 ms. Best GFLOPs: 51.0884
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #1: GFLOPs: 189.0785. Time: 0.3993 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #2: GFLOPs: 23.0365. Time: 3.2777 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #3: GFLOPs: 71.0624. Time: 1.0625 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #4: GFLOPs: 3.3896. Time: 22.2760 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #5: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 9216], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 9216], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused * 1024 + i0_1_i1_1_fused * 64 + i0_2_i1_2_fused * 2 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1024):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(9216, i2_0 * 9 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 9)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(72):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 1024 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 9)
                                        v1 = T.axis.spatial(9216, i2_0 * 9 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 9)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 1, 9, 1, 2):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused * 1024 + i0_1_i1_1_fused * 64 + i0_2_i1_2_fused * 2 + i1_4)
                                k = T.axis.reduce(9216, i2_0 * 9 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 2):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 1024 + i0_1_i1_1_fused * 64 + i0_2_i1_2_fused * 2 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[4, 16, 32, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 9])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32])
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #6: GFLOPs: 49.9278. Time: 1.5123 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #7: GFLOPs: 2.0954. Time: 36.0338 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #8: GFLOPs: 63.3049. Time: 1.1927 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #9: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 9216], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 9216], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(1, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(4, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_2_i1_2_fused * 64 + i1_3_init * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(3072):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(9216, i2_0 * 3 + (ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2))
                                        T.where(ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2 < 3)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(96):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) // 3)
                                        v1 = T.axis.spatial(9216, i2_0 * 3 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) % 3)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 4, 3, 1, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_2_i1_2_fused * 64 + i1_3 * 16 + i1_4)
                                k = T.axis.reduce(9216, i2_0 * 3 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 64):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_2_i1_2_fused * 64 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 1, 32, 4, 16])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[3072, 1, 3])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 3])
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 2])
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #10: GFLOPs: 56.3122. Time: 1.3408 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #11: GFLOPs: 15.4502. Time: 4.8870 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #12: GFLOPs: 3.8841. Time: 19.4395 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #13: GFLOPs: 96.0525. Time: 0.7861 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #14: GFLOPs: 4.9598. Time: 15.2235 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #15: GFLOPs: 1.8539. Time: 40.7288 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #16: GFLOPs: 46.7781. Time: 1.6141 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #17: GFLOPs: 50.0946. Time: 1.5073 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #18: GFLOPs: 26.7876. Time: 2.8187 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #19: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 9216], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 9216], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(16, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_4_init in T.serial(4):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused * 128 + i0_2_i1_2_fused * 4 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1024):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1, 0)
                                        v1 = T.axis.spatial(9216, i2_0 * 9 + (ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2))
                                        T.where(ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2 < 9)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(18):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 9)
                                        v1 = T.axis.spatial(9216, i2_0 * 9 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 9)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 1, 9, 1, 4):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused * 128 + i0_2_i1_2_fused * 4 + i1_4)
                                k = T.axis.reduce(9216, i2_0 * 9 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 4):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused * 128 + i0_2_i1_2_fused * 4 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[16, 2, 32, 1, 4])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 9])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62, l63 = sch.split(loop=l60, factors=[None, 32, 3])
sch.vectorize(loop=l63)
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l64, l65, l66, l67, l68 = sch.get_loops(block=b46)
l69, l70, l71 = sch.split(loop=l68, factors=[None, 32, 4])
sch.vectorize(loop=l71)
sch.bind(loop=l70, thread_axis="threadIdx.x")
b72 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b72, ann_key="meta_schedule.unroll_explicit")
b73, b74, b75, b76 = sch.get_child_blocks(b72)
l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l77, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l77, ann_key="pragma_unroll_explicit", ann_val=1)
l84, l85, l86, l87, l88, l89, l90 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l84, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l84, ann_key="pragma_unroll_explicit", ann_val=1)
l91, l92, l93, l94, l95, l96, l97, l98, l99, l100 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l91, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l91, ann_key="pragma_unroll_explicit", ann_val=1)
l101, l102, l103, l104, l105 = sch.get_loops(block=b76)
sch.annotate(block_or_loop=l101, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l101, ann_key="pragma_unroll_explicit", ann_val=1)
b106 = sch.get_block(name="T_matmul_NT", func_name="main")
l107, l108, l109, l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b106)
b117 = sch.decompose_reduction(block=b106, loop=l110)
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #20: GFLOPs: 16.2547. Time: 4.6452 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #21: GFLOPs: 1.7880. Time: 42.2291 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #22: GFLOPs: 14.7179. Time: 5.1302 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #23: GFLOPs: 18.5094. Time: 4.0793 ms. Best GFLOPs: 189.0785
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #24: GFLOPs: 195.9108. Time: 0.3854 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #25: GFLOPs: 49.5418. Time: 1.5241 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #26: GFLOPs: 12.6029. Time: 5.9911 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #27: GFLOPs: 51.3165. Time: 1.4714 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #28: GFLOPs: 1.4870. Time: 50.7773 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #29: GFLOPs: 31.5018. Time: 2.3969 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #30: GFLOPs: 7.7403. Time: 9.7548 ms. Best GFLOPs: 195.9108
[13:39:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_dense_add_nn_relu"] Trial #31: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 9216), "float32"], placeholder_1: T.Buffer[(4096, 9216), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 9216], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 9216], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i1_4_init in T.serial(2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused * 256 + i0_2_i1_2_fused * 2 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1024):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(9216, i2_0 * 9 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 9)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(18):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + (ax0_ax1_fused_0 * 256 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) // 9)
                                        v1 = T.axis.spatial(9216, i2_0 * 9 + (ax0_ax1_fused_0 * 256 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) % 9)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 1, 1, 9, 1, 2):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused * 256 + i0_2_i1_2_fused * 2 + i1_4)
                                k = T.axis.reduce(9216, i2_0 * 9 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 9216], "float32"], ["TENSOR", [4096, 9216], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 2):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused * 256 + i0_2_i1_2_fused * 2 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[8, 2, 128, 1, 2])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[1024, 1, 9])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 128])
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 128, 2])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l76, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l76, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
[13:39:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_dense_add_nn_relu"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |        37.2558 |       2.2263 |                2.2263 |     32 |            
 10 |                 fused_reshape |         1 |      1 |         0.0005 |       2.1825 |                2.1825 |     32 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |       195.9108 |     385.4085 |              385.4085 |     32 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |            N/A |          N/A |                   N/A |      0 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 1034.07

[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #0: GFLOPs: 12.8520. Time: 2.6115 ms. Best GFLOPs: 12.8520
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #1: GFLOPs: 1.3608. Time: 24.6636 ms. Best GFLOPs: 12.8520
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #2: GFLOPs: 8.2007. Time: 4.0926 ms. Best GFLOPs: 12.8520
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #3: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 4096), "float32"], placeholder_1: T.Buffer[(4096, 4096), "float32"], placeholder_2: T.Buffer[(1, 4096), "float32"], T_relu: T.Buffer[(1, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([1, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([1, 4096], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 4096], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x"):
            for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i1_4_init in T.grid(2, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(1, 0)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_1_i1_1_fused * 1024 + i0_2_i1_2_fused * 32 + i1_3_init * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [4096, 4096], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(2048):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1, 0)
                                    v1 = T.axis.spatial(4096, i2_0 * 2 + ax0_ax1_fused_1)
                                    T.where(ax0_ax1_fused_1 < 2)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(32):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 2)
                                        v1 = T.axis.spatial(4096, i2_0 * 2 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 2)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(2, 1, 2, 1, 1, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(1, 0)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_1_i1_1_fused * 1024 + i0_2_i1_2_fused * 32 + i1_3 * 16 + i1_4)
                                k = T.axis.reduce(4096, i2_0 * 2 + i2_1)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [1, 4096], "float32"], ["TENSOR", [4096, 4096], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(1, 32):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(1, ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused * 2048 + i0_1_i1_1_fused * 1024 + i0_2_i1_2_fused * 32 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1], placeholder_2[v0, v1])
                            T.writes(T_relu[v0, v1])
                            T_relu[v0, v1] = T.max(T_matmul_NT_local[v0, v1] + placeholder_2[v0, v1], T.float32(0))
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_relu", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6 = sch.get_loops(block=b0)
v7, v8, v9, v10, v11 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l12, l13, l14, l15, l16 = sch.split(loop=l4, factors=[v7, v8, v9, v10, v11])
v17, v18, v19, v20, v21 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[2, 2, 32, 2, 16])
l22, l23, l24, l25, l26 = sch.split(loop=l5, factors=[v17, v18, v19, v20, v21])
v27, v28, v29 = sch.sample_perfect_tile(loop=l6, n=3, max_innermost_factor=64, decision=[2048, 2, 1])
l30, l31, l32 = sch.split(loop=l6, factors=[v27, v28, v29])
sch.reorder(l12, l22, l13, l23, l14, l24, l30, l31, l15, l25, l32, l16, l26)
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="blockIdx.x")
l34 = sch.fuse(l13, l23)
sch.bind(loop=l34, thread_axis="vthread.x")
l35 = sch.fuse(l14, l24)
sch.bind(loop=l35, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b36 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b36, loop=l35, preserve_unit_loops=True)
b37 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b37, loop=l30, preserve_unit_loops=True)
l38, l39, l40, l41, l42, l43 = sch.get_loops(block=b37)
l44 = sch.fuse(l42, l43)
v45 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch", ann_val=v45)
b46 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b46, loop=l30, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52 = sch.get_loops(block=b46)
l53 = sch.fuse(l51, l52)
v54 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v54)
sch.reverse_compute_inline(block=b2)
sch.reverse_compute_inline(block=b1)
v55 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v55)
sch.enter_postproc()
sch.unannotate(block_or_loop=b37, ann_key="meta_schedule.cooperative_fetch")
l56, l57, l58, l59, l60 = sch.get_loops(block=b37)
l61, l62 = sch.split(loop=l60, factors=[None, 32])
sch.bind(loop=l62, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l63, l64, l65, l66, l67 = sch.get_loops(block=b46)
l68, l69, l70 = sch.split(loop=l67, factors=[None, 32, 4])
sch.vectorize(loop=l70)
sch.bind(loop=l69, thread_axis="threadIdx.x")
b71 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b71, ann_key="meta_schedule.unroll_explicit")
b72, b73, b74, b75 = sch.get_child_blocks(b71)
l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b72)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b73)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b74)
l99, l100, l101, l102, l103 = sch.get_loops(block=b75)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #4: GFLOPs: 4.5565. Time: 7.3659 ms. Best GFLOPs: 12.8520
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #5: GFLOPs: 15.8940. Time: 2.1117 ms. Best GFLOPs: 15.8940
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #6: GFLOPs: 82.6233. Time: 0.4062 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #7: GFLOPs: 5.4522. Time: 6.1558 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #8: GFLOPs: 1.0440. Time: 32.1474 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #9: GFLOPs: 12.2714. Time: 2.7350 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #10: GFLOPs: 10.1508. Time: 3.3064 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #11: GFLOPs: 57.0027. Time: 0.5888 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #12: GFLOPs: 0.9191. Time: 36.5168 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #13: GFLOPs: 2.7277. Time: 12.3042 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #14: GFLOPs: 43.4042. Time: 0.7733 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #15: GFLOPs: 57.5299. Time: 0.5834 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #16: GFLOPs: 21.0067. Time: 1.5977 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #17: GFLOPs: 52.2278. Time: 0.6426 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #18: GFLOPs: 9.7482. Time: 3.4430 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #19: GFLOPs: 7.6223. Time: 4.4032 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #20: GFLOPs: 28.6405. Time: 1.1719 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #21: GFLOPs: 52.9045. Time: 0.6344 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #22: GFLOPs: 1.6922. Time: 19.8337 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #23: GFLOPs: 41.9089. Time: 0.8008 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #24: GFLOPs: 7.5577. Time: 4.4409 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #25: GFLOPs: 24.8895. Time: 1.3485 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #26: GFLOPs: 19.7721. Time: 1.6975 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #27: GFLOPs: 7.4972. Time: 4.4767 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #28: GFLOPs: 59.7934. Time: 0.5613 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #29: GFLOPs: 7.2606. Time: 4.6226 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #30: GFLOPs: 1.6763. Time: 20.0220 ms. Best GFLOPs: 82.6233
[13:39:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_nn_dense_add_nn_relu_1"] Trial #31: GFLOPs: 10.8844. Time: 3.0836 ms. Best GFLOPs: 82.6233
[13:39:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_nn_dense_add_nn_relu_1"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |        37.2558 |       2.2263 |                2.2263 |     32 |            
 10 |                 fused_reshape |         1 |      1 |         0.0005 |       2.1825 |                2.1825 |     32 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |       195.9108 |     385.4085 |              385.4085 |     32 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |        82.6233 |     406.2128 |              406.2128 |     32 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 1440.29

[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #0: GFLOPs: 3.1846. Time: 0.5145 ms. Best GFLOPs: 3.1846
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #1: GFLOPs: 5.9675. Time: 0.2746 ms. Best GFLOPs: 5.9675
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #2: GFLOPs: 7.3682. Time: 0.2224 ms. Best GFLOPs: 7.3682
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #3: GFLOPs: 1.9298. Time: 0.8491 ms. Best GFLOPs: 7.3682
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #4: GFLOPs: 17.6168. Time: 0.0930 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #5: GFLOPs: 5.4391. Time: 0.3013 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #6: GFLOPs: 17.1224. Time: 0.0957 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #7: GFLOPs: 3.3021. Time: 0.4962 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #8: GFLOPs: 5.5848. Time: 0.2934 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #9: GFLOPs: 3.1854. Time: 0.5144 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #10: GFLOPs: 9.3419. Time: 0.1754 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #11: GFLOPs: 7.6696. Time: 0.2136 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #12: GFLOPs: 5.0817. Time: 0.3225 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #13: GFLOPs: 5.8556. Time: 0.2798 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #14: GFLOPs: 5.9316. Time: 0.2762 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #15: GFLOPs: 16.9772. Time: 0.0965 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #16: GFLOPs: 5.3598. Time: 0.3057 ms. Best GFLOPs: 17.6168
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #17: GFLOPs: 20.0716. Time: 0.0816 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #18: GFLOPs: 3.9277. Time: 0.4172 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #19: GFLOPs: 6.3647. Time: 0.2575 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #20: GFLOPs: 7.4363. Time: 0.2204 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #21: GFLOPs: 3.3826. Time: 0.4844 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #22: GFLOPs: 6.9242. Time: 0.2366 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #23: GFLOPs: 6.4246. Time: 0.2550 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #24: GFLOPs: 16.2243. Time: 0.1010 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #25: GFLOPs: 9.1776. Time: 0.1785 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #26: GFLOPs: 12.0419. Time: 0.1361 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #27: GFLOPs: 3.5009. Time: 0.4681 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #28: GFLOPs: 7.7461. Time: 0.2115 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #29: GFLOPs: 5.7752. Time: 0.2837 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #30: GFLOPs: 8.3879. Time: 0.1954 ms. Best GFLOPs: 20.0716
[13:39:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense_add"] Trial #31: GFLOPs: 5.9398. Time: 0.2759 ms. Best GFLOPs: 20.0716
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_dense_add"
 ID |                          Name |      FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------
  0 |   fused_nn_conv2d_add_nn_relu | 203793408 |      1 |      3895.2311 |      52.3187 |               52.3187 |     32 |            
  1 |           fused_nn_max_pool2d |    629856 |      1 |       186.7319 |       3.3730 |                3.3730 |     32 |            
  2 |                  fused_nn_lrn |    979776 |      1 |       161.9158 |       6.0511 |                6.0511 |     32 |            
  3 | fused_nn_conv2d_add_nn_relu_1 | 448270848 |      1 |      3075.5410 |     145.7535 |              145.7535 |     32 |            
  4 |         fused_nn_max_pool2d_1 |    389376 |      1 |       126.9556 |       3.0670 |                3.0670 |     32 |            
  5 |                fused_nn_lrn_1 |    605696 |      1 |       142.9203 |       4.2380 |                4.2380 |     32 |            
  6 | fused_nn_conv2d_add_nn_relu_2 | 299170560 |      1 |      1314.3627 |     227.6164 |              227.6164 |     32 |            
  7 | fused_nn_conv2d_add_nn_relu_3 | 224410368 |      1 |      1853.0010 |     121.1064 |              121.1064 |     32 |            
  8 | fused_nn_conv2d_add_nn_relu_4 | 149606912 |      1 |      1853.1185 |      80.7325 |               80.7325 |     32 |            
  9 |         fused_nn_max_pool2d_2 |     82944 |      1 |        37.2558 |       2.2263 |                2.2263 |     32 |            
 10 |                 fused_reshape |         1 |      1 |         0.0005 |       2.1825 |                2.1825 |     32 |            
 11 |    fused_nn_dense_add_nn_relu |  75505664 |      1 |       195.9108 |     385.4085 |              385.4085 |     32 |            
 12 |  fused_nn_dense_add_nn_relu_1 |  33562624 |      1 |        82.6233 |     406.2128 |              406.2128 |     32 |            
 13 |            fused_nn_dense_add |   1638600 |      1 |        20.0716 |      81.6376 |               81.6376 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 1521.92

[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_nn_dense_add_nn_relu_1"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 13
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_dense_add_nn_relu"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 12
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_nn_conv2d_add_nn_relu_2"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 11
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_nn_conv2d_add_nn_relu_1"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 10
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_conv2d_add_nn_relu_3"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 9
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_dense_add"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 8
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_nn_conv2d_add_nn_relu_4"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 7
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_nn_conv2d_add_nn_relu"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 6
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_nn_lrn"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 5
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_nn_lrn_1"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 4
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_nn_max_pool2d"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 3
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_nn_max_pool2d_1"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 2
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_max_pool2d_2"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 1
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_reshape"
[13:39:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 0
[[-1.0273618  -1.0719894  -1.2299227  -1.0144093  -1.0273492  -1.2677681
  -1.0142442  -1.498564   -1.4757857  -0.99966204 -1.0420551  -1.0171545
  -1.045497   -1.0255269  -1.0120339  -1.0249642  -1.017374   -0.99915206
  -1.0625278  -1.3642205  -1.8014995  -1.0385516  -1.0817356  -1.0280222
  -1.015593   -1.7213972  -1.0180905  -1.1671567  -1.0170368  -1.061521
  -1.0351306  -1.6042762  -1.017196   -1.762561   -1.013118   -1.2320064
  -1.0194387  -1.0246718  -1.0481607  -1.0297087  -1.6215802  -1.5936778
  -1.093599   -1.0372972  -1.0433843  -1.6628956  -1.2656494  -1.4713366
  -1.0224708  -0.99286705 -1.0168875  -1.0272876  -1.0455942  -1.0135723
  -1.0183889  -1.4416972  -1.7087299  -1.3434633  -1.2670125  -1.6595393
  -1.0455351  -1.0525595  -1.4549459  -1.0511951  -1.0187678  -1.0344703
  -1.1867932  -1.014298   -1.0307527  -1.0904057  -1.0243567  -1.6632409
  -1.2702669  -1.4851239  -1.0245378  -1.0247473  -1.0167259  -1.4439527
  -1.0492424  -1.1256984  -1.0173166  -1.2111157  -1.436876   -1.2038834
  -1.0277481  -1.424974   -1.0041782  -1.0330868  -1.028914   -1.5853413
  -1.0218239  -1.0573237  -1.0122287  -1.0132211  -1.5003729  -0.9966789
  -1.6007726  -1.0094129  -1.3585557  -1.0428213  -1.0470679  -1.0341552
  -1.3237313  -1.0228785  -1.5549794  -1.0046166  -1.0182753  -1.0409414
  -1.0213541  -1.6057723  -1.0186636  -1.239468   -1.2788599  -1.0159918
  -1.0323263  -1.0201304  -1.0256265  -1.0335711  -1.0244396  -1.0454298
  -1.0506324  -1.4434917  -1.0049425  -1.072185   -1.2664806  -1.0327986
  -1.0028763  -1.172764   -1.1550311  -1.0289032  -1.2083311  -1.0229152
  -1.0171276  -1.605845   -1.1806637  -1.019851   -1.338962   -1.0182219
  -1.2104213  -1.0197085  -1.4092755  -1.0206108  -1.0061913  -1.5450208
  -1.6934516  -1.0310409  -1.0139054  -1.0452213  -1.0297644  -1.2064955
  -1.029829   -1.5061765  -1.0016297  -1.0501745  -1.0293052  -1.0265766
  -1.5355141  -1.3007637  -1.7123297  -1.0155693  -1.4110374  -1.1906459
  -1.016989   -1.1985567  -1.2095124  -1.3500513  -1.0491277  -1.0176449
  -1.228337   -1.0104091  -1.0608337  -1.0166802  -1.0111328  -1.0139762
  -1.0422591  -1.0896815  -1.2175026  -1.5242431  -1.2091005  -1.4059384
  -1.0151964  -1.4614893  -1.2663361  -1.0304247  -1.111504   -1.1463543
  -1.035845   -1.6345214  -1.0645798  -1.0133665  -1.0385169  -1.0307355
  -1.0306289  -1.4365463  -1.0026847  -1.0293307  -1.2167579  -1.4228826
  -1.0346451  -1.0089486 ]]
[[-1.0273618  -1.0719893  -1.2299227  -1.0144093  -1.0273492  -1.2677681
  -1.0142442  -1.498564   -1.4757857  -0.99966204 -1.0420551  -1.0171545
  -1.045497   -1.0255269  -1.0120339  -1.0249642  -1.017374   -0.9991521
  -1.0625278  -1.3642205  -1.8014995  -1.0385516  -1.0817357  -1.0280222
  -1.015593   -1.721397   -1.0180905  -1.1671567  -1.0170368  -1.0615209
  -1.0351306  -1.6042762  -1.0171959  -1.7625608  -1.013118   -1.2320064
  -1.0194387  -1.0246718  -1.0481607  -1.0297087  -1.6215802  -1.5936778
  -1.093599   -1.0372972  -1.0433843  -1.6628956  -1.2656496  -1.4713366
  -1.0224707  -0.99286705 -1.0168877  -1.0272876  -1.0455942  -1.0135723
  -1.0183889  -1.4416972  -1.7087299  -1.3434633  -1.2670125  -1.6595395
  -1.0455351  -1.0525595  -1.4549459  -1.0511951  -1.0187678  -1.0344703
  -1.1867932  -1.0142978  -1.0307527  -1.0904056  -1.0243567  -1.6632409
  -1.2702669  -1.4851239  -1.0245379  -1.0247474  -1.0167259  -1.4439526
  -1.0492423  -1.1256984  -1.0173166  -1.2111157  -1.436876   -1.2038834
  -1.0277481  -1.424974   -1.0041782  -1.0330868  -1.028914   -1.5853412
  -1.0218239  -1.0573237  -1.0122287  -1.0132211  -1.5003728  -0.99667895
  -1.6007726  -1.0094129  -1.3585556  -1.0428213  -1.0470679  -1.0341552
  -1.3237314  -1.0228785  -1.5549794  -1.0046166  -1.0182753  -1.0409412
  -1.0213541  -1.6057723  -1.0186636  -1.239468   -1.2788599  -1.0159918
  -1.0323263  -1.0201303  -1.0256265  -1.0335711  -1.0244396  -1.0454297
  -1.0506324  -1.4434917  -1.0049425  -1.072185   -1.2664806  -1.0327986
  -1.0028763  -1.172764   -1.1550311  -1.0289032  -1.2083311  -1.0229152
  -1.0171278  -1.6058449  -1.1806637  -1.019851   -1.338962   -1.0182219
  -1.2104213  -1.0197085  -1.4092755  -1.0206108  -1.0061911  -1.5450208
  -1.6934516  -1.0310408  -1.0139055  -1.0452213  -1.0297643  -1.2064954
  -1.029829   -1.5061765  -1.0016297  -1.0501745  -1.0293052  -1.0265768
  -1.5355141  -1.3007637  -1.7123297  -1.0155693  -1.4110374  -1.1906459
  -1.016989   -1.1985568  -1.2095122  -1.3500513  -1.0491276  -1.0176449
  -1.228337   -1.0104091  -1.0608337  -1.0166802  -1.0111328  -1.0139762
  -1.0422592  -1.0896814  -1.2175026  -1.5242431  -1.2091006  -1.4059384
  -1.0151964  -1.4614893  -1.2663361  -1.0304247  -1.111504   -1.1463543
  -1.0358449  -1.6345214  -1.0645797  -1.0133665  -1.0385169  -1.0307355
  -1.0306289  -1.4365464  -1.0026847  -1.0293308  -1.2167579  -1.4228826
  -1.0346451  -1.0089486 ]]
