nohup: ignoring input
Starting to build with relay.
One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.
[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #0: "fused_squeeze"
[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_squeeze: T.Buffer[(1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 384):
            with T.block("T_squeeze"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder_1[ax0, ax1, 0])
                T.writes(T_squeeze[ax0, ax1])
                T_squeeze[ax0, ax1] = placeholder_1[ax0, ax1, 0]
    

[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_squeeze: T.Buffer[(1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1 in T.grid(1, 384):
                with T.block("T_squeeze"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder_1[ax0, ax1, 0])
                    T.writes(T_squeeze[ax0, ax1])
                    T_squeeze[ax0, ax1] = placeholder_1[ax0, ax1, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"
[13:35:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_transpose: T.Buffer[(16, 64, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 384, 16, 64], dtype="float32")
        T_transpose_1 = T.alloc_buffer([1, 16, 384, 64], dtype="float32")
        T_reshape_2 = T.alloc_buffer([16, 384, 64], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_reshape[ax0, ax1, ax2] + placeholder_1[ax2]
        for i0, i1, i2, i3 in T.grid(1, 384, 16, 64):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024]
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 64):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3])
                T.writes(T_transpose_1[ax0, ax1, ax2, ax3])
                T_transpose_1[ax0, ax1, ax2, ax3] = T_reshape_1[ax0, ax2, ax1, ax3]
        for i0, i1, i2 in T.grid(16, 384, 64):
            with T.block("T_reshape_2"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_transpose_1[0, ((ax2 // 64 + ax1) // 384 + ax0) % 16, (ax2 // 64 + ax1) % 384, ax2 % 64])
                T.writes(T_reshape_2[ax0, ax1, ax2])
                T_reshape_2[ax0, ax1, ax2] = T_transpose_1[0, ((ax2 // 64 + ax1) // 384 + ax0) % 16, (ax2 // 64 + ax1) % 384, ax2 % 64]
        for i0, i1, i2 in T.grid(16, 64, 384):
            with T.block("T_transpose_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape_2[ax0, ax2, ax1])
                T.writes(T_transpose[ax0, ax1, ax2])
                T_transpose[ax0, ax1, ax2] = T_reshape_2[ax0, ax2, ax1]
    

[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_transpose: T.Buffer[(16, 64, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2 in T.grid(16, 384, 64):
                with T.block("T_reshape_2"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[(ax2 // 64 + ax1) % 384, ((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64], placeholder_1[((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64])
                    T.writes(T_transpose[ax0, ax2, ax1])
                    T_transpose[ax0, ax2, ax1] = placeholder[((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 // 1024 + ((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) // 1024 + (ax2 // 64 + ax1) % 384) % 384) % 384, (((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 % 1024] + placeholder_1[(((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="T_transpose", func_name="main")
b4 = sch.get_block(name="T_transpose_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384), "int64"], T_multiply: T.Buffer[(1, 1, 1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_expand_dims = T.alloc_buffer([1, 1, 384], dtype="int64")
        T_expand_dims_1 = T.alloc_buffer([1, 1, 1, 384], dtype="int64")
        T_cast = T.alloc_buffer([1, 1, 1, 384], dtype="float32")
        T_subtract = T.alloc_buffer([1, 1, 1, 384], dtype="float32")
        compile_engine_const_1 = T.alloc_buffer([], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(1)
        for i0, i1, i2 in T.grid(1, 1, 384):
            with T.block("T_expand_dims"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax2])
                T.writes(T_expand_dims[ax0, ax1, ax2])
                T_expand_dims[ax0, ax1, ax2] = placeholder[ax0, ax2]
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 384):
            with T.block("T_expand_dims_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_expand_dims[ax0, ax1, ax3])
                T.writes(T_expand_dims_1[ax0, ax1, ax2, ax3])
                T_expand_dims_1[ax0, ax1, ax2, ax3] = T_expand_dims[ax0, ax1, ax3]
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 384):
            with T.block("T_cast"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_expand_dims_1[ax0, ax1, ax2, ax3])
                T.writes(T_cast[ax0, ax1, ax2, ax3])
                T_cast[ax0, ax1, ax2, ax3] = T.cast(T_expand_dims_1[ax0, ax1, ax2, ax3], "float32")
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 384):
            with T.block("T_subtract"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(compile_engine_const[()], T_cast[ax0, ax1, ax2, ax3])
                T.writes(T_subtract[ax0, ax1, ax2, ax3])
                T_subtract[ax0, ax1, ax2, ax3] = compile_engine_const[()] - T_cast[ax0, ax1, ax2, ax3]
        with T.block("compile_engine_const_1"):
            T.reads()
            T.writes(compile_engine_const_1[()])
            compile_engine_const_1[()] = T.float32(-10000)
        for i0, i1, i2, i3 in T.grid(1, 1, 1, 384):
            with T.block("T_multiply"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_subtract[ax0, ax1, ax2, ax3], compile_engine_const_1[()])
                T.writes(T_multiply[ax0, ax1, ax2, ax3])
                T_multiply[ax0, ax1, ax2, ax3] = T_subtract[ax0, ax1, ax2, ax3] * compile_engine_const_1[()]
    

[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384), "int64"], T_multiply: T.Buffer[(1, 1, 1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2, i3 in T.grid(1, 1, 1, 384):
                with T.block("T_multiply"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax3])
                    T.writes(T_multiply[ax0, ax1, ax2, ax3])
                    T_multiply[ax0, ax1, ax2, ax3] = (T.float32(1) - T.cast(placeholder[ax0, ax3], "float32")) * T.float32(-10000)
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_expand_dims", func_name="main")
b2 = sch.get_block(name="T_expand_dims_1", func_name="main")
b3 = sch.get_block(name="T_cast", func_name="main")
b4 = sch.get_block(name="T_subtract", func_name="main")
b5 = sch.get_block(name="compile_engine_const_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"
[13:35:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_transpose: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_reshape_1 = T.alloc_buffer([1, 384, 16, 64], dtype="float32")
        T_transpose_1 = T.alloc_buffer([1, 16, 64, 384], dtype="float32")
        T_reshape_2 = T.alloc_buffer([16, 64, 384], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_reshape[ax0, ax1, ax2] + placeholder_1[ax2]
        for i0, i1, i2, i3 in T.grid(1, 384, 16, 64):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024]
        for i0, i1, i2, i3 in T.grid(1, 16, 64, 384):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_reshape_1[ax0, ax3, ax1, ax2])
                T.writes(T_transpose_1[ax0, ax1, ax2, ax3])
                T_transpose_1[ax0, ax1, ax2, ax3] = T_reshape_1[ax0, ax3, ax1, ax2]
        for i0, i1, i2 in T.grid(16, 64, 384):
            with T.block("T_reshape_2"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_transpose_1[0, ((ax2 // 384 + ax1) // 64 + ax0) % 16, (ax2 // 384 + ax1) % 64, ax2 % 384])
                T.writes(T_reshape_2[ax0, ax1, ax2])
                T_reshape_2[ax0, ax1, ax2] = T_transpose_1[0, ((ax2 // 384 + ax1) // 64 + ax0) % 16, (ax2 // 384 + ax1) % 64, ax2 % 384]
        for i0, i1, i2 in T.grid(16, 384, 64):
            with T.block("T_transpose_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape_2[ax0, ax2, ax1])
                T.writes(T_transpose[ax0, ax1, ax2])
                T_transpose[ax0, ax1, ax2] = T_reshape_2[ax0, ax2, ax1]
    

[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_transpose: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1, i2 in T.grid(16, 64, 384):
                with T.block("T_reshape_2"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[ax2 % 384, ((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64], placeholder_1[((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64])
                    T.writes(T_transpose[ax0, ax2, ax1])
                    T_transpose[ax0, ax2, ax1] = placeholder[((((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64) % 1024 // 1024 + ((((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64) // 1024 + ax2 % 384) % 384) % 384, (((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64) % 1024 % 1024] + placeholder_1[(((ax2 // 384 + ax1) // 64 + ax0) % 16 * 64 + (ax2 // 384 + ax1) % 64) % 1024]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="T_transpose", func_name="main")
b4 = sch.get_block(name="T_transpose_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #4: "fused_mean"
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        placeholder_red = T.alloc_buffer([1, 384, 1], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 1, 1024):
            with T.block("placeholder_red"):
                ax0, ax1, ax2, k2 = T.axis.remap("SSSR", [i0, i1, i2, i3])
                T.reads(placeholder[ax0, ax1, k2])
                T.writes(placeholder_red[ax0, ax1, ax2])
                with T.init():
                    placeholder_red[ax0, ax1, ax2] = T.float32(0)
                placeholder_red[ax0, ax1, ax2] = placeholder_red[ax0, ax1, ax2] + placeholder[ax0, ax1, k2]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder_red[ax0, ax1, ax2])
                T.writes(T_divide[ax0, ax1, ax2])
                T_divide[ax0, ax1, ax2] = placeholder_red[ax0, ax1, ax2] * T.float32(0.0009765625)
    

[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 2 design space(s) generated
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            placeholder_red_shared = T.alloc_buffer([1, 384, 1], dtype="float32", scope="shared")
            for i0, i1, i2_0 in T.grid(1, 384, 1):
                for ax0, ax1, ax2, ax3_0 in T.grid(1, 1, 1, 16):
                    for ax3_1 in T.thread_binding(64, thread="threadIdx.x"):
                        with T.block("placeholder_red"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(384, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            k2 = T.axis.reduce(1024, ax3_0 * 64 + ax3_1)
                            T.reads(placeholder[ax0_1, ax1_1, k2])
                            T.writes(placeholder_red_shared[ax0_1, ax1_1, ax2_1])
                            with T.init():
                                placeholder_red_shared[ax0_1, ax1_1, ax2_1] = T.float32(0)
                            placeholder_red_shared[ax0_1, ax1_1, ax2_1] = placeholder_red_shared[ax0_1, ax1_1, ax2_1] + placeholder[ax0_1, ax1_1, k2]
                for i2_1 in T.thread_binding(64, thread="threadIdx.x"):
                    with T.block("T_divide"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(384, i1)
                        ax2 = T.axis.spatial(1, 0)
                        T.where(i2_1 < 1)
                        T.reads(placeholder_red_shared[ax0, ax1, ax2])
                        T.writes(T_divide[ax0, ax1, ax2])
                        T_divide[ax0, ax1, ax2] = placeholder_red_shared[ax0, ax1, ax2] * T.float32(0.0009765625)
    

b0 = sch.get_block(name="placeholder_red", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
b2, = sch.get_consumers(block=b0)
l3, l4, l5 = sch.get_loops(block=b2)
v6 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=4)
l7, l8 = sch.split(loop=l5, factors=[None, v6])
sch.bind(loop=l8, thread_axis="threadIdx.x")
sch.compute_at(block=b0, loop=l7, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l9, l10, l11, l12, l13, l14, l15 = sch.get_loops(block=b0)
l16, l17 = sch.split(loop=l15, factors=[None, v6])
sch.bind(loop=l17, thread_axis="threadIdx.x")
v18 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v18)
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            placeholder_red = T.alloc_buffer([1, 384, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 384, 1, 1024):
                with T.block("placeholder_red"):
                    ax0, ax1, ax2, k2 = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, k2])
                    T.writes(placeholder_red[ax0, ax1, ax2])
                    with T.init():
                        placeholder_red[ax0, ax1, ax2] = T.float32(0)
                    placeholder_red[ax0, ax1, ax2] = placeholder_red[ax0, ax1, ax2] + placeholder[ax0, ax1, k2]
            for i0, i1, i2 in T.grid(1, 384, 1):
                with T.block("T_divide"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder_red[ax0, ax1, ax2])
                    T.writes(T_divide[ax0, ax1, ax2])
                    T_divide[ax0, ax1, ax2] = placeholder_red[ax0, ax1, ax2] * T.float32(0.0009765625)
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #5: "fused_less_add_where_take_add_less_add_where_take_add"
[13:35:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384), "int64"], placeholder_1: T.Buffer[(30522, 1024), "float32"], placeholder_2: T.Buffer[(1, 384, 1024), "float32"], placeholder_3: T.Buffer[(1, 384), "int64"], placeholder_4: T.Buffer[(2, 1024), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="int64")
        T_less = T.alloc_buffer([1, 384], dtype="bool")
        compile_engine_const_1 = T.alloc_buffer([], dtype="int64")
        T_add_1 = T.alloc_buffer([1, 384], dtype="int64")
        T_where = T.alloc_buffer([1, 384], dtype="int64")
        T_take = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add_2 = T.alloc_buffer([1, 384, 1024], dtype="float32")
        compile_engine_const_2 = T.alloc_buffer([], dtype="int64")
        T_less_1 = T.alloc_buffer([1, 384], dtype="bool")
        compile_engine_const_3 = T.alloc_buffer([], dtype="int64")
        T_add_3 = T.alloc_buffer([1, 384], dtype="int64")
        T_where_1 = T.alloc_buffer([1, 384], dtype="int64")
        T_take_1 = T.alloc_buffer([1, 384, 1024], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.int64(0)
        for i0, i1 in T.grid(1, 384):
            with T.block("T_less"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[ax0, ax1], compile_engine_const[()])
                T.writes(T_less[ax0, ax1])
                T_less[ax0, ax1] = placeholder[ax0, ax1] < compile_engine_const[()]
        with T.block("compile_engine_const_1"):
            T.reads()
            T.writes(compile_engine_const_1[()])
            compile_engine_const_1[()] = T.int64(30522)
        for i0, i1 in T.grid(1, 384):
            with T.block("T_add"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[ax0, ax1], compile_engine_const_1[()])
                T.writes(T_add_1[ax0, ax1])
                T_add_1[ax0, ax1] = placeholder[ax0, ax1] + compile_engine_const_1[()]
        for i0, i1 in T.grid(1, 384):
            with T.block("T_where"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_less[ax0, ax1], T_add_1[ax0, ax1], placeholder[ax0, ax1])
                T.writes(T_where[ax0, ax1])
                T_where[ax0, ax1] = T.Select(T.cast(T_less[ax0, ax1], "int32") != 0, T_add_1[ax0, ax1], placeholder[ax0, ax1])
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_take"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder_1[T.min(T.max(T.int64(0), T_where[ax0, ax1]), T.int64(30521)), ax2], T_where[ax0, ax1])
                T.writes(T_take[ax0, ax1, ax2])
                T_take[ax0, ax1, ax2] = placeholder_1[T.min(T.max(T.int64(0), T_where[ax0, ax1]), T.int64(30521)), ax2]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_take[ax0, ax1, ax2], placeholder_2[ax0, ax1, ax2])
                T.writes(T_add_2[ax0, ax1, ax2])
                T_add_2[ax0, ax1, ax2] = T_take[ax0, ax1, ax2] + placeholder_2[ax0, ax1, ax2]
        with T.block("compile_engine_const_2"):
            T.reads()
            T.writes(compile_engine_const_2[()])
            compile_engine_const_2[()] = T.int64(0)
        for i0, i1 in T.grid(1, 384):
            with T.block("T_less_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder_3[ax0, ax1], compile_engine_const_2[()])
                T.writes(T_less_1[ax0, ax1])
                T_less_1[ax0, ax1] = placeholder_3[ax0, ax1] < compile_engine_const_2[()]
        with T.block("compile_engine_const_3"):
            T.reads()
            T.writes(compile_engine_const_3[()])
            compile_engine_const_3[()] = T.int64(2)
        for i0, i1 in T.grid(1, 384):
            with T.block("T_add_2"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder_3[ax0, ax1], compile_engine_const_3[()])
                T.writes(T_add_3[ax0, ax1])
                T_add_3[ax0, ax1] = placeholder_3[ax0, ax1] + compile_engine_const_3[()]
        for i0, i1 in T.grid(1, 384):
            with T.block("T_where_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_less_1[ax0, ax1], T_add_3[ax0, ax1], placeholder_3[ax0, ax1])
                T.writes(T_where_1[ax0, ax1])
                T_where_1[ax0, ax1] = T.Select(T.cast(T_less_1[ax0, ax1], "int32") != 0, T_add_3[ax0, ax1], placeholder_3[ax0, ax1])
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_take_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder_4[T.min(T.max(T.int64(0), T_where_1[ax0, ax1]), T.int64(1)), ax2], T_where_1[ax0, ax1])
                T.writes(T_take_1[ax0, ax1, ax2])
                T_take_1[ax0, ax1, ax2] = placeholder_4[T.min(T.max(T.int64(0), T_where_1[ax0, ax1]), T.int64(1)), ax2]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add_3"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_2[ax0, ax1, ax2], T_take_1[ax0, ax1, ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_add_2[ax0, ax1, ax2] + T_take_1[ax0, ax1, ax2]
    

[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384), "int64"], placeholder_1: T.Buffer[(30522, 1024), "float32"], placeholder_2: T.Buffer[(1, 384, 1024), "float32"], placeholder_3: T.Buffer[(1, 384), "int64"], placeholder_4: T.Buffer[(2, 1024), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2 in T.grid(1, 384, 1024):
                with T.block("T_add_3"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[ax0, ax1], placeholder_3[ax0, ax1], placeholder_1[T.min(T.max(T.int64(0), placeholder[ax0, ax1]), T.int64(30521)) : T.min(T.max(T.int64(0), placeholder[ax0, ax1] + T.int64(30522)), T.int64(30521)) + T.int64(1), ax2], placeholder_2[ax0, ax1, ax2], placeholder_4[T.min(T.max(T.int64(0), placeholder_3[ax0, ax1]), T.int64(1)) : T.min(T.max(T.int64(0), placeholder_3[ax0, ax1] + T.int64(2)), T.int64(1)) + T.int64(1), ax2])
                    T.writes(T_add[ax0, ax1, ax2])
                    T_add[ax0, ax1, ax2] = placeholder_1[T.min(T.max(T.int64(0), T.Select(T.cast(placeholder[ax0, ax1] < T.int64(0), "int32") != 0, placeholder[ax0, ax1] + T.int64(30522), placeholder[ax0, ax1])), T.int64(30521)), ax2] + placeholder_2[ax0, ax1, ax2] + placeholder_4[T.min(T.max(T.int64(0), T.Select(T.cast(placeholder_3[ax0, ax1] < T.int64(0), "int32") != 0, placeholder_3[ax0, ax1] + T.int64(2), placeholder_3[ax0, ax1])), T.int64(1)), ax2]
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_less", func_name="main")
b2 = sch.get_block(name="compile_engine_const_1", func_name="main")
b3 = sch.get_block(name="T_add", func_name="main")
b4 = sch.get_block(name="T_where", func_name="main")
b5 = sch.get_block(name="T_take", func_name="main")
b6 = sch.get_block(name="T_add_1", func_name="main")
b7 = sch.get_block(name="compile_engine_const_2", func_name="main")
b8 = sch.get_block(name="T_less_1", func_name="main")
b9 = sch.get_block(name="compile_engine_const_3", func_name="main")
b10 = sch.get_block(name="T_add_2", func_name="main")
b11 = sch.get_block(name="T_where_1", func_name="main")
b12 = sch.get_block(name="T_take_1", func_name="main")
b13 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b12)
sch.compute_inline(block=b11)
sch.compute_inline(block=b10)
sch.compute_inline(block=b9)
sch.compute_inline(block=b8)
sch.compute_inline(block=b7)
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v14 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b13, ann_key="meta_schedule.unroll_explicit", ann_val=v14)
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #6: "fused_reshape_add_reshape_transpose_reshape"
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_reshape: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_reshape_2 = T.alloc_buffer([1, 384, 16, 64], dtype="float32")
        T_transpose = T.alloc_buffer([1, 16, 384, 64], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024])
                T.writes(T_reshape_1[ax0, ax1, ax2])
                T_reshape_1[ax0, ax1, ax2] = placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape_1[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_reshape_1[ax0, ax1, ax2] + placeholder_1[ax2]
        for i0, i1, i2, i3 in T.grid(1, 384, 16, 64):
            with T.block("T_reshape_1"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024])
                T.writes(T_reshape_2[ax0, ax1, ax2, ax3])
                T_reshape_2[ax0, ax1, ax2, ax3] = T_add[0, ((ax2 * 64 + ax3) // 1024 + ax1) % 384, (ax2 * 64 + ax3) % 1024]
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 64):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_reshape_2[ax0, ax2, ax1, ax3])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = T_reshape_2[ax0, ax2, ax1, ax3]
        for i0, i1, i2 in T.grid(16, 384, 64):
            with T.block("T_reshape_2"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_transpose[0, ((ax2 // 64 + ax1) // 384 + ax0) % 16, (ax2 // 64 + ax1) % 384, ax2 % 64])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = T_transpose[0, ((ax2 // 64 + ax1) // 384 + ax0) % 16, (ax2 // 64 + ax1) % 384, ax2 % 64]
    

[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_reshape: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2 in T.grid(16, 384, 64):
                with T.block("T_reshape_2"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[(ax2 // 64 + ax1) % 384, ((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64], placeholder_1[((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64])
                    T.writes(T_reshape[ax0, ax1, ax2])
                    T_reshape[ax0, ax1, ax2] = placeholder[((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 // 1024 + ((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) // 1024 + (ax2 // 64 + ax1) % 384) % 384) % 384, (((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 % 1024] + placeholder_1[(((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="T_transpose", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #7: "fused_nn_batch_matmul"
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], placeholder_1: T.Buffer[(16, 384, 64), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(16, 384, 384, 64):
            with T.block("T_batch_matmul_NT"):
                b, i, j, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                T.reads(placeholder[b, i, k], placeholder_1[b, j, k])
                T.writes(T_batch_matmul_NT[b, i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                with T.init():
                    T_batch_matmul_NT[b, i, j] = T.float32(0)
                T_batch_matmul_NT[b, i, j] = T_batch_matmul_NT[b, i, j] + placeholder[b, i, k] * placeholder_1[b, j, k]
    

[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], placeholder_1: T.Buffer[(16, 384, 64), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 384], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_fused in T.thread_binding(32, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_fused in T.thread_binding(256, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_fused in T.thread_binding(96, thread="threadIdx.x"):
                        for i3_0 in T.serial(16):
                            for ax0_ax1_ax2_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_fused // 192)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused // 4 * 48 + ax0_ax1_ax2_fused % 192 // 4)
                                    v2 = T.axis.spatial(64, i3_0 * 4 + ax0_ax1_ax2_fused % 4)
                                    T.reads(placeholder[v0, v1, v2])
                                    T.writes(placeholder_shared[v0, v1, v2])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                            for ax0_ax1_ax2_fused in T.serial(6144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, ax0_ax1_ax2_fused // 384)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 4 * 96 + ax0_ax1_ax2_fused % 384 // 4)
                                    v2 = T.axis.spatial(64, i3_0 * 4 + ax0_ax1_ax2_fused % 4)
                                    T.reads(placeholder_1[v0, v1, v2])
                                    T.writes(placeholder_shared_1[v0, v1, v2])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                            for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(1, 1, 1, 3, 4, 1, 1, 1):
                                with T.block("T_batch_matmul_NT"):
                                    b = T.axis.spatial(16, i0_1_i1_1_i2_1_fused // 32 * 2 + i0_2_i1_2_i2_2_fused // 48)
                                    i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused // 4 * 48 + i0_1_i1_1_i2_1_fused % 32 // 16 * 24 + i0_2_i1_2_i2_2_fused % 48 // 2)
                                    j = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 4 * 96 + i0_1_i1_1_i2_1_fused % 16 * 6 + i0_2_i1_2_i2_2_fused % 2 * 3 + i2_3)
                                    k = T.axis.reduce(64, i3_0 * 4 + i3_2)
                                    T.reads(placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                    T.writes(T_batch_matmul_NT_local[b, i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                                    with T.init():
                                        T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                                    T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                        for ax0, ax1, ax2 in T.grid(1, 1, 3):
                            with T.block("T_batch_matmul_NT_local"):
                                v0 = T.axis.spatial(16, i0_1_i1_1_i2_1_fused // 32 * 2 + i0_2_i1_2_i2_2_fused // 48 + ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused // 4 * 48 + i0_1_i1_1_i2_1_fused % 32 // 16 * 24 + i0_2_i1_2_i2_2_fused % 48 // 2 + ax1)
                                v2 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 4 * 96 + i0_1_i1_1_i2_1_fused % 16 * 6 + i0_2_i1_2_i2_2_fused % 2 * 3 + ax2)
                                T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                                T.writes(T_batch_matmul_NT[v0, v1, v2])
                                T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 8, 2, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[8, 2, 24, 1, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 16, 2, 3, 1])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[16, 1, 4])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #8: "fused_reshape_divide_add"
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 384), "float32"], T_add: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 16, 384, 384], dtype="float32")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_divide = T.alloc_buffer([1, 16, 384, 384], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[((ax3 // 384 + ax2) // 384 + ax1) % 16, (ax3 // 384 + ax2) % 384, ax3 % 384])
                T.writes(T_reshape[ax0, ax1, ax2, ax3])
                T_reshape[ax0, ax1, ax2, ax3] = placeholder[((ax3 // 384 + ax2) // 384 + ax1) % 16, (ax3 // 384 + ax2) % 384, ax3 % 384]
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(8)
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_divide"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_reshape[ax0, ax1, ax2, ax3], compile_engine_const[()])
                T.writes(T_divide[ax0, ax1, ax2, ax3])
                T_divide[ax0, ax1, ax2, ax3] = T_reshape[ax0, ax1, ax2, ax3] / compile_engine_const[()]
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_divide[ax0, ax1, ax2, ax3], placeholder_1[ax0, 0, 0, ax3])
                T.writes(T_add[ax0, ax1, ax2, ax3])
                T_add[ax0, ax1, ax2, ax3] = T_divide[ax0, ax1, ax2, ax3] + placeholder_1[ax0, 0, 0, ax3]
    

[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(1, 1, 1, 384), "float32"], T_add: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":16})
            for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_divide"):
                    ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                    T.reads(placeholder[((ax3 // 384 + ax2) // 384 + ax1) % 16, (ax3 // 384 + ax2) % 384, ax3 % 384], placeholder_1[ax0, 0, 0, ax3])
                    T.writes(T_add[ax0, ax1, ax2, ax3])
                    T_add[ax0, ax1, ax2, ax3] = placeholder[((ax3 // 384 + ax2) // 384 + ax1) % 16, (ax3 // 384 + ax2) % 384, ax3 % 384] / T.float32(8) + placeholder_1[ax0, 0, 0, ax3]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="compile_engine_const", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v4 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v4)
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #9: "fused_nn_softmax"
[13:35:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_softmax_norm: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_softmax_maxelem = T.alloc_buffer([1, 16, 384], dtype="float32")
        T_softmax_exp = T.alloc_buffer([1, 16, 384, 384], dtype="float32")
        T_softmax_expsum = T.alloc_buffer([1, 16, 384], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_softmax_maxelem"):
                i0_1, i1_1, i2_1, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                T.reads(placeholder[i0_1, i1_1, i2_1, k])
                T.writes(T_softmax_maxelem[i0_1, i1_1, i2_1])
                with T.init():
                    T_softmax_maxelem[i0_1, i1_1, i2_1] = T.float32(-3.4028234663852886e+38)
                T_softmax_maxelem[i0_1, i1_1, i2_1] = T.max(T_softmax_maxelem[i0_1, i1_1, i2_1], placeholder[i0_1, i1_1, i2_1, k])
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_softmax_exp"):
                i0_2, i1_2, i2_2, i3_1 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[i0_2, i1_2, i2_2, i3_1], T_softmax_maxelem[i0_2, i1_2, i2_2])
                T.writes(T_softmax_exp[i0_2, i1_2, i2_2, i3_1])
                T_softmax_exp[i0_2, i1_2, i2_2, i3_1] = T.exp(placeholder[i0_2, i1_2, i2_2, i3_1] - T_softmax_maxelem[i0_2, i1_2, i2_2], dtype="float32")
        for i0_3, i1_3, i2_3, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_softmax_expsum"):
                i0_4, i1_4, i2_4, k = T.axis.remap("SSSR", [i0_3, i1_3, i2_3, i3])
                T.reads(T_softmax_exp[i0_4, i1_4, i2_4, k])
                T.writes(T_softmax_expsum[i0_4, i1_4, i2_4])
                with T.init():
                    T_softmax_expsum[i0_4, i1_4, i2_4] = T.float32(0)
                T_softmax_expsum[i0_4, i1_4, i2_4] = T_softmax_expsum[i0_4, i1_4, i2_4] + T_softmax_exp[i0_4, i1_4, i2_4, k]
        for i0_5, i1_5, i2_5, i3 in T.grid(1, 16, 384, 384):
            with T.block("T_softmax_norm"):
                i0_6, i1_6, i2_6, i3_2 = T.axis.remap("SSSS", [i0_5, i1_5, i2_5, i3])
                T.reads(T_softmax_exp[i0_6, i1_6, i2_6, i3_2], T_softmax_expsum[i0_6, i1_6, i2_6])
                T.writes(T_softmax_norm[i0_6, i1_6, i2_6, i3_2])
                T.block_attr({"axis":3})
                T_softmax_norm[i0_6, i1_6, i2_6, i3_2] = T_softmax_exp[i0_6, i1_6, i2_6, i3_2] / T_softmax_expsum[i0_6, i1_6, i2_6]
    

[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 4 design space(s) generated
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_softmax_norm: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem_shared = T.alloc_buffer([1, 16, 384], dtype="float32", scope="shared")
            T_softmax_expsum_shared = T.alloc_buffer([1, 16, 384], dtype="float32", scope="shared")
            for i0, i1, i2 in T.grid(1, 16, 384):
                for ax0, ax1, ax2, ax3_0 in T.grid(1, 1, 1, 1):
                    for ax3_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("T_softmax_maxelem"):
                            i0_1 = T.axis.spatial(1, 0)
                            i1_1, i2_1 = T.axis.remap("SS", [i1, i2])
                            k = T.axis.reduce(384, ax3_1)
                            T.where(ax3_1 < 384)
                            T.reads(placeholder[i0_1, i1_1, i2_1, k])
                            T.writes(T_softmax_maxelem_shared[i0_1, i1_1, i2_1])
                            with T.init():
                                T_softmax_maxelem_shared[i0_1, i1_1, i2_1] = T.float32(-3.4028234663852886e+38)
                            T_softmax_maxelem_shared[i0_1, i1_1, i2_1] = T.max(T_softmax_maxelem_shared[i0_1, i1_1, i2_1], placeholder[i0_1, i1_1, i2_1, k])
                for ax0, ax1, ax2, ax3_0 in T.grid(1, 1, 1, 1):
                    for ax3_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            i1_2, i2_2 = T.axis.remap("SS", [i1, i2])
                            k = T.axis.reduce(384, ax3_1)
                            T.where(ax3_1 < 384)
                            T.reads(placeholder[i0_2, i1_2, i2_2, k], T_softmax_maxelem_shared[i0_2, i1_2, i2_2])
                            T.writes(T_softmax_expsum_shared[i0_2, i1_2, i2_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2, i1_2, i2_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2, i1_2, i2_2] = T_softmax_expsum_shared[i0_2, i1_2, i2_2] + T.exp(placeholder[i0_2, i1_2, i2_2, k] - T_softmax_maxelem_shared[i0_2, i1_2, i2_2], dtype="float32")
                for i3_0 in T.serial(1):
                    for i3_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1_3, i2_3 = T.axis.remap("SS", [i1, i2])
                            i3 = T.axis.spatial(384, i3_1)
                            T.where(i3_1 < 384)
                            T.reads(placeholder[i0_3, i1_3, i2_3, i3], T_softmax_maxelem_shared[i0_3, i1_3, i2_3], T_softmax_expsum_shared[i0_3, i1_3, i2_3])
                            T.writes(T_softmax_norm[i0_3, i1_3, i2_3, i3])
                            T.block_attr({"axis":3})
                            T_softmax_norm[i0_3, i1_3, i2_3, i3] = T.exp(placeholder[i0_3, i1_3, i2_3, i3] - T_softmax_maxelem_shared[i0_3, i1_3, i2_3], dtype="float32") / T_softmax_expsum_shared[i0_3, i1_3, i2_3]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="T_softmax_expsum", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
b4, = sch.get_consumers(block=b2)
l5, l6, l7, l8 = sch.get_loops(block=b4)
v9 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l10, l11 = sch.split(loop=l8, factors=[None, v9])
sch.bind(loop=l11, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l7, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l12, l13, l14, l15, l16, l17, l18 = sch.get_loops(block=b2)
l19, l20 = sch.split(loop=l18, factors=[None, v9])
sch.bind(loop=l20, thread_axis="threadIdx.x")
b21, b22 = sch.get_consumers(block=b0)
l23, l24, l25, l26, l27, l28, l29, l30 = sch.get_loops(block=b21)
sch.compute_at(block=b0, loop=l25, preserve_unit_loops=True)
sch.set_scope(block=b0, buffer_index=0, storage_scope="shared")
l31, l32, l33, l34, l35, l36, l37 = sch.get_loops(block=b0)
l38, l39 = sch.split(loop=l37, factors=[None, v9])
sch.bind(loop=l39, thread_axis="threadIdx.x")
v40 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v40)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_softmax_norm: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem = T.alloc_buffer([1, 16, 384], dtype="float32")
            T_softmax_expsum_shared = T.alloc_buffer([1, 16, 384], dtype="float32", scope="shared")
            for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_maxelem"):
                    i0_1, i1_1, i2_1, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_1, i1_1, i2_1, k])
                    T.writes(T_softmax_maxelem[i0_1, i1_1, i2_1])
                    with T.init():
                        T_softmax_maxelem[i0_1, i1_1, i2_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1, i1_1, i2_1] = T.max(T_softmax_maxelem[i0_1, i1_1, i2_1], placeholder[i0_1, i1_1, i2_1, k])
            for i0, i1, i2 in T.grid(1, 16, 384):
                for ax0, ax1, ax2, ax3_0 in T.grid(1, 1, 1, 1):
                    for ax3_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("T_softmax_expsum"):
                            i0_2 = T.axis.spatial(1, 0)
                            i1_2, i2_2 = T.axis.remap("SS", [i1, i2])
                            k = T.axis.reduce(384, ax3_1)
                            T.where(ax3_1 < 384)
                            T.reads(placeholder[i0_2, i1_2, i2_2, k], T_softmax_maxelem[i0_2, i1_2, i2_2])
                            T.writes(T_softmax_expsum_shared[i0_2, i1_2, i2_2])
                            with T.init():
                                T_softmax_expsum_shared[i0_2, i1_2, i2_2] = T.float32(0)
                            T_softmax_expsum_shared[i0_2, i1_2, i2_2] = T_softmax_expsum_shared[i0_2, i1_2, i2_2] + T.exp(placeholder[i0_2, i1_2, i2_2, k] - T_softmax_maxelem[i0_2, i1_2, i2_2], dtype="float32")
                for i3_0 in T.serial(1):
                    for i3_1 in T.thread_binding(512, thread="threadIdx.x"):
                        with T.block("T_softmax_norm"):
                            i0_3 = T.axis.spatial(1, 0)
                            i1_3, i2_3 = T.axis.remap("SS", [i1, i2])
                            i3 = T.axis.spatial(384, i3_1)
                            T.where(i3_1 < 384)
                            T.reads(placeholder[i0_3, i1_3, i2_3, i3], T_softmax_maxelem[i0_3, i1_3, i2_3], T_softmax_expsum_shared[i0_3, i1_3, i2_3])
                            T.writes(T_softmax_norm[i0_3, i1_3, i2_3, i3])
                            T.block_attr({"axis":3})
                            T_softmax_norm[i0_3, i1_3, i2_3, i3] = T.exp(placeholder[i0_3, i1_3, i2_3, i3] - T_softmax_maxelem[i0_3, i1_3, i2_3], dtype="float32") / T_softmax_expsum_shared[i0_3, i1_3, i2_3]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="T_softmax_expsum", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
b3, = sch.get_consumers(block=b1)
l4, l5, l6, l7 = sch.get_loops(block=b3)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l9, l10 = sch.split(loop=l7, factors=[None, v8])
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b1, loop=l6, preserve_unit_loops=True)
sch.set_scope(block=b1, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b1)
l18, l19 = sch.split(loop=l17, factors=[None, v8])
sch.bind(loop=l19, thread_axis="threadIdx.x")
v20 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v20)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_softmax_norm: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem = T.alloc_buffer([1, 16, 384], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1, 16, 384], dtype="float32")
            for i0, i1, i2, i3_0 in T.grid(1, 16, 384, 1):
                for i3_1 in T.thread_binding(512, thread="threadIdx.x"):
                    with T.block("T_softmax_maxelem"):
                        i0_1 = T.axis.spatial(1, 0)
                        i1_1, i2_1 = T.axis.remap("SS", [i1, i2])
                        k = T.axis.reduce(384, i3_1)
                        T.where(i3_1 < 384)
                        T.reads(placeholder[i0_1, i1_1, i2_1, k])
                        T.writes(T_softmax_maxelem[i0_1, i1_1, i2_1])
                        with T.init():
                            T_softmax_maxelem[i0_1, i1_1, i2_1] = T.float32(-3.4028234663852886e+38)
                        T_softmax_maxelem[i0_1, i1_1, i2_1] = T.max(T_softmax_maxelem[i0_1, i1_1, i2_1], placeholder[i0_1, i1_1, i2_1, k])
            for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_expsum"):
                    i0_2, i1_2, i2_2, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_2, i1_2, i2_2, k], T_softmax_maxelem[i0_2, i1_2, i2_2])
                    T.writes(T_softmax_expsum[i0_2, i1_2, i2_2])
                    with T.init():
                        T_softmax_expsum[i0_2, i1_2, i2_2] = T.float32(0)
                    T_softmax_expsum[i0_2, i1_2, i2_2] = T_softmax_expsum[i0_2, i1_2, i2_2] + T.exp(placeholder[i0_2, i1_2, i2_2, k] - T_softmax_maxelem[i0_2, i1_2, i2_2], dtype="float32")
            for i0_3, i1_3, i2_3, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_norm"):
                    i0_4, i1_4, i2_4, i3_2 = T.axis.remap("SSSS", [i0_3, i1_3, i2_3, i3])
                    T.reads(placeholder[i0_4, i1_4, i2_4, i3_2], T_softmax_maxelem[i0_4, i1_4, i2_4], T_softmax_expsum[i0_4, i1_4, i2_4])
                    T.writes(T_softmax_norm[i0_4, i1_4, i2_4, i3_2])
                    T.block_attr({"axis":3})
                    T_softmax_norm[i0_4, i1_4, i2_4, i3_2] = T.exp(placeholder[i0_4, i1_4, i2_4, i3_2] - T_softmax_maxelem[i0_4, i1_4, i2_4], dtype="float32") / T_softmax_expsum[i0_4, i1_4, i2_4]
    

b0 = sch.get_block(name="T_softmax_maxelem", func_name="main")
b1 = sch.get_block(name="T_softmax_exp", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
v3 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=7)
l4, l5, l6, l7 = sch.get_loops(block=b0)
l8, l9 = sch.split(loop=l7, factors=[None, v3])
sch.bind(loop=l9, thread_axis="threadIdx.x")
v10 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v10)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #3:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_softmax_norm: T.Buffer[(1, 16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            T_softmax_maxelem = T.alloc_buffer([1, 16, 384], dtype="float32")
            T_softmax_expsum = T.alloc_buffer([1, 16, 384], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_maxelem"):
                    i0_1, i1_1, i2_1, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_1, i1_1, i2_1, k])
                    T.writes(T_softmax_maxelem[i0_1, i1_1, i2_1])
                    with T.init():
                        T_softmax_maxelem[i0_1, i1_1, i2_1] = T.float32(-3.4028234663852886e+38)
                    T_softmax_maxelem[i0_1, i1_1, i2_1] = T.max(T_softmax_maxelem[i0_1, i1_1, i2_1], placeholder[i0_1, i1_1, i2_1, k])
            for i0, i1, i2, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_expsum"):
                    i0_2, i1_2, i2_2, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[i0_2, i1_2, i2_2, k], T_softmax_maxelem[i0_2, i1_2, i2_2])
                    T.writes(T_softmax_expsum[i0_2, i1_2, i2_2])
                    with T.init():
                        T_softmax_expsum[i0_2, i1_2, i2_2] = T.float32(0)
                    T_softmax_expsum[i0_2, i1_2, i2_2] = T_softmax_expsum[i0_2, i1_2, i2_2] + T.exp(placeholder[i0_2, i1_2, i2_2, k] - T_softmax_maxelem[i0_2, i1_2, i2_2], dtype="float32")
            for i0_3, i1_3, i2_3, i3 in T.grid(1, 16, 384, 384):
                with T.block("T_softmax_norm"):
                    i0_4, i1_4, i2_4, i3_1 = T.axis.remap("SSSS", [i0_3, i1_3, i2_3, i3])
                    T.reads(placeholder[i0_4, i1_4, i2_4, i3_1], T_softmax_maxelem[i0_4, i1_4, i2_4], T_softmax_expsum[i0_4, i1_4, i2_4])
                    T.writes(T_softmax_norm[i0_4, i1_4, i2_4, i3_1])
                    T.block_attr({"axis":3})
                    T_softmax_norm[i0_4, i1_4, i2_4, i3_1] = T.exp(placeholder[i0_4, i1_4, i2_4, i3_1] - T_softmax_maxelem[i0_4, i1_4, i2_4], dtype="float32") / T_softmax_expsum[i0_4, i1_4, i2_4]
    

b0 = sch.get_block(name="T_softmax_exp", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b0)
v2 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v2)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #10: "fused_reshape"
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_reshape: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(16, 384, 384):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[0, ((ax2 // 384 + ax1) // 384 + ax0) % 16, (ax2 // 384 + ax1) % 384, ax2 % 384])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = placeholder[0, ((ax2 // 384 + ax1) // 384 + ax0) % 16, (ax2 // 384 + ax1) % 384, ax2 % 384]
    

[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 16, 384, 384), "float32"], T_reshape: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2 in T.grid(16, 384, 384):
                with T.block("T_reshape"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[0, ((ax2 // 384 + ax1) // 384 + ax0) % 16, (ax2 // 384 + ax1) % 384, ax2 % 384])
                    T.writes(T_reshape[ax0, ax1, ax2])
                    T_reshape[ax0, ax1, ax2] = placeholder[0, ((ax2 // 384 + ax1) // 384 + ax0) % 16, (ax2 // 384 + ax1) % 384, ax2 % 384]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #11: "fused_nn_batch_matmul_1"
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(16, 64, 384), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2, i3 in T.grid(16, 384, 64, 384):
            with T.block("T_batch_matmul_NT"):
                b, i, j, k = T.axis.remap("SSSR", [i0, i1, i2, i3])
                T.reads(placeholder[b, i, k], placeholder_1[b, j, k])
                T.writes(T_batch_matmul_NT[b, i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                with T.init():
                    T_batch_matmul_NT[b, i, j] = T.float32(0)
                T_batch_matmul_NT[b, i, j] = T_batch_matmul_NT[b, i, j] + placeholder[b, i, k] * placeholder_1[b, j, k]
    

[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(16, 64, 384), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 64], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([16, 384, 384], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([16, 64, 384], dtype="float32", scope="shared")
            for i0_0_i1_0_i2_0_fused in T.thread_binding(24, thread="blockIdx.x"):
                for i0_1_i1_1_i2_1_fused in T.thread_binding(8, thread="vthread.x"):
                    for i0_2_i1_2_i2_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                        for i3_0 in T.serial(128):
                            for ax0_ax1_ax2_fused in T.serial(3072):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 12 * 8 + ax0_ax1_ax2_fused // 384)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 12 // 4 * 128 + ax0_ax1_ax2_fused % 384 // 3)
                                    v2 = T.axis.spatial(384, i3_0 * 3 + ax0_ax1_ax2_fused % 3)
                                    T.reads(placeholder[v0, v1, v2])
                                    T.writes(placeholder_shared[v0, v1, v2])
                                    T.block_attr({"meta_schedule.cooperative_fetch":3})
                                    placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                            for ax0_ax1_ax2_fused in T.serial(384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 12 * 8 + ax0_ax1_ax2_fused // 48)
                                    v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused % 4 * 16 + ax0_ax1_ax2_fused % 48 // 3)
                                    v2 = T.axis.spatial(384, i3_0 * 3 + ax0_ax1_ax2_fused % 3)
                                    T.reads(placeholder_1[v0, v1, v2])
                                    T.writes(placeholder_shared_1[v0, v1, v2])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                            for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(1, 4, 2, 1, 3, 1, 1, 4):
                                with T.block("T_batch_matmul_NT"):
                                    b = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 12 * 8 + i0_1_i1_1_i2_1_fused // 4 * 4 + i0_3)
                                    i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 12 // 4 * 128 + i0_2_i1_2_i2_2_fused * 2 + i1_3)
                                    j = T.axis.spatial(64, i0_0_i1_0_i2_0_fused % 4 * 16 + i0_1_i1_1_i2_1_fused % 4 * 4 + i2_4)
                                    k = T.axis.reduce(384, i3_0 * 3 + i3_2)
                                    T.reads(placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                    T.writes(T_batch_matmul_NT_local[b, i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                                    with T.init():
                                        T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                                    T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                        for ax0, ax1, ax2 in T.grid(4, 2, 4):
                            with T.block("T_batch_matmul_NT_local"):
                                v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 12 * 8 + i0_1_i1_1_i2_1_fused // 4 * 4 + ax0)
                                v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 12 // 4 * 128 + i0_2_i1_2_i2_2_fused * 2 + ax1)
                                v2 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused % 4 * 16 + i0_1_i1_1_i2_1_fused % 4 * 4 + ax2)
                                T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                                T.writes(T_batch_matmul_NT[v0, v1, v2])
                                T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[2, 2, 1, 4, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[3, 1, 64, 2, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 4, 1, 1, 4])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[128, 1, 3])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #12: "fused_reshape_transpose_reshape"
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 16, 384, 64], dtype="float32")
        T_transpose = T.alloc_buffer([1, 384, 16, 64], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 16, 384, 64):
            with T.block("T_reshape"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(placeholder[((ax3 // 64 + ax2) // 384 + ax1) % 16, (ax3 // 64 + ax2) % 384, ax3 % 64])
                T.writes(T_reshape_1[ax0, ax1, ax2, ax3])
                T_reshape_1[ax0, ax1, ax2, ax3] = placeholder[((ax3 // 64 + ax2) // 384 + ax1) % 16, (ax3 // 64 + ax2) % 384, ax3 % 64]
        for i0, i1, i2, i3 in T.grid(1, 384, 16, 64):
            with T.block("T_transpose"):
                ax0, ax1, ax2, ax3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(T_reshape_1[ax0, ax2, ax1, ax3])
                T.writes(T_transpose[ax0, ax1, ax2, ax3])
                T_transpose[ax0, ax1, ax2, ax3] = T_reshape_1[ax0, ax2, ax1, ax3]
        for i0, i1 in T.grid(384, 1024):
            with T.block("T_reshape_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_transpose[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024 // 64, ax1 % 64])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_transpose[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024 // 64, ax1 % 64]
    

[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            for i0, i1 in T.grid(384, 1024):
                with T.block("T_reshape_1"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[ax1 % 1024 // 64, (ax1 // 1024 + ax0) % 384, ax1 % 64])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[((ax1 % 64 // 64 + (ax1 // 1024 + ax0) % 384) // 384 + ax1 % 1024 // 64) % 16, (ax1 % 64 // 64 + (ax1 // 1024 + ax0) % 384) % 384, ax1 % 64 % 64]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_transpose", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #13: "fused_nn_dense"
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(384, 1024, 1024):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
    

[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([1024, 1024], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(128, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(6, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(8, thread="threadIdx.x"):
                        for i2_0 in T.serial(16):
                            for ax0_ax1_fused in T.serial(6144):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_fused // 32 * 96 + ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(1024, i2_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(2048):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_fused % 32 * 32 + ax0_ax1_fused // 64)
                                    v1 = T.axis.spatial(1024, i2_0 * 64 + ax0_ax1_fused % 64)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(64, 1, 1, 1, 2, 32):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(384, i0_0_i1_0_fused // 32 * 96 + i0_1_i1_1_fused * 16 + i0_2_i1_2_fused * 2 + i0_4)
                                    j = T.axis.spatial(1024, i0_0_i1_0_fused % 32 * 32 + i1_4)
                                    k = T.axis.reduce(1024, i2_0 * 64 + i2_1)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(2, 32):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(384, i0_0_i1_0_fused // 32 * 96 + i0_1_i1_1_fused * 16 + i0_2_i1_2_fused * 2 + ax0)
                                v1 = T.axis.spatial(1024, i0_0_i1_0_fused % 32 * 32 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1])
                                T.writes(T_matmul_NT[v0, v1])
                                T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[4, 6, 8, 1, 2])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[32, 1, 1, 1, 32])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[16, 64, 1])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #14: "fused_add_sqrt_divide_multiply_add"
[13:35:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1024), "float32"], placeholder_2: T.Buffer[(1024,), "float32"], placeholder_3: T.Buffer[(1024,), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 384, 1], dtype="float32")
        T_sqrt = T.alloc_buffer([1, 384, 1], dtype="float32")
        T_divide = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_multiply = T.alloc_buffer([1, 384, 1024], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(9.999999960041972e-13)
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax1, ax2], compile_engine_const[()])
                T.writes(T_add_1[ax0, ax1, ax2])
                T_add_1[ax0, ax1, ax2] = placeholder[ax0, ax1, ax2] + compile_engine_const[()]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_sqrt"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_1[ax0, ax1, ax2])
                T.writes(T_sqrt[ax0, ax1, ax2])
                T_sqrt[ax0, ax1, ax2] = T.sqrt(T_add_1[ax0, ax1, ax2], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_divide"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder_1[ax0, ax1, ax2], T_sqrt[ax0, ax1, 0])
                T.writes(T_divide[ax0, ax1, ax2])
                T_divide[ax0, ax1, ax2] = placeholder_1[ax0, ax1, ax2] / T_sqrt[ax0, ax1, 0]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_multiply"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[ax0, ax1, ax2], placeholder_2[ax2])
                T.writes(T_multiply[ax0, ax1, ax2])
                T_multiply[ax0, ax1, ax2] = T_divide[ax0, ax1, ax2] * placeholder_2[ax2]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[ax0, ax1, ax2], placeholder_3[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_multiply[ax0, ax1, ax2] + placeholder_3[ax2]
    

[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1024), "float32"], placeholder_2: T.Buffer[(1024,), "float32"], placeholder_3: T.Buffer[(1024,), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":512})
            for i0, i1, i2 in T.grid(1, 384, 1024):
                with T.block("T_divide"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder_1[ax0, ax1, ax2], placeholder[ax0, ax1, 0], placeholder_2[ax2], placeholder_3[ax2])
                    T.writes(T_add[ax0, ax1, ax2])
                    T_add[ax0, ax1, ax2] = placeholder_1[ax0, ax1, ax2] / T.sqrt(placeholder[ax0, ax1, 0] + T.float32(9.999999960041972e-13), dtype="float32") * placeholder_2[ax2] + placeholder_3[ax2]
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_sqrt", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="T_add_1", func_name="main")
b5 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b4)
sch.reverse_compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v6 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b5, ann_key="meta_schedule.unroll_explicit", ann_val=v6)
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #15: "fused_reshape_1"
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(384, 1024):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = placeholder[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024]
    

[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1 in T.grid(384, 1024):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #16: "fused_nn_dense_1"
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(4096, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(384, 4096, 1024):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [4096, 1024], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
    

[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(4096, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_matmul_NT_local = T.alloc_buffer([384, 4096], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([4096, 1024], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(32, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(256, thread="threadIdx.x"):
                        for i2_0 in T.serial(1):
                            for ax0_ax1_fused in T.serial(65536):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_fused * 64 + ax0_ax1_fused // 1024)
                                    v1 = T.axis.spatial(1024, ax0_ax1_fused % 1024)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(4194304):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(4096, ax0_ax1_fused // 1024)
                                    v1 = T.axis.spatial(1024, ax0_ax1_fused % 1024)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(16, 2, 16, 64, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(384, i0_0_i1_0_fused * 64 + i0_1_i1_1_fused // 4 * 8 + i0_2_i1_2_fused // 64 * 2 + i0_3)
                                    j = T.axis.spatial(4096, i0_1_i1_1_fused % 4 * 1024 + i0_2_i1_2_fused % 64 * 16 + i1_3)
                                    k = T.axis.reduce(1024, i2_1 * 64 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [4096, 1024], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(2, 16):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(384, i0_0_i1_0_fused * 64 + i0_1_i1_1_fused // 4 * 8 + i0_2_i1_2_fused // 64 * 2 + ax0)
                                v1 = T.axis.spatial(4096, i0_1_i1_1_fused % 4 * 1024 + i0_2_i1_2_fused % 64 * 16 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1])
                                T.writes(T_matmul_NT[v0, v1])
                                T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[6, 8, 4, 2, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 4, 64, 16, 1])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[1, 16, 64])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"
[13:35:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(4096,), "float32"], T_reshape: T.Buffer[(384, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape_1 = T.alloc_buffer([1, 384, 4096], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 4096], dtype="float32")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_multiply = T.alloc_buffer([1, 384, 4096], dtype="float32")
        compile_engine_const_1 = T.alloc_buffer([], dtype="float32")
        T_divide = T.alloc_buffer([1, 384, 4096], dtype="float32")
        T_erf = T.alloc_buffer([1, 384, 4096], dtype="float32")
        compile_engine_const_2 = T.alloc_buffer([], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 384, 4096], dtype="float32")
        T_multiply_1 = T.alloc_buffer([1, 384, 4096], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 4096 + ax1) % 384, ax2 % 4096])
                T.writes(T_reshape_1[ax0, ax1, ax2])
                T_reshape_1[ax0, ax1, ax2] = placeholder[(ax2 // 4096 + ax1) % 384, ax2 % 4096]
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape_1[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_reshape_1[ax0, ax1, ax2] + placeholder_1[ax2]
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(0.5)
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_multiply"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add[ax0, ax1, ax2], compile_engine_const[()])
                T.writes(T_multiply[ax0, ax1, ax2])
                T_multiply[ax0, ax1, ax2] = T_add[ax0, ax1, ax2] * compile_engine_const[()]
        with T.block("compile_engine_const_1"):
            T.reads()
            T.writes(compile_engine_const_1[()])
            compile_engine_const_1[()] = T.float32(1.4142135381698608)
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_divide"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add[ax0, ax1, ax2], compile_engine_const_1[()])
                T.writes(T_divide[ax0, ax1, ax2])
                T_divide[ax0, ax1, ax2] = T_add[ax0, ax1, ax2] / compile_engine_const_1[()]
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_erf"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[ax0, ax1, ax2])
                T.writes(T_erf[ax0, ax1, ax2])
                T_erf[ax0, ax1, ax2] = T.erf(T_divide[ax0, ax1, ax2], dtype="float32")
        with T.block("compile_engine_const_2"):
            T.reads()
            T.writes(compile_engine_const_2[()])
            compile_engine_const_2[()] = T.float32(1)
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_add_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_erf[ax0, ax1, ax2], compile_engine_const_2[()])
                T.writes(T_add_1[ax0, ax1, ax2])
                T_add_1[ax0, ax1, ax2] = T_erf[ax0, ax1, ax2] + compile_engine_const_2[()]
        for i0, i1, i2 in T.grid(1, 384, 4096):
            with T.block("T_multiply_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[ax0, ax1, ax2], T_add_1[ax0, ax1, ax2])
                T.writes(T_multiply_1[ax0, ax1, ax2])
                T_multiply_1[ax0, ax1, ax2] = T_multiply[ax0, ax1, ax2] * T_add_1[ax0, ax1, ax2]
        for i0, i1 in T.grid(384, 4096):
            with T.block("T_reshape_1"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_multiply_1[0, (ax1 // 4096 + ax0) % 384, ax1 % 4096])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_multiply_1[0, (ax1 // 4096 + ax0) % 384, ax1 % 4096]
    

[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(4096,), "float32"], T_reshape: T.Buffer[(384, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1 in T.grid(384, 4096):
                with T.block("T_reshape_1"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[(ax1 // 4096 + ax0) % 384, ax1 % 4096], placeholder_1[ax1 % 4096])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = (placeholder[(ax1 % 4096 // 4096 + (ax1 // 4096 + ax0) % 384) % 384, ax1 % 4096 % 4096] + placeholder_1[ax1 % 4096]) * T.float32(0.5) * (T.erf((placeholder[(ax1 % 4096 // 4096 + (ax1 // 4096 + ax0) % 384) % 384, ax1 % 4096 % 4096] + placeholder_1[ax1 % 4096]) / T.float32(1.4142135381698608), dtype="float32") + T.float32(1))
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="compile_engine_const", func_name="main")
b3 = sch.get_block(name="T_multiply", func_name="main")
b4 = sch.get_block(name="compile_engine_const_1", func_name="main")
b5 = sch.get_block(name="T_divide", func_name="main")
b6 = sch.get_block(name="T_erf", func_name="main")
b7 = sch.get_block(name="compile_engine_const_2", func_name="main")
b8 = sch.get_block(name="T_add_1", func_name="main")
b9 = sch.get_block(name="T_multiply_1", func_name="main")
b10 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b9)
sch.compute_inline(block=b8)
sch.compute_inline(block=b7)
sch.compute_inline(block=b6)
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v11 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b10, ann_key="meta_schedule.unroll_explicit", ann_val=v11)
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #18: "fused_nn_dense_2"
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(1024, 4096), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(384, 1024, 4096):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
    

[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(1024, 4096), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":64})
            T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([384, 4096], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([1024, 4096], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(1, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(16, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                        for i2_0 in T.serial(128):
                            for ax0_ax1_fused in T.serial(12288):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, ax0_ax1_fused // 32)
                                    v1 = T.axis.spatial(4096, i2_0 * 32 + ax0_ax1_fused % 32)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":2})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(32768):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, ax0_ax1_fused // 32)
                                    v1 = T.axis.spatial(4096, i2_0 * 32 + ax0_ax1_fused % 32)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":1})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(16, 128, 1, 2, 3, 2):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(384, i0_3 * 3 + i0_4)
                                    j = T.axis.spatial(1024, i0_1_i1_1_fused * 64 + i0_2_i1_2_fused * 2 + i1_4)
                                    k = T.axis.reduce(4096, i2_0 * 32 + i2_1 * 2 + i2_2)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(384, 2):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(384, ax0)
                                v1 = T.axis.spatial(1024, i0_1_i1_1_fused * 64 + i0_2_i1_2_fused * 2 + ax1)
                                T.reads(T_matmul_NT_local[v0, v1])
                                T.writes(T_matmul_NT[v0, v1])
                                T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 1, 1, 128, 3])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 16, 32, 1, 2])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[128, 16, 2])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #19: "fused_reshape_add_add"
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], placeholder_2: T.Buffer[(1, 384, 1024), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 384, 1024], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add_1[ax0, ax1, ax2])
                T_add_1[ax0, ax1, ax2] = T_reshape[ax0, ax1, ax2] + placeholder_1[ax2]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add_1[ax0, ax1, ax2], placeholder_2[ax0, ax1, ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_add_1[ax0, ax1, ax2] + placeholder_2[ax0, ax1, ax2]
    

[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], placeholder_2: T.Buffer[(1, 384, 1024), "float32"], T_add: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1, i2 in T.grid(1, 384, 1024):
                with T.block("T_reshape"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024], placeholder_1[ax2], placeholder_2[ax0, ax1, ax2])
                    T.writes(T_add[ax0, ax1, ax2])
                    T_add[ax0, ax1, ax2] = placeholder[(ax2 // 1024 + ax1) % 384, ax2 % 1024] + placeholder_1[ax2] + placeholder_2[ax0, ax1, ax2]
    

b0 = sch.get_block(name="T_add", func_name="main")
b1 = sch.get_block(name="T_add_1", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.reverse_compute_inline(block=b1)
sch.reverse_compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #20: "fused_subtract"
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_subtract: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_subtract"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax1, ax2], placeholder_1[ax0, ax1, 0])
                T.writes(T_subtract[ax0, ax1, ax2])
                T_subtract[ax0, ax1, ax2] = placeholder[ax0, ax1, ax2] - placeholder_1[ax0, ax1, 0]
    

[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_subtract: T.Buffer[(1, 384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2 in T.grid(1, 384, 1024):
                with T.block("T_subtract"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[ax0, ax1, ax2], placeholder_1[ax0, ax1, 0])
                    T.writes(T_subtract[ax0, ax1, ax2])
                    T_subtract[ax0, ax1, ax2] = placeholder[ax0, ax1, ax2] - placeholder_1[ax0, ax1, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #21: "fused_power_mean"
[13:35:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_power = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_power_red = T.alloc_buffer([1, 384, 1], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(2)
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_power"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax1, ax2], compile_engine_const[()])
                T.writes(T_power[ax0, ax1, ax2])
                T_power[ax0, ax1, ax2] = T.pow(placeholder[ax0, ax1, ax2], compile_engine_const[()], dtype="float32")
        for i0, i1, i2, i3 in T.grid(1, 384, 1, 1024):
            with T.block("T_power_red"):
                ax0, ax1, ax2, k2 = T.axis.remap("SSSR", [i0, i1, i2, i3])
                T.reads(T_power[ax0, ax1, k2])
                T.writes(T_power_red[ax0, ax1, ax2])
                with T.init():
                    T_power_red[ax0, ax1, ax2] = T.float32(0)
                T_power_red[ax0, ax1, ax2] = T_power_red[ax0, ax1, ax2] + T_power[ax0, ax1, k2]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_divide"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_power_red[ax0, ax1, ax2])
                T.writes(T_divide[ax0, ax1, ax2])
                T_divide[ax0, ax1, ax2] = T_power_red[ax0, ax1, ax2] * T.float32(0.0009765625)
    

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 2 design space(s) generated
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            T_power_red_shared = T.alloc_buffer([1, 384, 1], dtype="float32", scope="shared")
            for i0, i1, i2_0 in T.grid(1, 384, 1):
                for ax0, ax1, ax2, ax3_0 in T.grid(1, 1, 1, 4):
                    for ax3_1 in T.thread_binding(256, thread="threadIdx.x"):
                        with T.block("T_power_red"):
                            ax0_1 = T.axis.spatial(1, 0)
                            ax1_1 = T.axis.spatial(384, i1)
                            ax2_1 = T.axis.spatial(1, 0)
                            k2 = T.axis.reduce(1024, ax3_0 * 256 + ax3_1)
                            T.reads(placeholder[ax0_1, ax1_1, k2])
                            T.writes(T_power_red_shared[ax0_1, ax1_1, ax2_1])
                            with T.init():
                                T_power_red_shared[ax0_1, ax1_1, ax2_1] = T.float32(0)
                            T_power_red_shared[ax0_1, ax1_1, ax2_1] = T_power_red_shared[ax0_1, ax1_1, ax2_1] + T.pow(placeholder[ax0_1, ax1_1, k2], T.float32(2), dtype="float32")
                for i2_1 in T.thread_binding(256, thread="threadIdx.x"):
                    with T.block("T_divide"):
                        ax0 = T.axis.spatial(1, 0)
                        ax1 = T.axis.spatial(384, i1)
                        ax2 = T.axis.spatial(1, 0)
                        T.where(i2_1 < 1)
                        T.reads(T_power_red_shared[ax0, ax1, ax2])
                        T.writes(T_divide[ax0, ax1, ax2])
                        T_divide[ax0, ax1, ax2] = T_power_red_shared[ax0, ax1, ax2] * T.float32(0.0009765625)
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_power", func_name="main")
b2 = sch.get_block(name="T_power_red", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
b4, = sch.get_consumers(block=b2)
l5, l6, l7 = sch.get_loops(block=b4)
v8 = sch.sample_categorical(candidates=[4, 8, 16, 32, 64, 128, 256, 512], probs=[0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125], decision=6)
l9, l10 = sch.split(loop=l7, factors=[None, v8])
sch.bind(loop=l10, thread_axis="threadIdx.x")
sch.compute_at(block=b2, loop=l9, preserve_unit_loops=True)
sch.set_scope(block=b2, buffer_index=0, storage_scope="shared")
l11, l12, l13, l14, l15, l16, l17 = sch.get_loops(block=b2)
l18, l19 = sch.split(loop=l17, factors=[None, v8])
sch.bind(loop=l19, thread_axis="threadIdx.x")
v20 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v20)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1024), "float32"], T_divide: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_power_red = T.alloc_buffer([1, 384, 1], dtype="float32")
            for i0, i1, i2, i3 in T.grid(1, 384, 1, 1024):
                with T.block("T_power_red"):
                    ax0, ax1, ax2, k2 = T.axis.remap("SSSR", [i0, i1, i2, i3])
                    T.reads(placeholder[ax0, ax1, k2])
                    T.writes(T_power_red[ax0, ax1, ax2])
                    with T.init():
                        T_power_red[ax0, ax1, ax2] = T.float32(0)
                    T_power_red[ax0, ax1, ax2] = T_power_red[ax0, ax1, ax2] + T.pow(placeholder[ax0, ax1, k2], T.float32(2), dtype="float32")
            for i0, i1, i2 in T.grid(1, 384, 1):
                with T.block("T_divide"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(T_power_red[ax0, ax1, ax2])
                    T.writes(T_divide[ax0, ax1, ax2])
                    T_divide[ax0, ax1, ax2] = T_power_red[ax0, ax1, ax2] * T.float32(0.0009765625)
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_power", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #22: "fused_add_sqrt_divide_multiply_add_reshape"
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1024), "float32"], placeholder_2: T.Buffer[(1024,), "float32"], placeholder_3: T.Buffer[(1024,), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        compile_engine_const = T.alloc_buffer([], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 1], dtype="float32")
        T_sqrt = T.alloc_buffer([1, 384, 1], dtype="float32")
        T_divide = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_multiply = T.alloc_buffer([1, 384, 1024], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 384, 1024], dtype="float32")
        with T.block("compile_engine_const"):
            T.reads()
            T.writes(compile_engine_const[()])
            compile_engine_const[()] = T.float32(9.999999960041972e-13)
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[ax0, ax1, ax2], compile_engine_const[()])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = placeholder[ax0, ax1, ax2] + compile_engine_const[()]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_sqrt"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add[ax0, ax1, ax2])
                T.writes(T_sqrt[ax0, ax1, ax2])
                T_sqrt[ax0, ax1, ax2] = T.sqrt(T_add[ax0, ax1, ax2], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_divide"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder_1[ax0, ax1, ax2], T_sqrt[ax0, ax1, 0])
                T.writes(T_divide[ax0, ax1, ax2])
                T_divide[ax0, ax1, ax2] = placeholder_1[ax0, ax1, ax2] / T_sqrt[ax0, ax1, 0]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_multiply"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_divide[ax0, ax1, ax2], placeholder_2[ax2])
                T.writes(T_multiply[ax0, ax1, ax2])
                T_multiply[ax0, ax1, ax2] = T_divide[ax0, ax1, ax2] * placeholder_2[ax2]
        for i0, i1, i2 in T.grid(1, 384, 1024):
            with T.block("T_add_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_multiply[ax0, ax1, ax2], placeholder_3[ax2])
                T.writes(T_add_1[ax0, ax1, ax2])
                T_add_1[ax0, ax1, ax2] = T_multiply[ax0, ax1, ax2] + placeholder_3[ax2]
        for i0, i1 in T.grid(384, 1024):
            with T.block("T_reshape"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(T_add_1[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024])
                T.writes(T_reshape[ax0, ax1])
                T_reshape[ax0, ax1] = T_add_1[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024]
    

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1024), "float32"], placeholder_2: T.Buffer[(1024,), "float32"], placeholder_3: T.Buffer[(1024,), "float32"], T_reshape: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1 in T.grid(384, 1024):
                with T.block("T_reshape"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder_1[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024], placeholder[0, (ax1 // 1024 + ax0) % 384, 0], placeholder_2[ax1 % 1024], placeholder_3[ax1 % 1024])
                    T.writes(T_reshape[ax0, ax1])
                    T_reshape[ax0, ax1] = placeholder_1[0, (ax1 // 1024 + ax0) % 384, ax1 % 1024] / T.sqrt(placeholder[0, (ax1 // 1024 + ax0) % 384, 0] + T.float32(9.999999960041972e-13), dtype="float32") * placeholder_2[ax1 % 1024] + placeholder_3[ax1 % 1024]
    

b0 = sch.get_block(name="compile_engine_const", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_sqrt", func_name="main")
b3 = sch.get_block(name="T_divide", func_name="main")
b4 = sch.get_block(name="T_multiply", func_name="main")
b5 = sch.get_block(name="T_add_1", func_name="main")
b6 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b5)
sch.compute_inline(block=b4)
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v7 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b6, ann_key="meta_schedule.unroll_explicit", ann_val=v7)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #23: "fused_nn_dense_3"
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(2, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1, i2 in T.grid(384, 2, 1024):
            with T.block("T_matmul_NT"):
                i, j, k = T.axis.remap("SSR", [i0, i1, i2])
                T.reads(placeholder[i, k], placeholder_1[j, k])
                T.writes(T_matmul_NT[i, j])
                T.block_attr({"layout_free_placeholders":[placeholder_1], "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [2, 1024], "float32"], None, "float32"]})
                with T.init():
                    T_matmul_NT[i, j] = T.float32(0)
                T_matmul_NT[i, j] = T_matmul_NT[i, j] + placeholder[i, k] * placeholder_1[j, k]
    

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(2, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 2), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            T_matmul_NT_local = T.alloc_buffer([384, 2], dtype="float32", scope="local")
            placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
            placeholder_shared_1 = T.alloc_buffer([2, 1024], dtype="float32", scope="shared")
            for i0_0_i1_0_fused in T.thread_binding(6, thread="blockIdx.x"):
                for i0_1_i1_1_fused in T.thread_binding(2, thread="vthread.x"):
                    for i0_2_i1_2_fused in T.thread_binding(1, thread="threadIdx.x"):
                        for i2_0 in T.serial(4):
                            for ax0_ax1_fused in T.serial(16384):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_fused * 64 + ax0_ax1_fused // 256)
                                    v1 = T.axis.spatial(1024, i2_0 * 256 + ax0_ax1_fused % 256)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                            for ax0_ax1_fused in T.serial(512):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(2, ax0_ax1_fused // 256)
                                    v1 = T.axis.spatial(1024, i2_0 * 256 + ax0_ax1_fused % 256)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    T.block_attr({"meta_schedule.cooperative_fetch":4})
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                            for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(256, 64, 1, 1, 1, 1):
                                with T.block("T_matmul_NT"):
                                    i = T.axis.spatial(384, i0_0_i1_0_fused * 64 + i0_3)
                                    j = T.axis.spatial(2, i0_1_i1_1_fused)
                                    k = T.axis.reduce(1024, i2_0 * 256 + i2_1)
                                    T.reads(placeholder_shared[i, k], placeholder_shared_1[j, k])
                                    T.writes(T_matmul_NT_local[i, j])
                                    T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [2, 1024], "float32"], None, "float32"]})
                                    with T.init():
                                        T_matmul_NT_local[i, j] = T.float32(0)
                                    T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                        for ax0, ax1 in T.grid(64, 1):
                            with T.block("T_matmul_NT_local"):
                                v0 = T.axis.spatial(384, i0_0_i1_0_fused * 64 + ax0)
                                v1 = T.axis.spatial(2, i0_1_i1_1_fused + ax1)
                                T.reads(T_matmul_NT_local[v0, v1])
                                T.writes(T_matmul_NT[v0, v1])
                                T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[6, 1, 1, 64, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 2, 1, 1, 1])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[4, 256, 1])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #24: "fused_reshape_add_split"
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 2), "float32"], placeholder_1: T.Buffer[(2,), "float32"], T_split: T.Buffer[(1, 384, 1), "float32"], T_split_1: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        T_reshape = T.alloc_buffer([1, 384, 2], dtype="float32")
        T_add = T.alloc_buffer([1, 384, 2], dtype="float32")
        for i0, i1, i2 in T.grid(1, 384, 2):
            with T.block("T_reshape"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(placeholder[(ax2 // 2 + ax1) % 384, ax2 % 2])
                T.writes(T_reshape[ax0, ax1, ax2])
                T_reshape[ax0, ax1, ax2] = placeholder[(ax2 // 2 + ax1) % 384, ax2 % 2]
        for i0, i1, i2 in T.grid(1, 384, 2):
            with T.block("T_add"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_reshape[ax0, ax1, ax2], placeholder_1[ax2])
                T.writes(T_add[ax0, ax1, ax2])
                T_add[ax0, ax1, ax2] = T_reshape[ax0, ax1, ax2] + placeholder_1[ax2]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_split"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add[ax0, ax1, ax2])
                T.writes(T_split[ax0, ax1, ax2])
                T_split[ax0, ax1, ax2] = T_add[ax0, ax1, ax2]
        for i0, i1, i2 in T.grid(1, 384, 1):
            with T.block("T_split_1"):
                ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                T.reads(T_add[ax0, ax1, ax2 + 1])
                T.writes(T_split_1[ax0, ax1, ax2])
                T_split_1[ax0, ax1, ax2] = T_add[ax0, ax1, ax2 + 1]
    

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 2), "float32"], placeholder_1: T.Buffer[(2,), "float32"], T_split: T.Buffer[(1, 384, 1), "float32"], T_split_1: T.Buffer[(1, 384, 1), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":0})
            for i0, i1, i2 in T.grid(1, 384, 1):
                with T.block("T_split"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[(ax2 // 2 + ax1) % 384, ax2 % 2], placeholder_1[ax2])
                    T.writes(T_split[ax0, ax1, ax2])
                    T_split[ax0, ax1, ax2] = placeholder[(ax2 // 2 + ax1) % 384, ax2 % 2] + placeholder_1[ax2]
            for i0, i1, i2 in T.grid(1, 384, 1):
                with T.block("T_split_1"):
                    ax0, ax1, ax2 = T.axis.remap("SSS", [i0, i1, i2])
                    T.reads(placeholder[((ax2 + 1) // 2 + ax1) % 384, (ax2 + 1) % 2], placeholder_1[ax2 + 1])
                    T.writes(T_split_1[ax0, ax1, ax2])
                    T_split_1[ax0, ax1, ax2] = placeholder[((ax2 + 1) // 2 + ax1) % 384, (ax2 + 1) % 2] + placeholder_1[ax2 + 1]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v3 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v3)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:96: Initializing Task #25: "fused_squeeze_1"
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:102: 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_squeeze: T.Buffer[(1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        for i0, i1 in T.grid(1, 384):
            with T.block("T_squeeze"):
                ax0, ax1 = T.axis.remap("SS", [i0, i1])
                T.reads(placeholder[ax0, ax1, 0])
                T.writes(T_squeeze[ax0, ax1])
                T_squeeze[ax0, ax1] = placeholder[ax0, ax1, 0]
    

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:106: Total 1 design space(s) generated
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:111: Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(1, 384, 1), "float32"], placeholder_1: T.Buffer[(1, 384, 1), "float32"], T_squeeze: T.Buffer[(1, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit":1024})
            for i0, i1 in T.grid(1, 384):
                with T.block("T_squeeze"):
                    ax0, ax1 = T.axis.remap("SS", [i0, i1])
                    T.reads(placeholder[ax0, ax1, 0])
                    T.writes(T_squeeze[ax0, ax1])
                    T_squeeze[ax0, ax1] = placeholder[ax0, ax1, 0]
    

b0 = sch.get_block(name="root", func_name="main")
v1 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.unroll_explicit", ann_val=v1)
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:111: 
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                                 fused_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 0
Total latency (us): 0

[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_squeeze"
[13:35:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:23] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:35:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"
[13:35:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:33] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:35:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"
[13:35:43] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:35:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"
[13:35:53] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:35:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_mean"
[13:36:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:36:07] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:36:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_less_add_where_take_add_less_add_where_take_add"
[13:36:18] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:36:19] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:37:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_reshape_add_reshape_transpose_reshape"
[13:37:14] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:37:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
terminate called after throwing an instance of 'tvm::runtime::InternalError'
  what():  [13:37:31] /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61: CUDAError: cuModuleUnload(module_[i]) failed with error: CUDA_ERROR_LAUNCH_TIMEOUT
Stack trace:
  0: tvm::runtime::CUDAModuleNode::~CUDAModuleNode()
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:61
  1: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::CUDAModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  2: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  3: tvm::runtime::ObjectPtr<tvm::runtime::Object>::reset()
        at /home/yj/tvm/include/tvm/runtime/object.h:451
  4: tvm::runtime::ObjectPtr<tvm::runtime::Object>::~ObjectPtr()
        at /home/yj/tvm/include/tvm/runtime/object.h:400
  5: tvm::runtime::ObjectRef::~ObjectRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:511
  6: tvm::runtime::Module::~Module()
        at /home/yj/tvm/include/tvm/runtime/module.h:48
  7: void std::_Destroy<tvm::runtime::Module>(tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:98
  8: void std::_Destroy_aux<false>::__destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:108
  9: void std::_Destroy<tvm::runtime::Module*>(tvm::runtime::Module*, tvm::runtime::Module*)
        at /usr/include/c++/9/bits/stl_construct.h:137
  10: void std::_Destroy<tvm::runtime::Module*, tvm::runtime::Module>(tvm::runtime::Module*, tvm::runtime::Module*, std::allocator<tvm::runtime::Module>&)
        at /usr/include/c++/9/bits/stl_construct.h:206
  11: std::vector<tvm::runtime::Module, std::allocator<tvm::runtime::Module> >::~vector()
        at /usr/include/c++/9/bits/stl_vector.h:677
  12: tvm::runtime::ModuleNode::~ModuleNode()
        at /home/yj/tvm/include/tvm/runtime/module.h:114
  13: tvm::runtime::LibraryModuleNode::~LibraryModuleNode()
        at /home/yj/tvm/src/runtime/library_module.cc:38
  14: tvm::runtime::SimpleObjAllocator::Handler<tvm::runtime::LibraryModuleNode>::Deleter_(tvm::runtime::Object*)
        at /home/yj/tvm/include/tvm/runtime/memory.h:138
  15: tvm::runtime::Object::DecRef()
        at /home/yj/tvm/include/tvm/runtime/object.h:805
  16: tvm::runtime::ObjectInternal::ObjectFree(void*)
        at /home/yj/tvm/src/runtime/object_internal.h:56
  17: TVMObjectFree
        at /home/yj/tvm/src/runtime/object.cc:249
  18: TVMModFree
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:422
  19: ffi_call_unix64
  20: ffi_call_int
  21: _call_function_pointer
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:920
  22: _ctypes_callproc
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/callproc.c:1263
  23: PyCFuncPtr_call
        at /usr/local/src/conda/python-3.9.7/Modules/_ctypes/_ctypes.c:4201
  24: _PyObject_MakeTpCall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:191
  25: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:116
  26: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  27: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  28: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3487
  29: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  30: function_code_fastcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:330
  31: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:367
  32: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  33: PyObject_CallOneArg
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:188
  34: call_unbound_noarg
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1527
  35: slot_tp_finalize
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:7007
  36: PyObject_CallFinalizer
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:195
  37: PyObject_CallFinalizerFromDealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:213
  38: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1273
  39: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  40: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  41: frame_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/frameobject.c:582
  42: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  43: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  44: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  45: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:167
  46: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  47: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  48: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  49: tb_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/traceback.c:166
  50: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  51: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  52: BaseException_clear
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:78
  53: BaseException_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/exceptions.c:88
  54: subtype_dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/typeobject.c:1351
  55: _Py_Dealloc
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/object.c:2209
  56: _Py_DECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:430
  57: _Py_XDECREF
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/object.h:497
  58: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:1498
  59: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  60: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  61: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  62: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  63: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  64: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  65: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  66: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  67: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  68: _PyEval_EvalCodeWithName
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4359
  69: PyEval_EvalCodeEx
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4375
  70: PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:826
  71: builtin_exec_impl.isra.17
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/bltinmodule.c:1026
  72: builtin_exec
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/clinic/bltinmodule.c.h:396
  73: cfunction_vectorcall_FASTCALL
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/methodobject.c:430
  74: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  75: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  76: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  77: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  78: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  79: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  80: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  81: _PyObject_VectorcallTstate
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:118
  82: PyObject_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/cpython/abstract.h:127
  83: call_function
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:5075
  84: _PyEval_EvalFrameDefault
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:3518
  85: _PyEval_EvalFrame
        at /tmp/build/80754af9/python-split_1631797238431/work/Include/internal/pycore_ceval.h:40
  86: _PyEval_EvalCode
        at /tmp/build/80754af9/python-split_1631797238431/work/Python/ceval.c:4327
  87: _PyFunction_Vectorcall
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:396
  88: PyVectorcall_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:231
  89: _PyObject_Call
        at /tmp/build/80754af9/python-split_1631797238431/work/Objects/call.c:266
  90: pymain_run_module
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:297
  91: pymain_run_python
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:598
  92: Py_RunMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:683
  93: Py_BytesMain
        at /tmp/build/80754af9/python-split_1631797238431/work/Modules/main.c:1129
  94: __libc_start_main
  95: 0x000055c5d6c43bc2
  96: 0xffffffffffffffff


[13:37:37] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_batch_matmul"
[13:37:40] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:38:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:38:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_reshape_divide_add"
[13:38:22] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:38:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:38:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_softmax"
[13:38:38] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:38:44] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:38:59] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_reshape"
[13:39:00] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:39:01] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:39:16] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_batch_matmul_1"
[13:39:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:39:51] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:40:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_reshape_transpose_reshape"
[13:40:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:40:04] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:40:11] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_dense"
[13:40:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:40:45] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:40:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_add_sqrt_divide_multiply_add"
[13:40:54] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:40:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:41:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_reshape_1"
[13:41:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:41:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:41:15] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_dense_1"
[13:41:17] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:41:47] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:42:05] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"
[13:42:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:42:08] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:42:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_dense_2"
[13:42:24] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:42:55] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:43:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_reshape_add_add"
[13:43:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:43:10] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_subtract"
[13:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:43:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:43:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_power_mean"
[13:43:31] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:43:49] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_add_sqrt_divide_multiply_add_reshape"
[13:43:56] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:43:57] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:44:06] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_dense_3"
[13:44:09] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:44:13] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_reshape_add_split"
[13:44:20] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:44:21] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_squeeze_1"
[13:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:32: Sending 32 sample(s) to builder
[13:44:32] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:53: Sending 32 sample(s) to runner
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #0: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #1: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #2: GFLOPs: 0.0000. Time: 0.0060 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #3: GFLOPs: 0.0000. Time: 0.0050 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #4: GFLOPs: 0.0000. Time: 0.0053 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #5: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #6: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #7: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #8: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #9: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #10: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #11: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #12: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #13: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #14: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #15: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #16: GFLOPs: 0.0000. Time: 0.0027 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #17: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #18: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #19: GFLOPs: 0.0000. Time: 0.0030 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #20: GFLOPs: 0.0000. Time: 0.0029 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #21: GFLOPs: 0.0000. Time: 0.0028 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #22: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #23: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #24: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #25: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #26: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #27: GFLOPs: 0.0000. Time: 0.0026 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #28: GFLOPs: 0.0000. Time: 0.0035 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #29: GFLOPs: 0.0000. Time: 0.0038 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #30: GFLOPs: 0.0000. Time: 0.0044 ms. Best GFLOPs: 0.0000
[13:44:42] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #0: "fused_squeeze"] Trial #31: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
/home/yj/anaconda3/lib/python3.9/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
[13:44:43] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #0: "fused_squeeze"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                                 fused_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 32
Total latency (us): 2.26184

[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #0: GFLOPs: 8.8116. Time: 0.0446 ms. Best GFLOPs: 8.8116
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #1: GFLOPs: 8.3107. Time: 0.0473 ms. Best GFLOPs: 8.8116
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #2: GFLOPs: 8.9794. Time: 0.0438 ms. Best GFLOPs: 8.9794
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #3: GFLOPs: 8.3101. Time: 0.0473 ms. Best GFLOPs: 8.9794
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #4: GFLOPs: 8.8312. Time: 0.0445 ms. Best GFLOPs: 8.9794
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #5: GFLOPs: 8.2695. Time: 0.0476 ms. Best GFLOPs: 8.9794
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #6: GFLOPs: 9.7317. Time: 0.0404 ms. Best GFLOPs: 9.7317
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #7: GFLOPs: 10.5001. Time: 0.0374 ms. Best GFLOPs: 10.5001
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #8: GFLOPs: 8.5105. Time: 0.0462 ms. Best GFLOPs: 10.5001
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #9: GFLOPs: 9.9028. Time: 0.0397 ms. Best GFLOPs: 10.5001
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #10: GFLOPs: 10.5210. Time: 0.0374 ms. Best GFLOPs: 10.5210
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #11: GFLOPs: 8.4439. Time: 0.0466 ms. Best GFLOPs: 10.5210
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #12: GFLOPs: 9.3193. Time: 0.0422 ms. Best GFLOPs: 10.5210
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #13: GFLOPs: 8.2717. Time: 0.0475 ms. Best GFLOPs: 10.5210
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #14: GFLOPs: 9.4907. Time: 0.0414 ms. Best GFLOPs: 10.5210
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #15: GFLOPs: 10.5997. Time: 0.0371 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #16: GFLOPs: 8.3338. Time: 0.0472 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #17: GFLOPs: 10.2175. Time: 0.0385 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #18: GFLOPs: 8.8246. Time: 0.0446 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #19: GFLOPs: 8.8982. Time: 0.0442 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #20: GFLOPs: 10.4976. Time: 0.0375 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #21: GFLOPs: 8.2114. Time: 0.0479 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #22: GFLOPs: 10.3212. Time: 0.0381 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #23: GFLOPs: 10.2722. Time: 0.0383 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #24: GFLOPs: 8.2405. Time: 0.0477 ms. Best GFLOPs: 10.5997
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #25: GFLOPs: 17.3213. Time: 0.0227 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #26: GFLOPs: 16.0931. Time: 0.0244 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #27: GFLOPs: 14.8027. Time: 0.0266 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #28: GFLOPs: 15.7504. Time: 0.0250 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #29: GFLOPs: 15.2212. Time: 0.0258 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #30: GFLOPs: 15.0058. Time: 0.0262 ms. Best GFLOPs: 17.3213
[13:44:43] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"] Trial #31: GFLOPs: 17.2872. Time: 0.0227 ms. Best GFLOPs: 17.3213
[13:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                                 fused_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 64
Total latency (us): 547.093

[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #0: GFLOPs: 0.2686. Time: 0.0029 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #1: GFLOPs: 0.2402. Time: 0.0032 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #2: GFLOPs: 0.2590. Time: 0.0030 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #3: GFLOPs: 0.2613. Time: 0.0029 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #4: GFLOPs: 0.2355. Time: 0.0033 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #5: GFLOPs: 0.2677. Time: 0.0029 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #6: GFLOPs: 0.2374. Time: 0.0032 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #7: GFLOPs: 0.2623. Time: 0.0029 ms. Best GFLOPs: 0.2686
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #8: GFLOPs: 0.2725. Time: 0.0028 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #9: GFLOPs: 0.2705. Time: 0.0028 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #10: GFLOPs: 0.2670. Time: 0.0029 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #11: GFLOPs: 0.2387. Time: 0.0032 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #12: GFLOPs: 0.2511. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #13: GFLOPs: 0.2625. Time: 0.0029 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #14: GFLOPs: 0.2516. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #15: GFLOPs: 0.2620. Time: 0.0029 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #16: GFLOPs: 0.2509. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #17: GFLOPs: 0.2596. Time: 0.0030 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #18: GFLOPs: 0.2482. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #19: GFLOPs: 0.2720. Time: 0.0028 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #20: GFLOPs: 0.2479. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #21: GFLOPs: 0.2506. Time: 0.0031 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #22: GFLOPs: 0.2548. Time: 0.0030 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #23: GFLOPs: 0.2673. Time: 0.0029 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #24: GFLOPs: 0.1987. Time: 0.0039 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #25: GFLOPs: 0.2332. Time: 0.0033 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #26: GFLOPs: 0.1358. Time: 0.0057 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #27: GFLOPs: 0.1749. Time: 0.0044 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #28: GFLOPs: 0.2193. Time: 0.0035 ms. Best GFLOPs: 0.2725
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #29: GFLOPs: 0.3488. Time: 0.0022 ms. Best GFLOPs: 0.3488
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #30: GFLOPs: 0.2492. Time: 0.0031 ms. Best GFLOPs: 0.3488
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"] Trial #31: GFLOPs: 0.2968. Time: 0.0026 ms. Best GFLOPs: 0.3488
[13:44:44] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  4 |                                                 fused_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 96
Total latency (us): 549.296

[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #0: GFLOPs: 5.1857. Time: 0.0758 ms. Best GFLOPs: 5.1857
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #1: GFLOPs: 4.9873. Time: 0.0788 ms. Best GFLOPs: 5.1857
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #2: GFLOPs: 4.6665. Time: 0.0843 ms. Best GFLOPs: 5.1857
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #3: GFLOPs: 6.6776. Time: 0.0589 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #4: GFLOPs: 4.5062. Time: 0.0873 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #5: GFLOPs: 4.8395. Time: 0.0813 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #6: GFLOPs: 5.3322. Time: 0.0737 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #7: GFLOPs: 5.1778. Time: 0.0759 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #8: GFLOPs: 4.8047. Time: 0.0818 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #9: GFLOPs: 5.8900. Time: 0.0668 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #10: GFLOPs: 4.5630. Time: 0.0862 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #11: GFLOPs: 5.9059. Time: 0.0666 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #12: GFLOPs: 5.3394. Time: 0.0736 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #13: GFLOPs: 5.9798. Time: 0.0658 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #14: GFLOPs: 5.2157. Time: 0.0754 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #15: GFLOPs: 5.3576. Time: 0.0734 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #16: GFLOPs: 5.5129. Time: 0.0713 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #17: GFLOPs: 5.3134. Time: 0.0740 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #18: GFLOPs: 6.4357. Time: 0.0611 ms. Best GFLOPs: 6.6776
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #19: GFLOPs: 6.8301. Time: 0.0576 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #20: GFLOPs: 5.5792. Time: 0.0705 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #21: GFLOPs: 5.2312. Time: 0.0752 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #22: GFLOPs: 5.7781. Time: 0.0681 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #23: GFLOPs: 4.5613. Time: 0.0862 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #24: GFLOPs: 4.7136. Time: 0.0834 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #25: GFLOPs: 4.9426. Time: 0.0796 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #26: GFLOPs: 4.9687. Time: 0.0791 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #27: GFLOPs: 5.9147. Time: 0.0665 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #28: GFLOPs: 4.8832. Time: 0.0805 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #29: GFLOPs: 4.8291. Time: 0.0814 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #30: GFLOPs: 5.3499. Time: 0.0735 ms. Best GFLOPs: 6.8301
[13:44:44] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"] Trial #31: GFLOPs: 5.9117. Time: 0.0665 ms. Best GFLOPs: 6.8301
[13:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 128
Total latency (us): 1931.01

[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #0: GFLOPs: 1.2906. Time: 0.3050 ms. Best GFLOPs: 1.2906
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #1: GFLOPs: 51.2245. Time: 0.0077 ms. Best GFLOPs: 51.2245
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #2: GFLOPs: 1.2133. Time: 0.3244 ms. Best GFLOPs: 51.2245
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #3: GFLOPs: 0.9938. Time: 0.3961 ms. Best GFLOPs: 51.2245
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #4: GFLOPs: 1.0421. Time: 0.3777 ms. Best GFLOPs: 51.2245
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #5: GFLOPs: 67.7794. Time: 0.0058 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #6: GFLOPs: 49.0061. Time: 0.0080 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #7: GFLOPs: 1.3156. Time: 0.2992 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #8: GFLOPs: 1.3160. Time: 0.2991 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #9: GFLOPs: 1.1337. Time: 0.3472 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #10: GFLOPs: 59.1389. Time: 0.0067 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #11: GFLOPs: 1.6875. Time: 0.2332 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #12: GFLOPs: 42.5897. Time: 0.0092 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #13: GFLOPs: 50.8562. Time: 0.0077 ms. Best GFLOPs: 67.7794
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #14: GFLOPs: 73.4448. Time: 0.0054 ms. Best GFLOPs: 73.4448
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #15: GFLOPs: 76.0983. Time: 0.0052 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #16: GFLOPs: 1.4900. Time: 0.2642 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #17: GFLOPs: 0.9763. Time: 0.4031 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #18: GFLOPs: 1.2937. Time: 0.3042 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #19: GFLOPs: 44.3177. Time: 0.0089 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #20: GFLOPs: 1.1945. Time: 0.3295 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #21: GFLOPs: 1.1517. Time: 0.3417 ms. Best GFLOPs: 76.0983
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #22: GFLOPs: 102.3166. Time: 0.0038 ms. Best GFLOPs: 102.3166
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #23: GFLOPs: 112.3821. Time: 0.0035 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #24: GFLOPs: 72.6305. Time: 0.0054 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #25: GFLOPs: 1.8281. Time: 0.2153 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #26: GFLOPs: 73.0796. Time: 0.0054 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #27: GFLOPs: 1.8701. Time: 0.2105 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #28: GFLOPs: 1.8702. Time: 0.2105 ms. Best GFLOPs: 112.3821
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #29: GFLOPs: 122.7182. Time: 0.0032 ms. Best GFLOPs: 122.7182
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #30: GFLOPs: 146.2430. Time: 0.0027 ms. Best GFLOPs: 146.2430
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #4: "fused_mean"] Trial #31: GFLOPs: 1.8704. Time: 0.2104 ms. Best GFLOPs: 146.2430
[13:44:45] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #4: "fused_mean"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |            N/A |          N/A |                   N/A |      0 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 160
Total latency (us): 2062.89

[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #0: GFLOPs: 80.4520. Time: 0.0098 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #1: GFLOPs: 56.1826. Time: 0.0140 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #2: GFLOPs: 66.8785. Time: 0.0118 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #3: GFLOPs: 69.5830. Time: 0.0113 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #4: GFLOPs: 64.4006. Time: 0.0122 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #5: GFLOPs: 72.9291. Time: 0.0108 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #6: GFLOPs: 51.5933. Time: 0.0153 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #7: GFLOPs: 50.5559. Time: 0.0156 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #8: GFLOPs: 62.1613. Time: 0.0127 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #9: GFLOPs: 50.7269. Time: 0.0155 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #10: GFLOPs: 62.8254. Time: 0.0125 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #11: GFLOPs: 51.3266. Time: 0.0153 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #12: GFLOPs: 44.3789. Time: 0.0177 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #13: GFLOPs: 40.0159. Time: 0.0197 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #14: GFLOPs: 45.6921. Time: 0.0172 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #15: GFLOPs: 40.4700. Time: 0.0195 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #16: GFLOPs: 60.7985. Time: 0.0129 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #17: GFLOPs: 45.3726. Time: 0.0173 ms. Best GFLOPs: 80.4520
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #18: GFLOPs: 91.2402. Time: 0.0086 ms. Best GFLOPs: 91.2402
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #19: GFLOPs: 46.6000. Time: 0.0169 ms. Best GFLOPs: 91.2402
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #20: GFLOPs: 102.1836. Time: 0.0077 ms. Best GFLOPs: 102.1836
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #21: GFLOPs: 87.2708. Time: 0.0090 ms. Best GFLOPs: 102.1836
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #22: GFLOPs: 103.1542. Time: 0.0076 ms. Best GFLOPs: 103.1542
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #23: GFLOPs: 58.2603. Time: 0.0135 ms. Best GFLOPs: 103.1542
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #24: GFLOPs: 95.5553. Time: 0.0082 ms. Best GFLOPs: 103.1542
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #25: GFLOPs: 103.4992. Time: 0.0076 ms. Best GFLOPs: 103.4992
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #26: GFLOPs: 105.1663. Time: 0.0075 ms. Best GFLOPs: 105.1663
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #27: GFLOPs: 108.2496. Time: 0.0073 ms. Best GFLOPs: 108.2496
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #28: GFLOPs: 108.5697. Time: 0.0073 ms. Best GFLOPs: 108.5697
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #29: GFLOPs: 108.3772. Time: 0.0073 ms. Best GFLOPs: 108.5697
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #30: GFLOPs: 104.9440. Time: 0.0075 ms. Best GFLOPs: 108.5697
[13:44:45] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #5: "fused_less_add_where_take_add_less_add_where_take_add"] Trial #31: GFLOPs: 55.2167. Time: 0.0143 ms. Best GFLOPs: 108.5697
[13:44:46] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #5: "fused_less_add_where_take_add_less_add_where_take_add"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |            N/A |          N/A |                   N/A |      0 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 192
Total latency (us): 2070.14

[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #0: GFLOPs: 88.1379. Time: 0.0045 ms. Best GFLOPs: 88.1379
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #1: GFLOPs: 86.7444. Time: 0.0045 ms. Best GFLOPs: 88.1379
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #2: GFLOPs: 88.1810. Time: 0.0045 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #3: GFLOPs: 88.0267. Time: 0.0045 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #4: GFLOPs: 63.4620. Time: 0.0062 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #5: GFLOPs: 85.5036. Time: 0.0046 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #6: GFLOPs: 82.3774. Time: 0.0048 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #7: GFLOPs: 87.8867. Time: 0.0045 ms. Best GFLOPs: 88.1810
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #8: GFLOPs: 88.2429. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #9: GFLOPs: 88.1686. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #10: GFLOPs: 87.3611. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #11: Error in building: LocalRunner: An exception occurred
Subprocess terminated
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024,), "float32"], T_reshape: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        for i0_i1_i2_fused_1 in T.thread_binding(256, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_i1_i2_fused_2 in T.thread_binding(1024, thread="threadIdx.x"):
                for i0_i1_i2_fused_0 in T.serial(2):
                    with T.block("T_reshape_2"):
                        ax0 = T.axis.spatial(16, (i0_i1_i2_fused_0 * 262144 + i0_i1_i2_fused_1 * 1024 + i0_i1_i2_fused_2) // 24576)
                        ax1 = T.axis.spatial(384, (i0_i1_i2_fused_0 * 262144 + i0_i1_i2_fused_1 * 1024 + i0_i1_i2_fused_2) % 24576 // 64)
                        ax2 = T.axis.spatial(64, (i0_i1_i2_fused_0 * 262144 + i0_i1_i2_fused_1 * 1024 + i0_i1_i2_fused_2) % 64)
                        T.where((i0_i1_i2_fused_0 * 256 + i0_i1_i2_fused_1) * 1024 + i0_i1_i2_fused_2 < 393216)
                        T.reads(placeholder[(ax2 // 64 + ax1) % 384, ((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64], placeholder_1[((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64])
                        T.writes(T_reshape[ax0, ax1, ax2])
                        T_reshape[ax0, ax1, ax2] = placeholder[((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 // 1024 + ((((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) // 1024 + (ax2 // 64 + ax1) % 384) % 384) % 384, (((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024 % 1024] + placeholder_1[(((ax2 // 64 + ax1) // 384 + ax0) % 16 * 64 + ax2 % 64) % 1024]
    

b0 = sch.get_block(name="T_reshape", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_reshape_1", func_name="main")
b3 = sch.get_block(name="T_transpose", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b3)
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.compute_inline(block=b0)
v5 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v5)
sch.enter_postproc()
b6 = sch.get_block(name="T_reshape_2", func_name="main")
l7, l8, l9 = sch.get_loops(block=b6)
l10 = sch.fuse(l7, l8, l9)
l11, l12, l13 = sch.split(loop=l10, factors=[None, 256, 1024])
sch.reorder(l12, l13, l11)
sch.bind(loop=l12, thread_axis="blockIdx.x")
sch.bind(loop=l13, thread_axis="threadIdx.x")
b14 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b14, ann_key="meta_schedule.unroll_explicit")
b15, = sch.get_child_blocks(b14)
l16, l17, l18 = sch.get_loops(block=b15)
sch.annotate(block_or_loop=l16, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l16, ann_key="pragma_unroll_explicit", ann_val=1)
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #12: GFLOPs: 82.5867. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #13: GFLOPs: 87.7491. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #14: GFLOPs: 86.5015. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #15: GFLOPs: 81.3390. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #16: GFLOPs: 82.3944. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #17: GFLOPs: 86.3585. Time: 0.0046 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #18: GFLOPs: 84.5189. Time: 0.0047 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #19: GFLOPs: 81.0437. Time: 0.0049 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #20: GFLOPs: 82.5608. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #21: GFLOPs: 82.3908. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #22: GFLOPs: 82.4568. Time: 0.0048 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #23: GFLOPs: 87.5325. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #24: GFLOPs: 86.6008. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #25: GFLOPs: 85.9741. Time: 0.0046 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #26: GFLOPs: 86.7937. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #27: GFLOPs: 87.4946. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #28: GFLOPs: 87.4732. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #29: GFLOPs: 87.4731. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #30: GFLOPs: 87.5487. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:46] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #6: "fused_reshape_add_reshape_transpose_reshape"] Trial #31: GFLOPs: 87.6537. Time: 0.0045 ms. Best GFLOPs: 88.2429
[13:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #6: "fused_reshape_add_reshape_transpose_reshape"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 224
Total latency (us): 2177.08

[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #0: GFLOPs: 22.3306. Time: 13.5236 ms. Best GFLOPs: 22.3306
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #1: GFLOPs: 1032.8006. Time: 0.2924 ms. Best GFLOPs: 1032.8006
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #2: GFLOPs: 45.6175. Time: 6.6200 ms. Best GFLOPs: 1032.8006
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #3: GFLOPs: 75.4966. Time: 4.0000 ms. Best GFLOPs: 1032.8006
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #4: GFLOPs: 130.2235. Time: 2.3190 ms. Best GFLOPs: 1032.8006
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #5: GFLOPs: 88.7202. Time: 3.4038 ms. Best GFLOPs: 1032.8006
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #6: GFLOPs: 1659.6470. Time: 0.1820 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #7: GFLOPs: 65.5648. Time: 4.6060 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #8: GFLOPs: 1305.3875. Time: 0.2313 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_batch_matmul"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], placeholder_1: T.Buffer[(16, 384, 64), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 384], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(8, 3, 3, 16):
                        with T.block("T_batch_matmul_NT_init"):
                            b = T.axis.spatial(16, i0_0_i1_0_i2_0_fused * 8 + i0_1_i1_1_i2_1_fused // 4 * 2 + i0_2_i1_2_i2_2_fused // 32)
                            i = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 4 * 96 + i0_2_i1_2_i2_2_fused % 32 // 8 * 24 + i1_3_init * 3 + i1_4_init)
                            j = T.axis.spatial(384, i0_2_i1_2_i2_2_fused % 8 * 48 + i2_3_init * 16 + i2_4_init)
                            T.reads()
                            T.writes(T_batch_matmul_NT_local[b, i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                            T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                    for i3_0 in T.serial(32):
                        for ax0_ax1_ax2_fused_0 in T.serial(32):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused * 8 + (ax0_ax1_ax2_fused_0 * 192 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) // 768)
                                        v1 = T.axis.spatial(384, (ax0_ax1_ax2_fused_0 * 192 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) % 768 // 2)
                                        v2 = T.axis.spatial(64, i3_0 * 2 + (ax0_ax1_ax2_fused_0 * 192 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) % 2)
                                        T.reads(placeholder[v0, v1, v2])
                                        T.writes(placeholder_shared[v0, v1, v2])
                                        placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in T.serial(24):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused * 8 + (ax0_ax1_ax2_fused_0 * 256 + ax0_ax1_ax2_fused_1 * 4 + ax0_ax1_ax2_fused_2) // 768)
                                        v1 = T.axis.spatial(384, (ax0_ax1_ax2_fused_0 * 256 + ax0_ax1_ax2_fused_1 * 4 + ax0_ax1_ax2_fused_2) % 768 // 2)
                                        v2 = T.axis.spatial(64, i3_0 * 2 + (ax0_ax1_ax2_fused_0 * 256 + ax0_ax1_ax2_fused_1 * 4 + ax0_ax1_ax2_fused_2) % 2)
                                        T.reads(placeholder_1[v0, v1, v2])
                                        T.writes(placeholder_shared_1[v0, v1, v2])
                                        placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                        for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(2, 1, 8, 3, 1, 1, 3, 16):
                            with T.block("T_batch_matmul_NT_update"):
                                b = T.axis.spatial(16, i0_0_i1_0_i2_0_fused * 8 + i0_1_i1_1_i2_1_fused // 4 * 2 + i0_2_i1_2_i2_2_fused // 32)
                                i = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 4 * 96 + i0_2_i1_2_i2_2_fused % 32 // 8 * 24 + i1_3 * 3 + i1_4)
                                j = T.axis.spatial(384, i0_2_i1_2_i2_2_fused % 8 * 48 + i2_3 * 16 + i2_4)
                                k = T.axis.reduce(64, i3_0 * 2 + i3_1)
                                T.reads(T_batch_matmul_NT_local[b, i, j], placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                T.writes(T_batch_matmul_NT_local[b, i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                                T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                    for ax0, ax1, ax2 in T.grid(1, 24, 48):
                        with T.block("T_batch_matmul_NT_local"):
                            v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused * 8 + i0_1_i1_1_i2_1_fused // 4 * 2 + i0_2_i1_2_i2_2_fused // 32 + ax0)
                            v1 = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 4 * 96 + i0_2_i1_2_i2_2_fused % 32 // 8 * 24 + ax1)
                            v2 = T.axis.spatial(384, i0_2_i1_2_i2_2_fused % 8 * 48 + ax2)
                            T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                            T.writes(T_batch_matmul_NT[v0, v1, v2])
                            T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[2, 4, 2, 1, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 4, 4, 8, 3])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 8, 3, 16])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[32, 2, 1])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
sch.enter_postproc()
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l67, l68, l69, l70, l71 = sch.get_loops(block=b46)
l72, l73, l74 = sch.split(loop=l71, factors=[None, 64, 3])
sch.vectorize(loop=l74)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch")
l75, l76, l77, l78, l79 = sch.get_loops(block=b56)
l80, l81, l82 = sch.split(loop=l79, factors=[None, 64, 4])
sch.vectorize(loop=l82)
sch.bind(loop=l81, thread_axis="threadIdx.x")
b83 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b83, ann_key="meta_schedule.unroll_explicit")
b84, b85, b86, b87 = sch.get_child_blocks(b83)
l88, l89, l90, l91, l92, l93, l94 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l88, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l88, ann_key="pragma_unroll_explicit", ann_val=1)
l95, l96, l97, l98, l99, l100, l101 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l95, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l95, ann_key="pragma_unroll_explicit", ann_val=1)
l102, l103, l104, l105, l106, l107, l108, l109, l110, l111, l112, l113 = sch.get_loops(block=b86)
sch.annotate(block_or_loop=l102, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l102, ann_key="pragma_unroll_explicit", ann_val=1)
l114, l115, l116, l117, l118, l119 = sch.get_loops(block=b87)
sch.annotate(block_or_loop=l114, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l114, ann_key="pragma_unroll_explicit", ann_val=1)
b120 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
l121, l122, l123, l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
b133 = sch.decompose_reduction(block=b120, loop=l124)
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #10: GFLOPs: 868.9935. Time: 0.3475 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #11: GFLOPs: 93.1092. Time: 3.2434 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #12: GFLOPs: 90.6391. Time: 3.3318 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #13: GFLOPs: 877.0011. Time: 0.3443 ms. Best GFLOPs: 1659.6470
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #14: GFLOPs: 2453.4690. Time: 0.1231 ms. Best GFLOPs: 2453.4690
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #15: GFLOPs: 1505.1253. Time: 0.2006 ms. Best GFLOPs: 2453.4690
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #16: GFLOPs: 670.3906. Time: 0.4505 ms. Best GFLOPs: 2453.4690
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #7: "fused_nn_batch_matmul"] Trial #17: Error in building: LocalRunner: An exception occurred
Traceback (most recent call last):
  File "/home/yj/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 153, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/local_runner.py", line 379, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/home/yj/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/home/yj/tvm/python/tvm/runtime/module.py", line 297, in evaluator
    blob = feval(*args)
  File "/home/yj/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 237, in __call__
    raise get_last_ffi_error()
tvm._ffi.base.TVMError: Traceback (most recent call last):
  7: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  5: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  4: operator()
        at /home/yj/tvm/src/runtime/rpc/rpc_module.cc:375
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1221
...
  1: Call
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  0: operator()
        at /home/yj/tvm/src/runtime/library_module.cc:80
  4: TVMFuncCall
        at /home/yj/tvm/src/runtime/c_runtime_api.cc:477
  3: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1217
  2: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /home/yj/tvm/include/tvm/runtime/packed_func.h:1213
  1: tvm::runtime::detail::PackFuncVoidAddr_<4, tvm::runtime::CUDAWrappedFunc>(tvm::runtime::CUDAWrappedFunc, std::vector<tvm::runtime::detail::ArgConvertCode, std::allocator<tvm::runtime::detail::ArgConvertCode> > const&)::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#1}::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /home/yj/tvm/src/runtime/cuda/../pack_args.h:183
  0: tvm::runtime::CUDAWrappedFunc::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*, void**) const
        at /home/yj/tvm/src/runtime/cuda/cuda_module.cc:190
  File "/home/yj/tvm/src/runtime/cuda/cuda_module.cc", line 190
  File "/home/yj/tvm/src/runtime/library_module.cc", line 80
TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------

  Check failed: ret == 0 (-1 vs. 0) : TVMError: CUDALaunch Error: CUDA_ERROR_OUT_OF_MEMORY
 grid=(3,1,1),  block=(48,1,1)

# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 64), "float32"], placeholder_1: T.Buffer[(16, 384, 64), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 384), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 384], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([16, 384, 64], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(3, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i0_3_init, i2_3_init, i1_4_init, i2_4_init in T.grid(8, 8, 4, 2):
                        with T.block("T_batch_matmul_NT_init"):
                            b = T.axis.spatial(16, i0_2_i1_2_i2_2_fused // 24 * 8 + i0_3_init)
                            i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused * 128 + i0_1_i1_1_i2_1_fused // 8 * 32 + i0_2_i1_2_i2_2_fused % 24 // 3 * 4 + i1_4_init)
                            j = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 8 * 48 + i0_2_i1_2_i2_2_fused % 3 * 16 + i2_3_init * 2 + i2_4_init)
                            T.reads()
                            T.writes(T_batch_matmul_NT_local[b, i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                            T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                    for i3_0 in T.serial(64):
                        for ax0_ax1_ax2_fused_0 in T.serial(22):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_fused_0 * 96 + ax0_ax1_ax2_fused_1 * 2 + ax0_ax1_ax2_fused_2) // 128)
                                        v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused * 128 + (ax0_ax1_ax2_fused_0 * 96 + ax0_ax1_ax2_fused_1 * 2 + ax0_ax1_ax2_fused_2) % 128)
                                        v2 = T.axis.spatial(64, i3_0)
                                        T.where((ax0_ax1_ax2_fused_0 * 48 + ax0_ax1_ax2_fused_1) * 2 + ax0_ax1_ax2_fused_2 < 2048)
                                        T.reads(placeholder[v0, v1, v2])
                                        T.writes(placeholder_shared[v0, v1, v2])
                                        placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in T.serial(128):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_fused_0 * 48 + ax0_ax1_ax2_fused_1) // 384)
                                    v1 = T.axis.spatial(384, (ax0_ax1_ax2_fused_0 * 48 + ax0_ax1_ax2_fused_1) % 384)
                                    v2 = T.axis.spatial(64, i3_0)
                                    T.reads(placeholder_1[v0, v1, v2])
                                    T.writes(placeholder_shared_1[v0, v1, v2])
                                    placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                        for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(1, 8, 1, 8, 1, 1, 4, 2):
                            with T.block("T_batch_matmul_NT_update"):
                                b = T.axis.spatial(16, i0_2_i1_2_i2_2_fused // 24 * 8 + i0_3)
                                i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused * 128 + i0_1_i1_1_i2_1_fused // 8 * 32 + i0_2_i1_2_i2_2_fused % 24 // 3 * 4 + i1_4)
                                j = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 8 * 48 + i0_2_i1_2_i2_2_fused % 3 * 16 + i2_3 * 2 + i2_4)
                                k = T.axis.reduce(64, i3_0)
                                T.reads(T_batch_matmul_NT_local[b, i, j], placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                T.writes(T_batch_matmul_NT_local[b, i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 64], "float32"], ["TENSOR", [16, 384, 64], "float32"], [16, 384, 384], "float32", 0, 1]})
                                T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                    for ax0, ax1, ax2 in T.grid(8, 4, 16):
                        with T.block("T_batch_matmul_NT_local"):
                            v0 = T.axis.spatial(16, i0_2_i1_2_i2_2_fused // 24 * 8 + ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused * 128 + i0_1_i1_1_i2_1_fused // 8 * 32 + i0_2_i1_2_i2_2_fused % 24 // 3 * 4 + ax1)
                            v2 = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 8 * 48 + i0_2_i1_2_i2_2_fused % 3 * 16 + ax2)
                            T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                            T.writes(T_batch_matmul_NT[v0, v1, v2])
                            T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 1, 2, 8, 1])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[3, 4, 8, 1, 4])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 8, 3, 8, 2])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
sch.enter_postproc()
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l67, l68, l69, l70, l71 = sch.get_loops(block=b46)
l72, l73, l74 = sch.split(loop=l71, factors=[None, 48, 2])
sch.vectorize(loop=l74)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch")
l75, l76, l77, l78, l79 = sch.get_loops(block=b56)
l80, l81 = sch.split(loop=l79, factors=[None, 48])
sch.bind(loop=l81, thread_axis="threadIdx.x")
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86 = sch.get_child_blocks(b82)
l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b86)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b118)
b131 = sch.decompose_reduction(block=b118, loop=l122)
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #18: GFLOPs: 1143.7711. Time: 0.2640 ms. Best GFLOPs: 2453.4690
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #19: GFLOPs: 2489.6025. Time: 0.1213 ms. Best GFLOPs: 2489.6025
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #20: GFLOPs: 1775.3803. Time: 0.1701 ms. Best GFLOPs: 2489.6025
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #21: GFLOPs: 5949.3998. Time: 0.0508 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #22: GFLOPs: 161.5221. Time: 1.8697 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #23: GFLOPs: 640.7822. Time: 0.4713 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #24: GFLOPs: 180.0719. Time: 1.6771 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #25: GFLOPs: 8.6436. Time: 34.9381 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #26: GFLOPs: 91.7691. Time: 3.2908 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #27: GFLOPs: 5.5568. Time: 54.3460 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #28: GFLOPs: 80.2680. Time: 3.7623 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #29: GFLOPs: 53.0424. Time: 5.6934 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #30: GFLOPs: 1275.3747. Time: 0.2368 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #7: "fused_nn_batch_matmul"] Trial #31: GFLOPs: 2107.7198. Time: 0.1433 ms. Best GFLOPs: 5949.3998
[13:44:47] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #7: "fused_nn_batch_matmul"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |            N/A |          N/A |                   N/A |      0 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 256
Total latency (us): 3395.32

[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #0: GFLOPs: 92.5743. Time: 0.0510 ms. Best GFLOPs: 92.5743
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #1: GFLOPs: 93.8024. Time: 0.0503 ms. Best GFLOPs: 93.8024
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #2: GFLOPs: 94.4888. Time: 0.0499 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #3: GFLOPs: 94.4670. Time: 0.0499 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #4: GFLOPs: 93.1974. Time: 0.0506 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #5: GFLOPs: 90.9467. Time: 0.0519 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #6: GFLOPs: 92.4670. Time: 0.0510 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #7: GFLOPs: 93.3957. Time: 0.0505 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #8: GFLOPs: 93.3661. Time: 0.0505 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #9: GFLOPs: 44.3453. Time: 0.1064 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #10: GFLOPs: 50.5797. Time: 0.0933 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #11: GFLOPs: 62.7398. Time: 0.0752 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #12: GFLOPs: 93.8330. Time: 0.0503 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #13: GFLOPs: 44.5439. Time: 0.1059 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #14: GFLOPs: 44.3868. Time: 0.1063 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #15: GFLOPs: 94.1263. Time: 0.0501 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #16: GFLOPs: 81.5114. Time: 0.0579 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #17: GFLOPs: 44.2101. Time: 0.1067 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #18: GFLOPs: 43.9130. Time: 0.1075 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #19: GFLOPs: 91.0583. Time: 0.0518 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #20: GFLOPs: 44.5108. Time: 0.1060 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #21: GFLOPs: 57.8185. Time: 0.0816 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #22: GFLOPs: 42.0671. Time: 0.1122 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #23: GFLOPs: 56.6548. Time: 0.0833 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #24: GFLOPs: 37.1980. Time: 0.1269 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #25: GFLOPs: 92.2314. Time: 0.0512 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #26: GFLOPs: 70.9507. Time: 0.0665 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #27: GFLOPs: 37.3221. Time: 0.1264 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #28: GFLOPs: 91.7142. Time: 0.0514 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #29: GFLOPs: 33.1186. Time: 0.1425 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #30: GFLOPs: 65.0124. Time: 0.0726 ms. Best GFLOPs: 94.4888
[13:44:47] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #8: "fused_reshape_divide_add"] Trial #31: GFLOPs: 44.3414. Time: 0.1064 ms. Best GFLOPs: 94.4888
[13:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #8: "fused_reshape_divide_add"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |            N/A |          N/A |                   N/A |      0 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 288
Total latency (us): 4593.83

[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #0: GFLOPs: 19.1459. Time: 0.4929 ms. Best GFLOPs: 19.1459
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #1: GFLOPs: 31.9399. Time: 0.2955 ms. Best GFLOPs: 31.9399
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #2: GFLOPs: 157.1040. Time: 0.0601 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #3: GFLOPs: 31.3723. Time: 0.3008 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #4: GFLOPs: 19.1595. Time: 0.4926 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #5: GFLOPs: 34.6267. Time: 0.2725 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #6: GFLOPs: 39.4755. Time: 0.2391 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #7: GFLOPs: 30.9777. Time: 0.3046 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #8: GFLOPs: 34.5137. Time: 0.2734 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #9: GFLOPs: 85.1575. Time: 0.1108 ms. Best GFLOPs: 157.1040
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #10: GFLOPs: 176.4070. Time: 0.0535 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #11: GFLOPs: 10.8802. Time: 0.8674 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #12: GFLOPs: 18.4906. Time: 0.5104 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #13: GFLOPs: 31.1653. Time: 0.3028 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #14: GFLOPs: 31.2657. Time: 0.3018 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #15: GFLOPs: 15.3398. Time: 0.6152 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #16: GFLOPs: 167.1875. Time: 0.0564 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #17: GFLOPs: 32.5130. Time: 0.2903 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #18: GFLOPs: 17.2313. Time: 0.5477 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #19: GFLOPs: 29.9283. Time: 0.3153 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #20: GFLOPs: 164.1657. Time: 0.0575 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #21: GFLOPs: 34.9492. Time: 0.2700 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #22: GFLOPs: 28.4452. Time: 0.3318 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #23: GFLOPs: 15.8197. Time: 0.5965 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #24: GFLOPs: 30.5293. Time: 0.3091 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #25: GFLOPs: 32.0372. Time: 0.2946 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #26: GFLOPs: 32.0941. Time: 0.2940 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #27: GFLOPs: 18.7649. Time: 0.5029 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #28: GFLOPs: 34.9492. Time: 0.2700 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #29: GFLOPs: 34.5822. Time: 0.2729 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #30: GFLOPs: 32.0855. Time: 0.2941 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #9: "fused_nn_softmax"] Trial #31: GFLOPs: 104.1769. Time: 0.0906 ms. Best GFLOPs: 176.4070
[13:44:48] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #9: "fused_nn_softmax"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 320
Total latency (us): 5877.75

[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0507 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0688 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0520 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0552 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0588 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0513 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0501 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0536 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0513 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0509 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0506 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0507 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0508 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0509 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0507 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0508 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0575 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0502 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0504 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.1058 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0777 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0745 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0755 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0822 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0800 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0895 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0766 ms. Best GFLOPs: 0.0000
[13:44:48] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #10: "fused_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0784 ms. Best GFLOPs: 0.0000
[13:44:49] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #10: "fused_reshape"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |            N/A |          N/A |                   N/A |      0 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 352
Total latency (us): 7080.38

[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #0: GFLOPs: 877.5392. Time: 0.3441 ms. Best GFLOPs: 877.5392
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #1: GFLOPs: 16.5618. Time: 18.2341 ms. Best GFLOPs: 877.5392
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #2: GFLOPs: 3127.6378. Time: 0.0966 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #3: GFLOPs: 1002.5236. Time: 0.3012 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #4: GFLOPs: 1227.6963. Time: 0.2460 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #5: GFLOPs: 538.3964. Time: 0.5609 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #6: GFLOPs: 684.6579. Time: 0.4411 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #7: GFLOPs: 1076.9114. Time: 0.2804 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #8: GFLOPs: 271.5360. Time: 1.1122 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #9: GFLOPs: 198.4227. Time: 1.5220 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #10: GFLOPs: 75.0740. Time: 4.0226 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #11: GFLOPs: 283.0845. Time: 1.0668 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_batch_matmul_1"] Trial #12: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(16, 64, 384), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 64], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([16, 384, 384], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([16, 64, 384], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(12, thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i0_4_init, i1_4_init in T.grid(4, 4, 16):
                        with T.block("T_batch_matmul_NT_init"):
                            b = T.axis.spatial(16, i0_1_i1_1_i2_1_fused // 3 * 4 + i0_4_init)
                            i = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 3 * 128 + i0_2_i1_2_i2_2_fused // 16 * 64 + i1_3_init * 16 + i1_4_init)
                            j = T.axis.spatial(64, i0_0_i1_0_i2_0_fused * 16 + i0_2_i1_2_i2_2_fused % 16)
                            T.reads()
                            T.writes(T_batch_matmul_NT_local[b, i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                            T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                    for i3_0 in T.serial(384):
                        for ax0_ax1_ax2_fused_0 in T.serial(48):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, (ax0_ax1_ax2_fused_0 * 128 + ax0_ax1_ax2_fused_1 * 4 + ax0_ax1_ax2_fused_2) // 384)
                                        v1 = T.axis.spatial(384, (ax0_ax1_ax2_fused_0 * 128 + ax0_ax1_ax2_fused_1 * 4 + ax0_ax1_ax2_fused_2) % 384)
                                        v2 = T.axis.spatial(384, i3_0)
                                        T.reads(placeholder[v0, v1, v2])
                                        T.writes(placeholder_shared[v0, v1, v2])
                                        placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in T.serial(8):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, (ax0_ax1_ax2_fused_0 * 32 + ax0_ax1_ax2_fused_1) // 16)
                                    v1 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused * 16 + (ax0_ax1_ax2_fused_0 * 32 + ax0_ax1_ax2_fused_1) % 16)
                                    v2 = T.axis.spatial(384, i3_0)
                                    T.reads(placeholder_1[v0, v1, v2])
                                    T.writes(placeholder_shared_1[v0, v1, v2])
                                    placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                        for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(1, 1, 4, 1, 1, 4, 16, 1):
                            with T.block("T_batch_matmul_NT_update"):
                                b = T.axis.spatial(16, i0_1_i1_1_i2_1_fused // 3 * 4 + i0_4)
                                i = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 3 * 128 + i0_2_i1_2_i2_2_fused // 16 * 64 + i1_3 * 16 + i1_4)
                                j = T.axis.spatial(64, i0_0_i1_0_i2_0_fused * 16 + i0_2_i1_2_i2_2_fused % 16)
                                k = T.axis.reduce(384, i3_0)
                                T.reads(T_batch_matmul_NT_local[b, i, j], placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                T.writes(T_batch_matmul_NT_local[b, i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                                T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                    for ax0, ax1, ax2 in T.grid(4, 64, 1):
                        with T.block("T_batch_matmul_NT_local"):
                            v0 = T.axis.spatial(16, i0_1_i1_1_i2_1_fused // 3 * 4 + ax0)
                            v1 = T.axis.spatial(384, i0_1_i1_1_i2_1_fused % 3 * 128 + i0_2_i1_2_i2_2_fused // 16 * 64 + ax1)
                            v2 = T.axis.spatial(64, i0_0_i1_0_i2_0_fused * 16 + i0_2_i1_2_i2_2_fused % 16 + ax2)
                            T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                            T.writes(T_batch_matmul_NT[v0, v1, v2])
                            T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 4, 1, 1, 4])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 3, 2, 4, 16])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[4, 1, 16, 1, 1])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[384, 1, 1])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
sch.enter_postproc()
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l67, l68, l69, l70, l71 = sch.get_loops(block=b46)
l72, l73, l74 = sch.split(loop=l71, factors=[None, 32, 4])
sch.vectorize(loop=l74)
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch")
l75, l76, l77, l78, l79 = sch.get_loops(block=b56)
l80, l81 = sch.split(loop=l79, factors=[None, 32])
sch.bind(loop=l81, thread_axis="threadIdx.x")
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86 = sch.get_child_blocks(b82)
l87, l88, l89, l90, l91, l92, l93 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l94, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l94, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b86)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b118)
b131 = sch.decompose_reduction(block=b118, loop=l122)
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #13: GFLOPs: 14.6617. Time: 20.5972 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #14: GFLOPs: 463.9739. Time: 0.6509 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #15: GFLOPs: 96.2499. Time: 3.1376 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #16: GFLOPs: 106.9467. Time: 2.8237 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #17: GFLOPs: 1002.4693. Time: 0.3012 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #18: GFLOPs: 1497.3683. Time: 0.2017 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #19: GFLOPs: 980.4942. Time: 0.3080 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #20: GFLOPs: 2021.5556. Time: 0.1494 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #21: GFLOPs: 109.6429. Time: 2.7543 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #11: "fused_nn_batch_matmul_1"] Trial #22: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(16, 384, 384), "float32"], placeholder_1: T.Buffer[(16, 64, 384), "float32"], T_batch_matmul_NT: T.Buffer[(16, 384, 64), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_batch_matmul_NT_local = T.alloc_buffer([16, 384, 64], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([16, 384, 384], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([16, 64, 384], dtype="float32", scope="shared")
        for i0_0_i1_0_i2_0_fused in T.thread_binding(24, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_i2_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_i2_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i1_3_init, i2_3_init, i0_4_init, i2_4_init in T.grid(4, 2, 2, 4):
                        with T.block("T_batch_matmul_NT_init"):
                            b = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 6 * 4 + i0_2_i1_2_i2_2_fused // 16 * 2 + i0_4_init)
                            i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 6 * 64 + i0_1_i1_1_i2_1_fused // 4 * 32 + i0_2_i1_2_i2_2_fused % 16 // 2 * 4 + i1_3_init)
                            j = T.axis.spatial(64, i0_1_i1_1_i2_1_fused % 4 * 16 + i0_2_i1_2_i2_2_fused % 2 * 8 + i2_3_init * 4 + i2_4_init)
                            T.reads()
                            T.writes(T_batch_matmul_NT_local[b, i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                            T_batch_matmul_NT_local[b, i, j] = T.float32(0)
                    for i3_0 in T.serial(16):
                        for ax0_ax1_ax2_fused_0 in T.serial(192):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 6 * 4 + (ax0_ax1_ax2_fused_0 * 32 + ax0_ax1_ax2_fused_1) // 1536)
                                    v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 6 * 64 + (ax0_ax1_ax2_fused_0 * 32 + ax0_ax1_ax2_fused_1) % 1536 // 24)
                                    v2 = T.axis.spatial(384, i3_0 * 24 + (ax0_ax1_ax2_fused_0 * 32 + ax0_ax1_ax2_fused_1) % 24)
                                    T.reads(placeholder[v0, v1, v2])
                                    T.writes(placeholder_shared[v0, v1, v2])
                                    placeholder_shared[v0, v1, v2] = placeholder[v0, v1, v2]
                        for ax0_ax1_ax2_fused_0 in T.serial(64):
                            for ax0_ax1_ax2_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_ax2_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 6 * 4 + (ax0_ax1_ax2_fused_0 * 96 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) // 1536)
                                        v1 = T.axis.spatial(64, (ax0_ax1_ax2_fused_0 * 96 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) % 1536 // 24)
                                        v2 = T.axis.spatial(384, i3_0 * 24 + (ax0_ax1_ax2_fused_0 * 96 + ax0_ax1_ax2_fused_1 * 3 + ax0_ax1_ax2_fused_2) % 24)
                                        T.reads(placeholder_1[v0, v1, v2])
                                        T.writes(placeholder_shared_1[v0, v1, v2])
                                        placeholder_shared_1[v0, v1, v2] = placeholder_1[v0, v1, v2]
                        for i3_1, i0_3, i1_3, i2_3, i3_2, i0_4, i1_4, i2_4 in T.grid(6, 1, 4, 2, 4, 2, 1, 4):
                            with T.block("T_batch_matmul_NT_update"):
                                b = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 6 * 4 + i0_2_i1_2_i2_2_fused // 16 * 2 + i0_4)
                                i = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 6 * 64 + i0_1_i1_1_i2_1_fused // 4 * 32 + i0_2_i1_2_i2_2_fused % 16 // 2 * 4 + i1_3)
                                j = T.axis.spatial(64, i0_1_i1_1_i2_1_fused % 4 * 16 + i0_2_i1_2_i2_2_fused % 2 * 8 + i2_3 * 4 + i2_4)
                                k = T.axis.reduce(384, i3_0 * 24 + i3_1 * 4 + i3_2)
                                T.reads(T_batch_matmul_NT_local[b, i, j], placeholder_shared[b, i, k], placeholder_shared_1[b, j, k])
                                T.writes(T_batch_matmul_NT_local[b, i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["batch_matmul.cuda", ["TENSOR", [16, 384, 384], "float32"], ["TENSOR", [16, 64, 384], "float32"], [16, 384, 64], "float32", 0, 1]})
                                T_batch_matmul_NT_local[b, i, j] = T_batch_matmul_NT_local[b, i, j] + placeholder_shared[b, i, k] * placeholder_shared_1[b, j, k]
                    for ax0, ax1, ax2 in T.grid(2, 4, 8):
                        with T.block("T_batch_matmul_NT_local"):
                            v0 = T.axis.spatial(16, i0_0_i1_0_i2_0_fused // 6 * 4 + i0_2_i1_2_i2_2_fused // 16 * 2 + ax0)
                            v1 = T.axis.spatial(384, i0_0_i1_0_i2_0_fused % 6 * 64 + i0_1_i1_1_i2_1_fused // 4 * 32 + i0_2_i1_2_i2_2_fused % 16 // 2 * 4 + ax1)
                            v2 = T.axis.spatial(64, i0_1_i1_1_i2_1_fused % 4 * 16 + i0_2_i1_2_i2_2_fused % 2 * 8 + ax2)
                            T.reads(T_batch_matmul_NT_local[v0, v1, v2])
                            T.writes(T_batch_matmul_NT[v0, v1, v2])
                            T_batch_matmul_NT[v0, v1, v2] = T_batch_matmul_NT_local[v0, v1, v2]
    

b0 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4, l5 = sch.get_loops(block=b0)
v6, v7, v8, v9, v10 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[4, 1, 2, 1, 2])
l11, l12, l13, l14, l15 = sch.split(loop=l2, factors=[v6, v7, v8, v9, v10])
v16, v17, v18, v19, v20 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[6, 2, 8, 4, 1])
l21, l22, l23, l24, l25 = sch.split(loop=l3, factors=[v16, v17, v18, v19, v20])
v26, v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 4, 2, 2, 4])
l31, l32, l33, l34, l35 = sch.split(loop=l4, factors=[v26, v27, v28, v29, v30])
v36, v37, v38 = sch.sample_perfect_tile(loop=l5, n=3, max_innermost_factor=64, decision=[16, 6, 4])
l39, l40, l41 = sch.split(loop=l5, factors=[v36, v37, v38])
sch.reorder(l11, l21, l31, l12, l22, l32, l13, l23, l33, l39, l40, l14, l24, l34, l41, l15, l25, l35)
l42 = sch.fuse(l11, l21, l31)
sch.bind(loop=l42, thread_axis="blockIdx.x")
l43 = sch.fuse(l12, l22, l32)
sch.bind(loop=l43, thread_axis="vthread.x")
l44 = sch.fuse(l13, l23, l33)
sch.bind(loop=l44, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b45 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b45, loop=l44, preserve_unit_loops=True)
b46 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b46, loop=l39, preserve_unit_loops=True)
l47, l48, l49, l50, l51, l52, l53 = sch.get_loops(block=b46)
l54 = sch.fuse(l51, l52, l53)
v55 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch", ann_val=v55)
b56 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b56, loop=l39, preserve_unit_loops=True)
l57, l58, l59, l60, l61, l62, l63 = sch.get_loops(block=b56)
l64 = sch.fuse(l61, l62, l63)
v65 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch", ann_val=v65)
v66 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v66)
sch.enter_postproc()
sch.unannotate(block_or_loop=b46, ann_key="meta_schedule.cooperative_fetch")
l67, l68, l69, l70, l71 = sch.get_loops(block=b46)
l72, l73 = sch.split(loop=l71, factors=[None, 32])
sch.bind(loop=l73, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b56, ann_key="meta_schedule.cooperative_fetch")
l74, l75, l76, l77, l78 = sch.get_loops(block=b56)
l79, l80, l81 = sch.split(loop=l78, factors=[None, 32, 3])
sch.vectorize(loop=l81)
sch.bind(loop=l80, thread_axis="threadIdx.x")
b82 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b82, ann_key="meta_schedule.unroll_explicit")
b83, b84, b85, b86 = sch.get_child_blocks(b82)
l87, l88, l89, l90, l91, l92 = sch.get_loops(block=b83)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l93, l94, l95, l96, l97, l98, l99 = sch.get_loops(block=b84)
sch.annotate(block_or_loop=l93, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l93, ann_key="pragma_unroll_explicit", ann_val=1)
l100, l101, l102, l103, l104, l105, l106, l107, l108, l109, l110, l111 = sch.get_loops(block=b85)
sch.annotate(block_or_loop=l100, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l100, ann_key="pragma_unroll_explicit", ann_val=1)
l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b86)
sch.annotate(block_or_loop=l112, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l112, ann_key="pragma_unroll_explicit", ann_val=1)
b118 = sch.get_block(name="T_batch_matmul_NT", func_name="main")
l119, l120, l121, l122, l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b118)
b131 = sch.decompose_reduction(block=b118, loop=l122)
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #23: GFLOPs: 1906.4844. Time: 0.1584 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #24: GFLOPs: 506.6357. Time: 0.5961 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #25: GFLOPs: 458.7419. Time: 0.6583 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #26: GFLOPs: 1120.0444. Time: 0.2696 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #27: GFLOPs: 905.3685. Time: 0.3336 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #28: GFLOPs: 238.3879. Time: 1.2668 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #29: GFLOPs: 2764.4142. Time: 0.1092 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #30: GFLOPs: 512.2591. Time: 0.5895 ms. Best GFLOPs: 3127.6378
[13:44:49] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #11: "fused_nn_batch_matmul_1"] Trial #31: GFLOPs: 1157.7987. Time: 0.2608 ms. Best GFLOPs: 3127.6378
[13:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #11: "fused_nn_batch_matmul_1"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |            N/A |          N/A |                   N/A |      0 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 384
Total latency (us): 9397.7

[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #0: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #1: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #2: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #3: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #4: GFLOPs: 0.0000. Time: 0.0042 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #5: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #6: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #7: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #8: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #9: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #10: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #11: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #12: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #13: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #14: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #15: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #16: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #17: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #18: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #19: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #20: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #21: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #22: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #23: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #24: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #25: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #26: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #27: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #28: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #29: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #30: GFLOPs: 0.0000. Time: 0.0048 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #12: "fused_reshape_transpose_reshape"] Trial #31: GFLOPs: 0.0000. Time: 0.0045 ms. Best GFLOPs: 0.0000
[13:44:50] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #12: "fused_reshape_transpose_reshape"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |            N/A |          N/A |                   N/A |      0 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 416
Total latency (us): 9494.85

[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #0: GFLOPs: 1661.9522. Time: 0.4846 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #1: GFLOPs: 59.9861. Time: 13.4249 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #2: GFLOPs: 21.5141. Time: 37.4315 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #3: GFLOPs: 752.0314. Time: 1.0708 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #4: GFLOPs: 397.9533. Time: 2.0236 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #5: GFLOPs: 1641.2889. Time: 0.4907 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #6: GFLOPs: 326.5966. Time: 2.4658 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #7: GFLOPs: 73.9091. Time: 10.8959 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #8: GFLOPs: 1410.1718. Time: 0.5711 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_dense"] Trial #9: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 1024], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(8, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(8, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i0_3_init, i0_4_init, i1_4_init in T.grid(2, 3, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 96 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 6 + i0_3_init * 3 + i0_4_init)
                            j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(256):
                        for ax0_ax1_fused_0 in T.serial(6):
                            for ax0_ax1_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 96 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1) // 4)
                                    v1 = T.axis.spatial(1024, i2_0 * 4 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1) % 4)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(8):
                            for ax0_ax1_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + (ax0_ax1_fused_0 * 256 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 4)
                                        v1 = T.axis.spatial(1024, i2_0 * 4 + (ax0_ax1_fused_0 * 256 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 4)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(2, 2, 1, 2, 3, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 96 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 6 + i0_3 * 3 + i0_4)
                                j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + i1_4)
                                k = T.axis.reduce(1024, i2_0 * 4 + i2_1 * 2 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(6, 16):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 96 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 6 + ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[4, 2, 8, 2, 3])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[2, 4, 8, 1, 16])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[256, 2, 2])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60 = sch.split(loop=l58, factors=[None, 64])
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l61, l62, l63, l64, l65 = sch.get_loops(block=b44)
l66, l67, l68 = sch.split(loop=l65, factors=[None, 64, 4])
sch.vectorize(loop=l68)
sch.bind(loop=l67, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #10: GFLOPs: 234.0384. Time: 3.4409 ms. Best GFLOPs: 1661.9522
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #11: GFLOPs: 2327.2001. Time: 0.3460 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #12: GFLOPs: 185.8228. Time: 4.3337 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #13: GFLOPs: 20.6859. Time: 38.9301 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #14: GFLOPs: 1288.8116. Time: 0.6248 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #15: GFLOPs: 91.4156. Time: 8.8093 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #16: GFLOPs: 1590.8854. Time: 0.5062 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #17: GFLOPs: 50.9709. Time: 15.7993 ms. Best GFLOPs: 2327.2001
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #18: GFLOPs: 4197.9491. Time: 0.1918 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #19: GFLOPs: 497.1255. Time: 1.6199 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #20: GFLOPs: 80.1386. Time: 10.0489 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #21: GFLOPs: 1078.1781. Time: 0.7469 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #22: GFLOPs: 3379.5167. Time: 0.2383 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #23: GFLOPs: 138.3617. Time: 5.8203 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #24: GFLOPs: 30.3671. Time: 26.5191 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #25: GFLOPs: 397.5830. Time: 2.0255 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #26: GFLOPs: 29.6102. Time: 27.1970 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_dense"] Trial #27: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 1024], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i0_3_init, i1_3_init, i0_4_init, i1_4_init in T.grid(2, 2, 3, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_1_i1_1_fused // 2 * 48 + i0_2_i1_2_fused // 4 * 6 + i0_3_init * 3 + i0_4_init)
                            j = T.axis.spatial(1024, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused % 2 * 128 + i0_2_i1_2_fused % 4 * 32 + i1_3_init * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(512):
                        for ax0_ax1_fused_0 in T.serial(8):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, (ax0_ax1_fused_0 * 96 + ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2) // 2)
                                        v1 = T.axis.spatial(1024, i2_0 * 2 + (ax0_ax1_fused_0 * 96 + ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2) % 2)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(16):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_fused * 256 + (ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1) // 2)
                                    v1 = T.axis.spatial(1024, i2_0 * 2 + (ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1) % 2)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 2, 2, 2, 3, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_1_i1_1_fused // 2 * 48 + i0_2_i1_2_fused // 4 * 6 + i0_3 * 3 + i0_4)
                                j = T.axis.spatial(1024, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused % 2 * 128 + i0_2_i1_2_fused % 4 * 32 + i1_3 * 16 + i1_4)
                                k = T.axis.reduce(1024, i2_0 * 2 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(6, 32):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_1_i1_1_fused // 2 * 48 + i0_2_i1_2_fused // 4 * 6 + ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_fused * 256 + i0_1_i1_1_fused % 2 * 128 + i0_2_i1_2_fused % 4 * 32 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 8, 8, 2, 3])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[4, 2, 4, 2, 16])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60, l61 = sch.split(loop=l58, factors=[None, 32, 3])
sch.vectorize(loop=l61)
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b44)
l67, l68 = sch.split(loop=l66, factors=[None, 32])
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #28: GFLOPs: 944.9906. Time: 0.8522 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #29: GFLOPs: 223.0443. Time: 3.6105 ms. Best GFLOPs: 4197.9491
[13:44:50] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #13: "fused_nn_dense"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(1024, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 1024], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(64, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(48, thread="threadIdx.x"):
                    for i1_3_init, i0_4_init, i1_4_init in T.grid(8, 2, 2):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 2 * 6 + i0_2_i1_2_fused // 16 * 2 + i0_4_init)
                            j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 2 * 256 + i0_2_i1_2_fused % 16 * 16 + i1_3_init * 2 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(128):
                        for ax0_ax1_fused_0 in T.serial(8):
                            for ax0_ax1_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + (ax0_ax1_fused_0 * 192 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 8)
                                        v1 = T.axis.spatial(1024, i2_0 * 8 + (ax0_ax1_fused_0 * 192 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 8)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(86):
                            for ax0_ax1_fused_1 in T.thread_binding(48, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + (ax0_ax1_fused_0 * 48 + ax0_ax1_fused_1) // 8)
                                    v1 = T.axis.spatial(1024, i2_0 * 8 + (ax0_ax1_fused_0 * 48 + ax0_ax1_fused_1) % 8)
                                    T.where(ax0_ax1_fused_0 * 48 + ax0_ax1_fused_1 < 4096)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(2, 1, 8, 4, 2, 2):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 2 * 6 + i0_2_i1_2_fused // 16 * 2 + i0_4)
                                j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 2 * 256 + i0_2_i1_2_fused % 16 * 16 + i1_3 * 2 + i1_4)
                                k = T.axis.reduce(1024, i2_0 * 8 + i2_1 * 4 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [1024, 1024], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(2, 16):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 2 * 6 + i0_2_i1_2_fused // 16 * 2 + ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 2 * 256 + i0_2_i1_2_fused % 16 * 16 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[2, 32, 3, 1, 2])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[2, 2, 16, 8, 2])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[128, 2, 4])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60, l61 = sch.split(loop=l58, factors=[None, 48, 4])
sch.vectorize(loop=l61)
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b44)
l67, l68 = sch.split(loop=l66, factors=[None, 48])
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:51] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #13: "fused_nn_dense"] Trial #31: GFLOPs: 2855.7940. Time: 0.2820 ms. Best GFLOPs: 4197.9491
[13:44:53] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #13: "fused_nn_dense"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |            N/A |          N/A |                   N/A |      0 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 448
Total latency (us): 27910.8

[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #0: GFLOPs: 188.7213. Time: 0.0063 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #1: GFLOPs: 164.0574. Time: 0.0072 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #2: GFLOPs: 184.9723. Time: 0.0064 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #3: GFLOPs: 154.3674. Time: 0.0076 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #4: GFLOPs: 176.9143. Time: 0.0067 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #5: GFLOPs: 103.9635. Time: 0.0114 ms. Best GFLOPs: 188.7213
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #6: GFLOPs: 191.1490. Time: 0.0062 ms. Best GFLOPs: 191.1490
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #7: GFLOPs: 139.2480. Time: 0.0085 ms. Best GFLOPs: 191.1490
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #8: GFLOPs: 91.4017. Time: 0.0129 ms. Best GFLOPs: 191.1490
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #9: GFLOPs: 194.9329. Time: 0.0061 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #10: GFLOPs: 169.4215. Time: 0.0070 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #11: GFLOPs: 191.7459. Time: 0.0062 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #12: GFLOPs: 145.0464. Time: 0.0081 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #13: GFLOPs: 194.4651. Time: 0.0061 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #14: GFLOPs: 92.2946. Time: 0.0128 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #15: GFLOPs: 191.5338. Time: 0.0062 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #16: GFLOPs: 116.2448. Time: 0.0102 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #17: GFLOPs: 133.3941. Time: 0.0088 ms. Best GFLOPs: 194.9329
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #18: GFLOPs: 195.0729. Time: 0.0060 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #19: GFLOPs: 93.1257. Time: 0.0127 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #20: GFLOPs: 194.1820. Time: 0.0061 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #21: GFLOPs: 131.1236. Time: 0.0090 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #22: GFLOPs: 105.8268. Time: 0.0112 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #23: GFLOPs: 191.7322. Time: 0.0062 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #24: GFLOPs: 100.6713. Time: 0.0117 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #25: GFLOPs: 194.7397. Time: 0.0061 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #26: GFLOPs: 138.4741. Time: 0.0085 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #27: GFLOPs: 87.3017. Time: 0.0135 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #28: GFLOPs: 147.2437. Time: 0.0080 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #29: GFLOPs: 164.6632. Time: 0.0072 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #30: GFLOPs: 171.1835. Time: 0.0069 ms. Best GFLOPs: 195.0729
[13:44:53] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #14: "fused_add_sqrt_divide_multiply_add"] Trial #31: GFLOPs: 180.9396. Time: 0.0065 ms. Best GFLOPs: 195.0729
[13:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #14: "fused_add_sqrt_divide_multiply_add"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |            N/A |          N/A |                   N/A |      0 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 480
Total latency (us): 28201.2

[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0041 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0044 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0044 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0043 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0043 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0068 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0051 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0057 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0062 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0062 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0066 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0044 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0039 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #15: "fused_reshape_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0040 ms. Best GFLOPs: 0.0000
[13:44:54] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #15: "fused_reshape_1"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 512
Total latency (us): 28387.1

[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #0: GFLOPs: 191.5000. Time: 16.8210 ms. Best GFLOPs: 191.5000
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #1: GFLOPs: 30.7376. Time: 104.7976 ms. Best GFLOPs: 191.5000
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #2: GFLOPs: 598.9955. Time: 5.3777 ms. Best GFLOPs: 598.9955
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #3: GFLOPs: 396.1713. Time: 8.1309 ms. Best GFLOPs: 598.9955
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #4: GFLOPs: 192.8864. Time: 16.7001 ms. Best GFLOPs: 598.9955
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #5: GFLOPs: 401.2623. Time: 8.0277 ms. Best GFLOPs: 598.9955
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #6: GFLOPs: 3316.5388. Time: 0.9713 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #7: GFLOPs: 413.4753. Time: 7.7906 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #8: GFLOPs: 1122.0019. Time: 2.8710 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #9: GFLOPs: 2517.2425. Time: 1.2797 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #10: GFLOPs: 1422.5442. Time: 2.2644 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #16: "fused_nn_dense_1"] Trial #11: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 1024), "float32"], placeholder_1: T.Buffer[(4096, 1024), "float32"], T_matmul_NT: T.Buffer[(384, 4096), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 4096], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 1024], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([4096, 1024], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(48, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(128, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i0_3_init, i1_4_init in T.grid(2, 4):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_0_i1_0_fused // 4 * 32 + i0_1_i1_1_fused // 8 * 2 + i0_3_init)
                            j = T.axis.spatial(4096, i0_0_i1_0_fused % 4 * 1024 + i0_1_i1_1_fused % 8 * 128 + i0_2_i1_2_fused * 4 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [4096, 1024], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(512):
                        for ax0_ax1_fused_0 in T.serial(1):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_fused // 4 * 32 + (ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 2)
                                        v1 = T.axis.spatial(1024, i2_0 * 2 + (ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 2)
                                        T.where(ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2 < 64)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(64):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(4096, i0_0_i1_0_fused % 4 * 1024 + (ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1) // 2)
                                    v1 = T.axis.spatial(1024, i2_0 * 2 + (ax0_ax1_fused_0 * 32 + ax0_ax1_fused_1) % 2)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 2, 1, 2, 1, 4):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_0_i1_0_fused // 4 * 32 + i0_1_i1_1_fused // 8 * 2 + i0_3)
                                j = T.axis.spatial(4096, i0_0_i1_0_fused % 4 * 1024 + i0_1_i1_1_fused % 8 * 128 + i0_2_i1_2_fused * 4 + i1_4)
                                k = T.axis.reduce(1024, i2_0 * 2 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 1024], "float32"], ["TENSOR", [4096, 1024], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(2, 4):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_0_i1_0_fused // 4 * 32 + i0_1_i1_1_fused // 8 * 2 + ax0)
                            v1 = T.axis.spatial(4096, i0_0_i1_0_fused % 4 * 1024 + i0_1_i1_1_fused % 8 * 128 + i0_2_i1_2_fused * 4 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[12, 16, 1, 2, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[4, 8, 32, 1, 4])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[512, 1, 2])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60, l61 = sch.split(loop=l58, factors=[None, 32, 4])
sch.vectorize(loop=l61)
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b44)
l67, l68 = sch.split(loop=l66, factors=[None, 32])
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #12: GFLOPs: 15.9142. Time: 202.4114 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #13: GFLOPs: 1123.2809. Time: 2.8677 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #14: GFLOPs: 73.2927. Time: 43.9502 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #15: GFLOPs: 209.5215. Time: 15.3742 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #16: GFLOPs: 31.7122. Time: 101.5768 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #17: GFLOPs: 1910.0772. Time: 1.6864 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #18: GFLOPs: 58.5292. Time: 55.0362 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #19: GFLOPs: 354.9323. Time: 9.0756 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #20: GFLOPs: 2441.0196. Time: 1.3196 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #21: GFLOPs: 858.6694. Time: 3.7514 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #22: GFLOPs: 1278.5186. Time: 2.5195 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #23: GFLOPs: 951.9024. Time: 3.3840 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #24: GFLOPs: 1119.3055. Time: 2.8779 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #25: GFLOPs: 92.2618. Time: 34.9140 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #26: GFLOPs: 20.3820. Time: 158.0424 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #27: GFLOPs: 152.1056. Time: 21.1776 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #28: GFLOPs: 836.2286. Time: 3.8521 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #29: GFLOPs: 55.2835. Time: 58.2674 ms. Best GFLOPs: 3316.5388
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #30: GFLOPs: 4725.3204. Time: 0.6817 ms. Best GFLOPs: 4725.3204
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #16: "fused_nn_dense_1"] Trial #31: GFLOPs: 9.7046. Time: 331.9270 ms. Best GFLOPs: 4725.3204
[13:44:55] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #16: "fused_nn_dense_1"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |            N/A |          N/A |                   N/A |      0 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 544
Total latency (us): 44747.8

[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #0: GFLOPs: 229.4073. Time: 0.0343 ms. Best GFLOPs: 229.4073
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #1: GFLOPs: 221.6195. Time: 0.0355 ms. Best GFLOPs: 229.4073
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #2: GFLOPs: 212.9195. Time: 0.0369 ms. Best GFLOPs: 229.4073
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #3: GFLOPs: 202.0104. Time: 0.0389 ms. Best GFLOPs: 229.4073
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #4: GFLOPs: 218.2462. Time: 0.0360 ms. Best GFLOPs: 229.4073
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #5: GFLOPs: 231.3148. Time: 0.0340 ms. Best GFLOPs: 231.3148
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #6: GFLOPs: 220.2539. Time: 0.0357 ms. Best GFLOPs: 231.3148
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #7: GFLOPs: 233.1323. Time: 0.0337 ms. Best GFLOPs: 233.1323
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #8: GFLOPs: 230.0495. Time: 0.0342 ms. Best GFLOPs: 233.1323
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #9: GFLOPs: 233.6771. Time: 0.0337 ms. Best GFLOPs: 233.6771
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #10: GFLOPs: 228.9436. Time: 0.0344 ms. Best GFLOPs: 233.6771
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #11: GFLOPs: 233.2006. Time: 0.0337 ms. Best GFLOPs: 233.6771
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #12: GFLOPs: 233.7300. Time: 0.0336 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #13: GFLOPs: 232.9076. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #14: GFLOPs: 233.4070. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #15: GFLOPs: 230.5840. Time: 0.0341 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #16: GFLOPs: 233.6237. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #17: GFLOPs: 233.4919. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #18: GFLOPs: 233.0801. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #19: GFLOPs: 213.8971. Time: 0.0368 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #20: GFLOPs: 232.7128. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #21: GFLOPs: 233.5202. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #22: GFLOPs: 232.9263. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #23: GFLOPs: 232.8830. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #24: GFLOPs: 223.4791. Time: 0.0352 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #25: GFLOPs: 232.7535. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #26: GFLOPs: 233.0462. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #27: GFLOPs: 233.2162. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #28: GFLOPs: 230.2667. Time: 0.0342 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #29: GFLOPs: 233.2895. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #30: GFLOPs: 232.9146. Time: 0.0338 ms. Best GFLOPs: 233.7300
[13:44:55] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"] Trial #31: GFLOPs: 233.6662. Time: 0.0337 ms. Best GFLOPs: 233.7300
[13:44:56] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |            N/A |          N/A |                   N/A |      0 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 576
Total latency (us): 45555.3

[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_dense_2"] Trial #0: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(1024, 4096), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 4096], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 4096], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(768, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(32, thread="threadIdx.x"):
                    for i0_3_init, i1_3_init in T.grid(2, 4):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_0_i1_0_fused * 192 + i0_1_i1_1_fused // 32 * 8 + i0_2_i1_2_fused // 8 * 2 + i0_3_init)
                            j = T.axis.spatial(1024, i0_1_i1_1_fused % 32 * 32 + i0_2_i1_2_fused % 8 * 4 + i1_3_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(1024):
                        for ax0_ax1_fused_0 in T.serial(12):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(2):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, i0_0_i1_0_fused * 192 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) // 4)
                                        v1 = T.axis.spatial(4096, i2_0 * 4 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1 * 2 + ax0_ax1_fused_2) % 4)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(32):
                            for ax0_ax1_fused_1 in T.thread_binding(32, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 4)
                                        v1 = T.axis.spatial(4096, i2_0 * 4 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 4)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(1, 2, 4, 4, 1, 1):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_0_i1_0_fused * 192 + i0_1_i1_1_fused // 32 * 8 + i0_2_i1_2_fused // 8 * 2 + i0_3)
                                j = T.axis.spatial(1024, i0_1_i1_1_fused % 32 * 32 + i0_2_i1_2_fused % 8 * 4 + i1_3)
                                k = T.axis.reduce(4096, i2_0 * 4 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(2, 4):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_0_i1_0_fused * 192 + i0_1_i1_1_fused // 32 * 8 + i0_2_i1_2_fused // 8 * 2 + ax0)
                            v1 = T.axis.spatial(1024, i0_1_i1_1_fused % 32 * 32 + i0_2_i1_2_fused % 8 * 4 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[2, 24, 4, 2, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[1, 32, 8, 4, 1])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[1024, 1, 4])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60, l61 = sch.split(loop=l58, factors=[None, 32, 2])
sch.vectorize(loop=l61)
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b44)
l67, l68, l69 = sch.split(loop=l66, factors=[None, 32, 4])
sch.vectorize(loop=l69)
sch.bind(loop=l68, thread_axis="threadIdx.x")
b70 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b70, ann_key="meta_schedule.unroll_explicit")
b71, b72, b73, b74 = sch.get_child_blocks(b70)
l75, l76, l77, l78, l79, l80, l81 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l75, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l75, ann_key="pragma_unroll_explicit", ann_val=1)
l82, l83, l84, l85, l86, l87, l88 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l82, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l82, ann_key="pragma_unroll_explicit", ann_val=1)
l89, l90, l91, l92, l93, l94, l95, l96, l97, l98 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l89, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l89, ann_key="pragma_unroll_explicit", ann_val=1)
l99, l100, l101, l102, l103 = sch.get_loops(block=b74)
sch.annotate(block_or_loop=l99, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l99, ann_key="pragma_unroll_explicit", ann_val=1)
b104 = sch.get_block(name="T_matmul_NT", func_name="main")
l105, l106, l107, l108, l109, l110, l111, l112, l113, l114 = sch.get_loops(block=b104)
b115 = sch.decompose_reduction(block=b104, loop=l108)
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #1: GFLOPs: 48.3471. Time: 66.6270 ms. Best GFLOPs: 48.3471
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #2: GFLOPs: 433.0776. Time: 7.4380 ms. Best GFLOPs: 433.0776
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #3: GFLOPs: 22.0712. Time: 145.9471 ms. Best GFLOPs: 433.0776
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #4: GFLOPs: 2387.8313. Time: 1.3490 ms. Best GFLOPs: 2387.8313
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #5: GFLOPs: 5192.7990. Time: 0.6203 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #6: GFLOPs: 1933.0756. Time: 1.6664 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #7: GFLOPs: 2198.3769. Time: 1.4653 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #8: GFLOPs: 594.5064. Time: 5.4183 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #9: GFLOPs: 3290.4526. Time: 0.9790 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #10: GFLOPs: 189.8833. Time: 16.9642 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #11: GFLOPs: 28.5950. Time: 112.6501 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #12: GFLOPs: 459.1373. Time: 7.0158 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #13: GFLOPs: 149.4692. Time: 21.5511 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #14: GFLOPs: 2837.3280. Time: 1.1353 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #15: GFLOPs: 204.8356. Time: 15.7259 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #16: GFLOPs: 3874.6747. Time: 0.8314 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #17: GFLOPs: 238.6773. Time: 13.4962 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #18: GFLOPs: 244.4880. Time: 13.1754 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #19: GFLOPs: 142.0652. Time: 22.6743 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #20: GFLOPs: 2661.4515. Time: 1.2103 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #21: GFLOPs: 588.1576. Time: 5.4768 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #22: GFLOPs: 574.1719. Time: 5.6102 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_dense_2"] Trial #23: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(1024, 4096), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 4096], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 4096], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(2, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":512, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(32, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(64, thread="threadIdx.x"):
                    for i0_3_init, i1_4_init in T.grid(6, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_1_i1_1_fused // 8 * 96 + i0_2_i1_2_fused // 4 * 6 + i0_3_init)
                            j = T.axis.spatial(1024, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused % 8 * 64 + i0_2_i1_2_fused % 4 * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(512):
                        for ax0_ax1_fused_0 in T.serial(16):
                            for ax0_ax1_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(3):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(384, (ax0_ax1_fused_0 * 192 + ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2) // 8)
                                        v1 = T.axis.spatial(4096, i2_0 * 8 + (ax0_ax1_fused_0 * 192 + ax0_ax1_fused_1 * 3 + ax0_ax1_fused_2) % 8)
                                        T.reads(placeholder[v0, v1])
                                        T.writes(placeholder_shared[v0, v1])
                                        placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(64):
                            for ax0_ax1_fused_1 in T.thread_binding(64, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(1024, i0_0_i1_0_fused * 512 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1) // 8)
                                    v1 = T.axis.spatial(4096, i2_0 * 8 + (ax0_ax1_fused_0 * 64 + ax0_ax1_fused_1) % 8)
                                    T.reads(placeholder_1[v0, v1])
                                    T.writes(placeholder_shared_1[v0, v1])
                                    placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(8, 6, 1, 1, 1, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_1_i1_1_fused // 8 * 96 + i0_2_i1_2_fused // 4 * 6 + i0_3)
                                j = T.axis.spatial(1024, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused % 8 * 64 + i0_2_i1_2_fused % 4 * 16 + i1_4)
                                k = T.axis.reduce(4096, i2_0 * 8 + i2_1)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(6, 16):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_1_i1_1_fused // 8 * 96 + i0_2_i1_2_fused // 4 * 6 + ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_fused * 512 + i0_1_i1_1_fused % 8 * 64 + i0_2_i1_2_fused % 4 * 16 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[1, 4, 16, 6, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[2, 8, 4, 1, 16])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[512, 8, 1])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60, l61 = sch.split(loop=l58, factors=[None, 64, 3])
sch.vectorize(loop=l61)
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l62, l63, l64, l65, l66 = sch.get_loops(block=b44)
l67, l68 = sch.split(loop=l66, factors=[None, 64])
sch.bind(loop=l68, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79, l80 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l81, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l81, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #24: GFLOPs: 845.3315. Time: 3.8106 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #25: GFLOPs: 2431.1364. Time: 1.3250 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #26: GFLOPs: 564.4823. Time: 5.7065 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #27: GFLOPs: 139.8520. Time: 23.0331 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #28: GFLOPs: 461.7379. Time: 6.9763 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #29: GFLOPs: 43.9026. Time: 73.3721 ms. Best GFLOPs: 5192.7990
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:65: [Task #18: "fused_nn_dense_2"] Trial #30: Error in building: LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(placeholder: T.Buffer[(384, 4096), "float32"], placeholder_1: T.Buffer[(1024, 4096), "float32"], T_matmul_NT: T.Buffer[(384, 1024), "float32"]) -> None:
        # function attr dict
        T.func_attr({"tir.noalias": True, "global_symbol": "main"})
        # body
        # with T.block("root")
        T_matmul_NT_local = T.alloc_buffer([384, 1024], dtype="float32", scope="local")
        placeholder_shared = T.alloc_buffer([384, 4096], dtype="float32", scope="shared")
        placeholder_shared_1 = T.alloc_buffer([1024, 4096], dtype="float32", scope="shared")
        for i0_0_i1_0_fused in T.thread_binding(4, thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step":1024, "pragma_unroll_explicit":1}):
            for i0_1_i1_1_fused in T.thread_binding(16, thread="vthread.x"):
                for i0_2_i1_2_fused in T.thread_binding(128, thread="threadIdx.x"):
                    for i0_3_init, i1_4_init in T.grid(3, 16):
                        with T.block("T_matmul_NT_init"):
                            i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 3 + i0_3_init)
                            j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + i1_4_init)
                            T.reads()
                            T.writes(T_matmul_NT_local[i, j])
                            T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                            T_matmul_NT_local[i, j] = T.float32(0)
                    for i2_0 in T.serial(256):
                        for ax0_ax1_fused_0 in T.serial(24):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                with T.block("placeholder_shared"):
                                    v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1) // 16)
                                    v1 = T.axis.spatial(4096, i2_0 * 16 + (ax0_ax1_fused_0 * 128 + ax0_ax1_fused_1) % 16)
                                    T.reads(placeholder[v0, v1])
                                    T.writes(placeholder_shared[v0, v1])
                                    placeholder_shared[v0, v1] = placeholder[v0, v1]
                        for ax0_ax1_fused_0 in T.serial(16):
                            for ax0_ax1_fused_1 in T.thread_binding(128, thread="threadIdx.x"):
                                for ax0_ax1_fused_2 in T.vectorized(4):
                                    with T.block("placeholder_shared"):
                                        v0 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) // 16)
                                        v1 = T.axis.spatial(4096, i2_0 * 16 + (ax0_ax1_fused_0 * 512 + ax0_ax1_fused_1 * 4 + ax0_ax1_fused_2) % 16)
                                        T.reads(placeholder_1[v0, v1])
                                        T.writes(placeholder_shared_1[v0, v1])
                                        placeholder_shared_1[v0, v1] = placeholder_1[v0, v1]
                        for i2_1, i0_3, i1_3, i2_2, i0_4, i1_4 in T.grid(4, 3, 1, 4, 1, 16):
                            with T.block("T_matmul_NT_update"):
                                i = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 3 + i0_3)
                                j = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + i1_4)
                                k = T.axis.reduce(4096, i2_0 * 16 + i2_1 * 4 + i2_2)
                                T.reads(T_matmul_NT_local[i, j], placeholder_shared[i, k], placeholder_shared_1[j, k])
                                T.writes(T_matmul_NT_local[i, j])
                                T.block_attr({"layout_free_placeholders":[placeholder_1], "meta_schedule.thread_extent_high_inclusive":1024, "meta_schedule.thread_extent_low_inclusive":32, "meta_schedule.tiling_structure":"SSSRRSRS", "workload":["dense_small_batch.gpu", ["TENSOR", [384, 4096], "float32"], ["TENSOR", [1024, 4096], "float32"], None, "float32"]})
                                T_matmul_NT_local[i, j] = T_matmul_NT_local[i, j] + placeholder_shared[i, k] * placeholder_shared_1[j, k]
                    for ax0, ax1 in T.grid(3, 16):
                        with T.block("T_matmul_NT_local"):
                            v0 = T.axis.spatial(384, i0_0_i1_0_fused // 2 * 192 + i0_1_i1_1_fused // 4 * 48 + i0_2_i1_2_fused // 8 * 3 + ax0)
                            v1 = T.axis.spatial(1024, i0_0_i1_0_fused % 2 * 512 + i0_1_i1_1_fused % 4 * 128 + i0_2_i1_2_fused % 8 * 16 + ax1)
                            T.reads(T_matmul_NT_local[v0, v1])
                            T.writes(T_matmul_NT[v0, v1])
                            T_matmul_NT[v0, v1] = T_matmul_NT_local[v0, v1]
    

b0 = sch.get_block(name="T_matmul_NT", func_name="main")
b1 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l2, l3, l4 = sch.get_loops(block=b0)
v5, v6, v7, v8, v9 = sch.sample_perfect_tile(loop=l2, n=5, max_innermost_factor=64, decision=[2, 4, 16, 3, 1])
l10, l11, l12, l13, l14 = sch.split(loop=l2, factors=[v5, v6, v7, v8, v9])
v15, v16, v17, v18, v19 = sch.sample_perfect_tile(loop=l3, n=5, max_innermost_factor=64, decision=[2, 4, 8, 1, 16])
l20, l21, l22, l23, l24 = sch.split(loop=l3, factors=[v15, v16, v17, v18, v19])
v25, v26, v27 = sch.sample_perfect_tile(loop=l4, n=3, max_innermost_factor=64, decision=[256, 4, 4])
l28, l29, l30 = sch.split(loop=l4, factors=[v25, v26, v27])
sch.reorder(l10, l20, l11, l21, l12, l22, l28, l29, l13, l23, l30, l14, l24)
l31 = sch.fuse(l10, l20)
sch.bind(loop=l31, thread_axis="blockIdx.x")
l32 = sch.fuse(l11, l21)
sch.bind(loop=l32, thread_axis="vthread.x")
l33 = sch.fuse(l12, l22)
sch.bind(loop=l33, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=1024)
b34 = sch.cache_write(block=b0, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b34, loop=l33, preserve_unit_loops=True)
b35 = sch.cache_read(block=b0, read_buffer_index=0, storage_scope="shared")
sch.compute_at(block=b35, loop=l28, preserve_unit_loops=True)
l36, l37, l38, l39, l40, l41 = sch.get_loops(block=b35)
l42 = sch.fuse(l40, l41)
v43 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch", ann_val=v43)
b44 = sch.cache_read(block=b0, read_buffer_index=1, storage_scope="shared")
sch.compute_at(block=b44, loop=l28, preserve_unit_loops=True)
l45, l46, l47, l48, l49, l50 = sch.get_loops(block=b44)
l51 = sch.fuse(l49, l50)
v52 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch", ann_val=v52)
v53 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.unroll_explicit", ann_val=v53)
sch.enter_postproc()
sch.unannotate(block_or_loop=b35, ann_key="meta_schedule.cooperative_fetch")
l54, l55, l56, l57, l58 = sch.get_loops(block=b35)
l59, l60 = sch.split(loop=l58, factors=[None, 128])
sch.bind(loop=l60, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b44, ann_key="meta_schedule.cooperative_fetch")
l61, l62, l63, l64, l65 = sch.get_loops(block=b44)
l66, l67, l68 = sch.split(loop=l65, factors=[None, 128, 4])
sch.vectorize(loop=l68)
sch.bind(loop=l67, thread_axis="threadIdx.x")
b69 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b69, ann_key="meta_schedule.unroll_explicit")
b70, b71, b72, b73 = sch.get_child_blocks(b69)
l74, l75, l76, l77, l78, l79 = sch.get_loops(block=b70)
sch.annotate(block_or_loop=l74, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l74, ann_key="pragma_unroll_explicit", ann_val=1)
l80, l81, l82, l83, l84, l85, l86 = sch.get_loops(block=b71)
sch.annotate(block_or_loop=l80, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l80, ann_key="pragma_unroll_explicit", ann_val=1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b72)
sch.annotate(block_or_loop=l87, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l87, ann_key="pragma_unroll_explicit", ann_val=1)
l97, l98, l99, l100, l101 = sch.get_loops(block=b73)
sch.annotate(block_or_loop=l97, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l97, ann_key="pragma_unroll_explicit", ann_val=1)
b102 = sch.get_block(name="T_matmul_NT", func_name="main")
l103, l104, l105, l106, l107, l108, l109, l110, l111, l112 = sch.get_loops(block=b102)
b113 = sch.decompose_reduction(block=b102, loop=l106)
[13:44:56] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #18: "fused_nn_dense_2"] Trial #31: GFLOPs: 84.5986. Time: 38.0766 ms. Best GFLOPs: 5192.7990
[13:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #18: "fused_nn_dense_2"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |            N/A |          N/A |                   N/A |      0 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 608
Total latency (us): 60443.1

[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #0: GFLOPs: 63.6675. Time: 0.0124 ms. Best GFLOPs: 63.6675
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #1: GFLOPs: 64.6290. Time: 0.0122 ms. Best GFLOPs: 64.6290
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #2: GFLOPs: 64.6716. Time: 0.0122 ms. Best GFLOPs: 64.6716
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #3: GFLOPs: 60.2807. Time: 0.0130 ms. Best GFLOPs: 64.6716
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #4: GFLOPs: 64.6898. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #5: GFLOPs: 63.8554. Time: 0.0123 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #6: GFLOPs: 64.6754. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #7: GFLOPs: 64.5769. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #8: GFLOPs: 63.8924. Time: 0.0123 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #9: GFLOPs: 64.6170. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #10: GFLOPs: 64.6721. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #11: GFLOPs: 64.4917. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #12: GFLOPs: 64.6126. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #13: GFLOPs: 63.7466. Time: 0.0123 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #14: GFLOPs: 64.6537. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #15: GFLOPs: 64.6251. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #16: GFLOPs: 63.6498. Time: 0.0124 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #17: GFLOPs: 64.5603. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #18: GFLOPs: 64.6360. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #19: GFLOPs: 63.4454. Time: 0.0124 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #20: GFLOPs: 64.6082. Time: 0.0122 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #21: GFLOPs: 55.2873. Time: 0.0142 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #22: GFLOPs: 56.2310. Time: 0.0140 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #23: GFLOPs: 56.0439. Time: 0.0140 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #24: GFLOPs: 60.2048. Time: 0.0131 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #25: GFLOPs: 59.8359. Time: 0.0131 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #26: GFLOPs: 59.1327. Time: 0.0133 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #27: GFLOPs: 60.7870. Time: 0.0129 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #28: GFLOPs: 59.3485. Time: 0.0133 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #29: GFLOPs: 60.7763. Time: 0.0129 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #30: GFLOPs: 61.3198. Time: 0.0128 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #19: "fused_reshape_add_add"] Trial #31: GFLOPs: 59.6394. Time: 0.0132 ms. Best GFLOPs: 64.6898
[13:44:57] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #19: "fused_reshape_add_add"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |            N/A |          N/A |                   N/A |      0 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 640
Total latency (us): 61026.7

[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #0: GFLOPs: 90.1224. Time: 0.0044 ms. Best GFLOPs: 90.1224
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #1: GFLOPs: 93.9748. Time: 0.0042 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #2: GFLOPs: 90.4980. Time: 0.0043 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #3: GFLOPs: 91.1131. Time: 0.0043 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #4: GFLOPs: 91.2268. Time: 0.0043 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #5: GFLOPs: 92.2090. Time: 0.0043 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #6: GFLOPs: 92.2978. Time: 0.0043 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #7: GFLOPs: 50.2664. Time: 0.0078 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #8: GFLOPs: 50.3696. Time: 0.0078 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #9: GFLOPs: 45.9621. Time: 0.0086 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #10: GFLOPs: 46.9275. Time: 0.0084 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #11: GFLOPs: 45.1363. Time: 0.0087 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #12: GFLOPs: 65.2540. Time: 0.0060 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #13: GFLOPs: 57.2440. Time: 0.0069 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #14: GFLOPs: 68.9561. Time: 0.0057 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #15: GFLOPs: 62.1608. Time: 0.0063 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #16: GFLOPs: 44.8962. Time: 0.0088 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #17: GFLOPs: 60.6067. Time: 0.0065 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #18: GFLOPs: 59.4503. Time: 0.0066 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #19: GFLOPs: 55.9573. Time: 0.0070 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #20: GFLOPs: 53.6142. Time: 0.0073 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #21: GFLOPs: 50.2393. Time: 0.0078 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #22: GFLOPs: 50.3473. Time: 0.0078 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #23: GFLOPs: 62.9112. Time: 0.0063 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #24: GFLOPs: 47.2307. Time: 0.0083 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #25: GFLOPs: 54.7965. Time: 0.0072 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #26: GFLOPs: 50.2364. Time: 0.0078 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #27: GFLOPs: 45.3115. Time: 0.0087 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #28: GFLOPs: 61.8108. Time: 0.0064 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #29: GFLOPs: 62.4031. Time: 0.0063 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #30: GFLOPs: 62.6710. Time: 0.0063 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #20: "fused_subtract"] Trial #31: GFLOPs: 45.6438. Time: 0.0086 ms. Best GFLOPs: 93.9748
[13:44:58] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #20: "fused_subtract"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |            N/A |          N/A |                   N/A |      0 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 672
Total latency (us): 61231.7

[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #0: GFLOPs: 1.2483. Time: 0.3153 ms. Best GFLOPs: 1.2483
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #1: GFLOPs: 56.5262. Time: 0.0070 ms. Best GFLOPs: 56.5262
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #2: GFLOPs: 64.5871. Time: 0.0061 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #3: GFLOPs: 54.2842. Time: 0.0073 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #4: GFLOPs: 1.3434. Time: 0.2930 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #5: GFLOPs: 34.6349. Time: 0.0114 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #6: GFLOPs: 17.1991. Time: 0.0229 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #7: GFLOPs: 62.4132. Time: 0.0063 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #8: GFLOPs: 54.3867. Time: 0.0072 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #9: GFLOPs: 35.1477. Time: 0.0112 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #10: GFLOPs: 1.3497. Time: 0.2916 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #11: GFLOPs: 1.3364. Time: 0.2945 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #12: GFLOPs: 1.3315. Time: 0.2956 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #13: GFLOPs: 1.3393. Time: 0.2939 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #14: GFLOPs: 1.3143. Time: 0.2995 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #15: GFLOPs: 8.8287. Time: 0.0446 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #16: GFLOPs: 31.4824. Time: 0.0125 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #17: GFLOPs: 32.2672. Time: 0.0122 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #18: GFLOPs: 64.3485. Time: 0.0061 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #19: GFLOPs: 1.4024. Time: 0.2807 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #20: GFLOPs: 63.1499. Time: 0.0062 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #21: GFLOPs: 1.3608. Time: 0.2892 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #22: GFLOPs: 31.8004. Time: 0.0124 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #23: GFLOPs: 9.0372. Time: 0.0436 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #24: GFLOPs: 1.3486. Time: 0.2919 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #25: GFLOPs: 34.8947. Time: 0.0113 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #26: GFLOPs: 17.2342. Time: 0.0228 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #27: GFLOPs: 63.8970. Time: 0.0062 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #28: GFLOPs: 17.1221. Time: 0.0230 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #29: GFLOPs: 1.3361. Time: 0.2946 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #30: GFLOPs: 53.8845. Time: 0.0073 ms. Best GFLOPs: 64.5871
[13:44:58] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #21: "fused_power_mean"] Trial #31: GFLOPs: 9.7892. Time: 0.0402 ms. Best GFLOPs: 64.5871
[13:44:59] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #21: "fused_power_mean"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |        64.5871 |       6.0941 |              298.6106 |     32 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |            N/A |          N/A |                   N/A |      0 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 704
Total latency (us): 61530.3

[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #0: GFLOPs: 182.7858. Time: 0.0065 ms. Best GFLOPs: 182.7858
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #1: GFLOPs: 185.0593. Time: 0.0064 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #2: GFLOPs: 168.2062. Time: 0.0070 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #3: GFLOPs: 182.2666. Time: 0.0065 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #4: GFLOPs: 184.2458. Time: 0.0064 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #5: GFLOPs: 183.2637. Time: 0.0064 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #6: GFLOPs: 98.0330. Time: 0.0120 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #7: GFLOPs: 88.9540. Time: 0.0133 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #8: GFLOPs: 99.5438. Time: 0.0119 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #9: GFLOPs: 87.9204. Time: 0.0134 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #10: GFLOPs: 115.5795. Time: 0.0102 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #11: GFLOPs: 117.5076. Time: 0.0100 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #12: GFLOPs: 90.0304. Time: 0.0131 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #13: GFLOPs: 91.1247. Time: 0.0129 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #14: GFLOPs: 106.1451. Time: 0.0111 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #15: GFLOPs: 103.4413. Time: 0.0114 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #16: GFLOPs: 127.6472. Time: 0.0092 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #17: GFLOPs: 99.5652. Time: 0.0119 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #18: GFLOPs: 111.1178. Time: 0.0106 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #19: GFLOPs: 97.5931. Time: 0.0121 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #20: GFLOPs: 115.2407. Time: 0.0102 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #21: GFLOPs: 111.3763. Time: 0.0106 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #22: GFLOPs: 89.8262. Time: 0.0131 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #23: GFLOPs: 142.1742. Time: 0.0083 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #24: GFLOPs: 159.5020. Time: 0.0074 ms. Best GFLOPs: 185.0593
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #25: GFLOPs: 187.8036. Time: 0.0063 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #26: GFLOPs: 183.8122. Time: 0.0064 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #27: GFLOPs: 162.7368. Time: 0.0073 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #28: GFLOPs: 96.8423. Time: 0.0122 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #29: GFLOPs: 103.9716. Time: 0.0113 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #30: GFLOPs: 99.6314. Time: 0.0118 ms. Best GFLOPs: 187.8036
[13:44:59] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #22: "fused_add_sqrt_divide_multiply_add_reshape"] Trial #31: GFLOPs: 130.3767. Time: 0.0091 ms. Best GFLOPs: 187.8036
[13:45:00] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #22: "fused_add_sqrt_divide_multiply_add_reshape"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |        64.5871 |       6.0941 |              298.6106 |     32 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |       187.8036 |       6.2833 |                6.2833 |     32 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |            N/A |          N/A |                   N/A |      0 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 736
Total latency (us): 61536.6

[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #0: GFLOPs: 3.4086. Time: 0.4614 ms. Best GFLOPs: 3.4086
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #1: GFLOPs: 16.8208. Time: 0.0935 ms. Best GFLOPs: 16.8208
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #2: GFLOPs: 17.5261. Time: 0.0897 ms. Best GFLOPs: 17.5261
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #3: GFLOPs: 42.3014. Time: 0.0372 ms. Best GFLOPs: 42.3014
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #4: GFLOPs: 5.8593. Time: 0.2684 ms. Best GFLOPs: 42.3014
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #5: GFLOPs: 9.9895. Time: 0.1575 ms. Best GFLOPs: 42.3014
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #6: GFLOPs: 6.7076. Time: 0.2345 ms. Best GFLOPs: 42.3014
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #7: GFLOPs: 48.5925. Time: 0.0324 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #8: GFLOPs: 28.7387. Time: 0.0547 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #9: GFLOPs: 32.9208. Time: 0.0478 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #10: GFLOPs: 21.3619. Time: 0.0736 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #11: GFLOPs: 41.1096. Time: 0.0383 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #12: GFLOPs: 37.2624. Time: 0.0422 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #13: GFLOPs: 7.8805. Time: 0.1996 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #14: GFLOPs: 17.0790. Time: 0.0921 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #15: GFLOPs: 20.2698. Time: 0.0776 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #16: GFLOPs: 23.4974. Time: 0.0669 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #17: GFLOPs: 19.3983. Time: 0.0811 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #18: GFLOPs: 40.9020. Time: 0.0385 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #19: GFLOPs: 9.4906. Time: 0.1657 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #20: GFLOPs: 11.4714. Time: 0.1371 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #21: GFLOPs: 9.8975. Time: 0.1589 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #22: GFLOPs: 36.4208. Time: 0.0432 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #23: GFLOPs: 10.1558. Time: 0.1549 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #24: GFLOPs: 8.0286. Time: 0.1959 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #25: GFLOPs: 21.0400. Time: 0.0748 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #26: GFLOPs: 6.0438. Time: 0.2602 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #27: GFLOPs: 16.6800. Time: 0.0943 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #28: GFLOPs: 42.7259. Time: 0.0368 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #29: GFLOPs: 11.6871. Time: 0.1346 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #30: GFLOPs: 30.5959. Time: 0.0514 ms. Best GFLOPs: 48.5925
[13:45:00] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #23: "fused_nn_dense_3"] Trial #31: GFLOPs: 27.9614. Time: 0.0563 ms. Best GFLOPs: 48.5925
[13:45:01] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #23: "fused_nn_dense_3"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |        64.5871 |       6.0941 |              298.6106 |     32 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |       187.8036 |       6.2833 |                6.2833 |     32 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |        48.5925 |      32.3684 |               32.3684 |     32 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |            N/A |          N/A |                   N/A |      0 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 768
Total latency (us): 61569

[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #0: GFLOPs: 0.1925. Time: 0.0040 ms. Best GFLOPs: 0.1925
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #1: GFLOPs: 0.1932. Time: 0.0040 ms. Best GFLOPs: 0.1932
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #2: GFLOPs: 0.1945. Time: 0.0039 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #3: GFLOPs: 0.1926. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #4: GFLOPs: 0.1934. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #5: GFLOPs: 0.1932. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #6: GFLOPs: 0.1937. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #7: GFLOPs: 0.1944. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #8: GFLOPs: 0.1925. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #9: GFLOPs: 0.1914. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #10: GFLOPs: 0.1911. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #11: GFLOPs: 0.1918. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #12: GFLOPs: 0.1929. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #13: GFLOPs: 0.1929. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #14: GFLOPs: 0.1932. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #15: GFLOPs: 0.1922. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #16: GFLOPs: 0.1923. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #17: GFLOPs: 0.1928. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #18: GFLOPs: 0.1924. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #19: GFLOPs: 0.1931. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #20: GFLOPs: 0.1924. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #21: GFLOPs: 0.1935. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #22: GFLOPs: 0.1925. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #23: GFLOPs: 0.1897. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #24: GFLOPs: 0.1927. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #25: GFLOPs: 0.1928. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #26: GFLOPs: 0.1930. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #27: GFLOPs: 0.1906. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #28: GFLOPs: 0.1934. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #29: GFLOPs: 0.1928. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #30: GFLOPs: 0.1931. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:01] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #24: "fused_reshape_add_split"] Trial #31: GFLOPs: 0.1922. Time: 0.0040 ms. Best GFLOPs: 0.1945
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #24: "fused_reshape_add_split"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |        64.5871 |       6.0941 |              298.6106 |     32 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |       187.8036 |       6.2833 |                6.2833 |     32 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |        48.5925 |      32.3684 |               32.3684 |     32 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |         0.1945 |       3.9484 |                3.9484 |     32 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |            N/A |          N/A |                   N/A |      0 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 800
Total latency (us): 61572.9

[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #0: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #1: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #2: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #3: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #4: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #5: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #6: GFLOPs: 0.0000. Time: 0.0023 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #7: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #8: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #9: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #10: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #11: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #12: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #13: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #14: GFLOPs: 0.0000. Time: 0.0025 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #15: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #16: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #17: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #18: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #19: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #20: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #21: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #22: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #23: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #24: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #25: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #26: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #27: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #28: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #29: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #30: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/measure_callback/echo_statistics.cc:52: [Task #25: "fused_squeeze_1"] Trial #31: GFLOPs: 0.0000. Time: 0.0022 ms. Best GFLOPs: 0.0000
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/gradient_based.cc:172: [Updated] Task #25: "fused_squeeze_1"
 ID |                                                       Name |       FLOP | Weight | Speed (GFLOPS) | Latency (us) | Weighted Latency (us) | Trials | Terminated 
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
  0 |                                              fused_squeeze |          1 |      1 |         0.0004 |       2.2618 |                2.2618 |     32 |            
  1 |      fused_reshape_add_reshape_transpose_reshape_transpose |     393216 |     24 |        17.3213 |      22.7013 |              544.8316 |     32 |            
  2 |       fused_expand_dims_expand_dims_cast_subtract_multiply |        768 |      1 |         0.3488 |       2.2021 |                2.2021 |     32 |            
  3 |    fused_reshape_add_reshape_transpose_reshape_transpose_1 |     393216 |     24 |         6.8301 |      57.5713 |             1381.7120 |     32 |            
  4 |                                                 fused_mean |     393600 |     49 |       146.2430 |       2.6914 |              131.8791 |     32 |            
  5 |      fused_less_add_where_take_add_less_add_where_take_add |     787200 |      1 |       108.5697 |       7.2506 |                7.2506 |     32 |            
  6 |                fused_reshape_add_reshape_transpose_reshape |     393216 |     24 |        88.2429 |       4.4561 |              106.9455 |     32 |            
  7 |                                      fused_nn_batch_matmul |  301989888 |     24 |      5949.3998 |      50.7597 |             1218.2334 |     32 |            
  8 |                                   fused_reshape_divide_add |    4718592 |     24 |        94.4888 |      49.9381 |             1198.5145 |     32 |            
  9 |                                           fused_nn_softmax |    9437184 |     24 |       176.4070 |      53.4967 |             1283.9200 |     32 |            
 10 |                                              fused_reshape |          1 |     24 |         0.0000 |      50.1093 |             1202.6243 |     32 |            
 11 |                                    fused_nn_batch_matmul_1 |  301989888 |     24 |      3127.6378 |      96.5553 |             2317.3263 |     32 |            
 12 |                            fused_reshape_transpose_reshape |          1 |     24 |         0.0002 |       4.0477 |               97.1454 |     32 |            
 13 |                                             fused_nn_dense |  805306368 |     96 |      4197.9491 |     191.8333 |            18415.9952 |     32 |            
 14 |                         fused_add_sqrt_divide_multiply_add |    1180032 |     48 |       195.0729 |       6.0492 |              290.3608 |     32 |            
 15 |                                            fused_reshape_1 |          1 |     48 |         0.0003 |       3.8736 |              185.9311 |     32 |            
 16 |                                           fused_nn_dense_1 | 3221225472 |     24 |      4725.3204 |     681.6946 |            16360.6709 |     32 |            
 17 | fused_reshape_add_multiply_divide_erf_add_multiply_reshape |    7864320 |     24 |       233.7300 |      33.6470 |              807.5287 |     32 |            
 18 |                                           fused_nn_dense_2 | 3221225472 |     24 |      5192.7990 |     620.3255 |            14887.8111 |     32 |            
 19 |                                      fused_reshape_add_add |     786432 |     48 |        64.6898 |      12.1570 |              583.5348 |     32 |            
 20 |                                             fused_subtract |     393216 |     49 |        93.9748 |       4.1843 |              205.0293 |     32 |            
 21 |                                           fused_power_mean |     393600 |     49 |        64.5871 |       6.0941 |              298.6106 |     32 |            
 22 |                 fused_add_sqrt_divide_multiply_add_reshape |    1180032 |      1 |       187.8036 |       6.2833 |                6.2833 |     32 |            
 23 |                                           fused_nn_dense_3 |    1572864 |      1 |        48.5925 |      32.3684 |               32.3684 |     32 |            
 24 |                                    fused_reshape_add_split |        768 |      1 |         0.1945 |       3.9484 |                3.9484 |     32 |            
 25 |                                            fused_squeeze_1 |          1 |      1 |         0.0005 |       2.1723 |                2.1723 |     32 |            
---------------------------------------------------------------------------------------------------------------------------------------------------------------------
Total trials: 832
Total latency (us): 61575.1

[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #13: "fused_nn_dense"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #13 has finished. Remaining task(s): 25
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #16: "fused_nn_dense_1"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #16 has finished. Remaining task(s): 24
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #18: "fused_nn_dense_2"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #18 has finished. Remaining task(s): 23
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #11: "fused_nn_batch_matmul_1"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #11 has finished. Remaining task(s): 22
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #3: "fused_reshape_add_reshape_transpose_reshape_transpose_1"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #3 has finished. Remaining task(s): 21
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #9: "fused_nn_softmax"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #9 has finished. Remaining task(s): 20
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #7: "fused_nn_batch_matmul"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #7 has finished. Remaining task(s): 19
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #10: "fused_reshape"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #10 has finished. Remaining task(s): 18
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #8: "fused_reshape_divide_add"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #8 has finished. Remaining task(s): 17
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #17: "fused_reshape_add_multiply_divide_erf_add_multiply_reshape"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #17 has finished. Remaining task(s): 16
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #19: "fused_reshape_add_add"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #19 has finished. Remaining task(s): 15
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #1: "fused_reshape_add_reshape_transpose_reshape_transpose"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #1 has finished. Remaining task(s): 14
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #21: "fused_power_mean"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #21 has finished. Remaining task(s): 13
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #14: "fused_add_sqrt_divide_multiply_add"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #14 has finished. Remaining task(s): 12
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #20: "fused_subtract"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #20 has finished. Remaining task(s): 11
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #15: "fused_reshape_1"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #15 has finished. Remaining task(s): 10
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #4: "fused_mean"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #4 has finished. Remaining task(s): 9
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #6: "fused_reshape_add_reshape_transpose_reshape"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #6 has finished. Remaining task(s): 8
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #12: "fused_reshape_transpose_reshape"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #12 has finished. Remaining task(s): 7
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #23: "fused_nn_dense_3"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #23 has finished. Remaining task(s): 6
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #5: "fused_less_add_where_take_add_less_add_where_take_add"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #5 has finished. Remaining task(s): 5
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #22: "fused_add_sqrt_divide_multiply_add_reshape"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #22 has finished. Remaining task(s): 4
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #24: "fused_reshape_add_split"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #24 has finished. Remaining task(s): 3
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #0: "fused_squeeze"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #0 has finished. Remaining task(s): 2
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #2: "fused_expand_dims_expand_dims_cast_subtract_multiply"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #2 has finished. Remaining task(s): 1
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:125: Scheduler picks Task #25: "fused_squeeze_1"
[13:45:02] /home/yj/tvm/src/meta_schedule/task_scheduler/task_scheduler.cc:138: Task #25 has finished. Remaining task(s): 0
[[-5.4794602 -5.472217  -5.477391  -5.4770374 -5.4629254 -5.4810567
  -5.475374  -5.469065  -5.484395  -5.4680448 -5.479964  -5.460811
  -5.47154   -5.483414  -5.4771724 -5.4793277 -5.485556  -5.468996
  -5.468783  -5.471098  -5.479858  -5.4904695 -5.4698315 -5.465471
  -5.479239  -5.47814   -5.4729447 -5.4665165 -5.475222  -5.4739943
  -5.4658647 -5.4795995 -5.4756575 -5.478787  -5.4724603 -5.4708905
  -5.471332  -5.466795  -5.4642787 -5.473695  -5.4771633 -5.4804087
  -5.4719205 -5.4686174 -5.4717135 -5.4735317 -5.4637036 -5.4698744
  -5.484214  -5.4715033 -5.480965  -5.4739733 -5.471507  -5.465304
  -5.4677725 -5.4736814 -5.4671288 -5.4652624 -5.4714193 -5.4789453
  -5.4664497 -5.4750595 -5.474721  -5.4643273 -5.4738855 -5.4684024
  -5.4672627 -5.4759684 -5.460948  -5.4792523 -5.4702263 -5.4754467
  -5.467968  -5.487839  -5.472014  -5.469766  -5.474851  -5.4695477
  -5.4745584 -5.4668927 -5.4743614 -5.46796   -5.4560375 -5.470436
  -5.4677224 -5.468903  -5.469584  -5.462754  -5.467128  -5.4665756
  -5.466481  -5.4650764 -5.472782  -5.4672627 -5.4679537 -5.471075
  -5.4690366 -5.4590945 -5.464774  -5.4728928 -5.470575  -5.463492
  -5.469459  -5.462143  -5.473716  -5.4662485 -5.4528475 -5.4686456
  -5.4680223 -5.459622  -5.462484  -5.463254  -5.456598  -5.46232
  -5.4710565 -5.4669857 -5.4650073 -5.4660363 -5.465048  -5.470982
  -5.467421  -5.4632835 -5.460215  -5.469595  -5.4535394 -5.454752
  -5.4481006 -5.473943  -5.462308  -5.456747  -5.47105   -5.4696436
  -5.4673414 -5.4813175 -5.4694953 -5.4705486 -5.478645  -5.4807863
  -5.469511  -5.4801655 -5.4746747 -5.469226  -5.460738  -5.4711275
  -5.480608  -5.4839106 -5.4748945 -5.471517  -5.4778333 -5.474482
  -5.4741707 -5.4743505 -5.474307  -5.471409  -5.4739723 -5.4651456
  -5.464134  -5.483083  -5.4762826 -5.462601  -5.4858613 -5.474298
  -5.479066  -5.4787188 -5.474052  -5.481252  -5.466301  -5.4656477
  -5.4833884 -5.4841723 -5.4627814 -5.477062  -5.476796  -5.4691525
  -5.4596624 -5.4714136 -5.4724083 -5.4731455 -5.4755554 -5.481509
  -5.4781985 -5.4754767 -5.4784026 -5.472857  -5.4804087 -5.472853
  -5.467372  -5.473005  -5.4802685 -5.46337   -5.479168  -5.467187
  -5.4809995 -5.4708095 -5.4710493 -5.461748  -5.481425  -5.486016
  -5.4539895 -5.4718223 -5.480843  -5.468538  -5.460403  -5.471685
  -5.479786  -5.4842596 -5.486499  -5.4778376 -5.4818997 -5.4714875
  -5.4691854 -5.481673  -5.4665685 -5.471003  -5.490409  -5.4735785
  -5.4763765 -5.4795856 -5.4846177 -5.4573293 -5.469996  -5.4779663
  -5.46795   -5.4633765 -5.4648156 -5.469159  -5.471167  -5.4818144
  -5.477968  -5.48791   -5.465738  -5.4655766 -5.4544153 -5.477605
  -5.4661965 -5.4507194 -5.470658  -5.4676657 -5.4721866 -5.468592
  -5.462799  -5.4766407 -5.4781556 -5.473235  -5.482928  -5.471532
  -5.478365  -5.4755526 -5.4787083 -5.473558  -5.465176  -5.4797306
  -5.4722652 -5.4865236 -5.473008  -5.4866195 -5.470557  -5.4697556
  -5.467186  -5.4728336 -5.4714055 -5.476215  -5.4748535 -5.4637685
  -5.4723563 -5.469684  -5.4743433 -5.461026  -5.460839  -5.464355
  -5.4699903 -5.4761167 -5.464327  -5.4671125 -5.4615107 -5.4567122
  -5.4579096 -5.474095  -5.4628434 -5.473219  -5.4620924 -5.4693165
  -5.475942  -5.4638042 -5.4639797 -5.4791417 -5.470346  -5.468344
  -5.468912  -5.4529605 -5.465467  -5.4693646 -5.4728928 -5.464968
  -5.4682317 -5.47337   -5.4608316 -5.4738812 -5.4729486 -5.4772186
  -5.4516015 -5.4755955 -5.475514  -5.469572  -5.4643874 -5.4584494
  -5.4672923 -5.4687376 -5.4754353 -5.4670753 -5.463321  -5.4589343
  -5.4695096 -5.467051  -5.4692492 -5.4672623 -5.4809303 -5.4807415
  -5.4588013 -5.469222  -5.462293  -5.468643  -5.47972   -5.4725184
  -5.4692974 -5.4753747 -5.4717093 -5.462841  -5.4635153 -5.4710617
  -5.4613576 -5.4650736 -5.454196  -5.4709806 -5.4695287 -5.4685073
  -5.4711094 -5.4690976 -5.473582  -5.4700155 -5.458983  -5.473752
  -5.456548  -5.463154  -5.466612  -5.4602623 -5.4544234 -5.4601617
  -5.477946  -5.468419  -5.438025  -5.4663124 -5.4601693 -5.4651155
  -5.46273   -5.458449  -5.464858  -5.4673696 -5.463162  -5.4626026
  -5.4789996 -5.4824114 -5.4688554 -5.4673405 -5.452223  -5.4571996
  -5.463584  -5.4534206 -5.471111  -5.4570613 -5.462277  -5.467788
  -5.473564  -5.459208  -5.471151  -5.4648967 -5.4611855 -5.4678907
  -5.4700594 -5.4649296 -5.468151  -5.4728055 -5.4654107 -5.481143 ]]
[[-5.4794593 -5.4722147 -5.4773927 -5.4770427 -5.4629245 -5.4810586
  -5.4753776 -5.469063  -5.484398  -5.4680448 -5.4799657 -5.4608107
  -5.471541  -5.483414  -5.4771795 -5.479323  -5.485557  -5.4690003
  -5.4687815 -5.471097  -5.4798636 -5.4904714 -5.469836  -5.465474
  -5.4792423 -5.4781384 -5.4729476 -5.46652   -5.475226  -5.4739985
  -5.465858  -5.4796014 -5.47566   -5.4787903 -5.4724627 -5.4708934
  -5.471334  -5.46679   -5.4642797 -5.473696  -5.4771667 -5.4804063
  -5.47192   -5.468617  -5.47171   -5.473538  -5.4637046 -5.4698796
  -5.4842095 -5.471502  -5.4809637 -5.473971  -5.471508  -5.465305
  -5.467773  -5.473682  -5.4671264 -5.465263  -5.471419  -5.478943
  -5.466454  -5.475061  -5.474726  -5.46433   -5.4738855 -5.468404
  -5.4672675 -5.4759626 -5.4609485 -5.479256  -5.470227  -5.475448
  -5.4679656 -5.48784   -5.4720154 -5.469767  -5.4748516 -5.4695444
  -5.474562  -5.4668927 -5.4743633 -5.467962  -5.456035  -5.470435
  -5.4677253 -5.4689054 -5.4695826 -5.4627547 -5.467125  -5.466577
  -5.466482  -5.4650807 -5.472782  -5.4672637 -5.4679537 -5.471076
  -5.4690356 -5.459092  -5.4647727 -5.4728928 -5.4705753 -5.463496
  -5.469461  -5.4621453 -5.473719  -5.466251  -5.4528527 -5.4686413
  -5.4680243 -5.4596257 -5.462484  -5.463261  -5.4566016 -5.4623184
  -5.4710555 -5.466986  -5.465011  -5.4660387 -5.4650474 -5.470984
  -5.467426  -5.4632897 -5.460217  -5.4695935 -5.4535327 -5.454754
  -5.448101  -5.4739423 -5.4623103 -5.4567456 -5.4710507 -5.4696455
  -5.4673448 -5.4813166 -5.469499  -5.470553  -5.478643  -5.480784
  -5.4695086 -5.4801664 -5.4746737 -5.469226  -5.460734  -5.471126
  -5.4806085 -5.483912  -5.4749    -5.4715257 -5.4778314 -5.4744887
  -5.474173  -5.4743524 -5.4743114 -5.471412  -5.4739747 -5.4651504
  -5.464137  -5.4830856 -5.4762797 -5.4626    -5.485868  -5.4742975
  -5.4790616 -5.47872   -5.474054  -5.481255  -5.4663053 -5.4656487
  -5.483393  -5.4841723 -5.462788  -5.477059  -5.476801  -5.4691534
  -5.45966   -5.4714103 -5.472411  -5.473149  -5.4755588 -5.481508
  -5.4782014 -5.4754786 -5.478406  -5.472856  -5.480414  -5.4728518
  -5.4673753 -5.473004  -5.480266  -5.4633713 -5.4791675 -5.4671874
  -5.4810023 -5.470815  -5.471055  -5.4617515 -5.481424  -5.486016
  -5.4539924 -5.471825  -5.480845  -5.468535  -5.460401  -5.4716897
  -5.479785  -5.484263  -5.486502  -5.477837  -5.4818997 -5.4714904
  -5.469188  -5.4816766 -5.4665713 -5.471001  -5.490408  -5.4735794
  -5.4763775 -5.4795914 -5.484614  -5.4573293 -5.469995  -5.477965
  -5.4679523 -5.463377  -5.4648166 -5.4691596 -5.4711657 -5.481814
  -5.4779725 -5.48791   -5.4657397 -5.465575  -5.454415  -5.4776087
  -5.4661994 -5.450722  -5.470656  -5.467672  -5.4721904 -5.468597
  -5.462799  -5.476644  -5.478156  -5.4732337 -5.4829316 -5.471541
  -5.4783664 -5.4755607 -5.478712  -5.4735603 -5.465176  -5.479736
  -5.472262  -5.486524  -5.473005  -5.4866204 -5.470553  -5.4697576
  -5.4671903 -5.4728346 -5.4714103 -5.4762154 -5.4748545 -5.463763
  -5.47236   -5.469689  -5.4743485 -5.4610324 -5.460838  -5.464353
  -5.4699945 -5.4761186 -5.4643216 -5.467116  -5.461516  -5.456714
  -5.45791   -5.4740934 -5.4628463 -5.473221  -5.4620976 -5.4693174
  -5.475948  -5.463808  -5.463988  -5.479138  -5.470347  -5.468343
  -5.4689145 -5.4529586 -5.465472  -5.4693637 -5.472895  -5.464972
  -5.4682283 -5.4733686 -5.460831  -5.473882  -5.4729476 -5.477223
  -5.4516025 -5.4755893 -5.475515  -5.4695745 -5.464386  -5.458445
  -5.467297  -5.468736  -5.4754305 -5.4670763 -5.4633317 -5.458942
  -5.469505  -5.467048  -5.4692507 -5.467263  -5.4809294 -5.4807444
  -5.4588046 -5.4692264 -5.4622903 -5.4686427 -5.479717  -5.4725237
  -5.4693017 -5.4753766 -5.4717097 -5.4628406 -5.4635105 -5.471061
  -5.46136   -5.4650736 -5.4542007 -5.4709845 -5.4695296 -5.4685097
  -5.471108  -5.4691    -5.4735875 -5.470015  -5.4589825 -5.4737506
  -5.456549  -5.463159  -5.4666133 -5.4602637 -5.4544272 -5.4601617
  -5.477944  -5.4684196 -5.438029  -5.466313  -5.460171  -5.465119
  -5.4627347 -5.4584494 -5.464858  -5.467367  -5.463163  -5.462606
  -5.479002  -5.482416  -5.4688506 -5.4673448 -5.4522243 -5.4572005
  -5.463587  -5.45342   -5.4711175 -5.457064  -5.462279  -5.467792
  -5.4735646 -5.4592056 -5.4711456 -5.4648967 -5.4611874 -5.467891
  -5.470056  -5.4649305 -5.468153  -5.4728036 -5.4654145 -5.481138 ]]
